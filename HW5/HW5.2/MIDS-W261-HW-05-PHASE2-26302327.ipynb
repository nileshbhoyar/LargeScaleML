{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW5\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  Nilesh Bhoyar   \n",
    "__Class:__ MIDS w261 (Section *Your Section Goes Here*, e.g., Fall 2016 Group 1)     \n",
    "__Email:__  nilesh.bhoyar@iSchool.Berkeley.edu     \n",
    "__Week:__   5\n",
    "\n",
    "__Due Time:__ 2 Phases. \n",
    "\n",
    "* __HW5 Phase 1__ \n",
    "This can be done on a local machine (with a unit test on the cloud such as AltaScale's PaaS or on AWS) and is due Tuesday, Week 6 by 8AM (West coast time). It will primarily focus on building a unit/systems and for pairwise similarity calculations pipeline (for stripe documents)\n",
    "\n",
    "* __HW5 Phase 2__ \n",
    "This will require the AltaScale cluster and will be due Tuesday, Week 7 by 8AM (West coast time). \n",
    "The focus of  HW5 Phase 2  will be to scale up the unit/systems tests to the Google 5 gram corpus. This will be a group exercise \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a name=\"TOC\"></a> \n",
    "\n",
    "1.  [HW Instructions](#1)   \n",
    "2.  [HW References](#2)\n",
    "3.  [HW Problems](#3)   \n",
    "       \n",
    "    5.4.  [HW5.4](#5.4)    \n",
    "    5.5.  [HW5.5](#5.5)    \n",
    "    5.6.  [HW5.6](#5.6)    \n",
    "    5.7.  [HW5.7](#5.7)    \n",
    "    5.8.  [HW5.8](#5.8)    \n",
    "    5.9.  [HW5.9](#5.9)    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "# 1 Instructions\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale   \n",
    "DATSCIW261 ASSIGNMENT #5\n",
    "\n",
    "Version 2017-9-2 \n",
    "\n",
    "\n",
    "### IMPORTANT\n",
    "\n",
    "This homework must be completed in the cloud \n",
    "\n",
    "### === INSTRUCTIONS for SUBMISSIONS ===   \n",
    "Follow the instructions for submissions carefully.\n",
    "\n",
    "Each student has a `HW-<user>` repository for all assignments.   \n",
    "\n",
    "Click this link to enable you to create a github repo within the MIDS261 Classroom:   \n",
    "https://classroom.github.com/assignment-invitations/3b1d6c8e58351209f9dd865537111ff8   \n",
    "and follow the instructions to create a HW repo.\n",
    "\n",
    "Push the following to your HW github repo into the master branch:\n",
    "* Your local HW5 directory. Your repo file structure should look like this:\n",
    "\n",
    "```\n",
    "HW-<user>\n",
    "    --HW3\n",
    "       |__MIDS-W261-HW-03-<Student_id>.ipynb\n",
    "       |__MIDS-W261-HW-03-<Student_id>.pdf\n",
    "       |__some other hw3 file\n",
    "    --HW4\n",
    "       |__MIDS-W261-HW-04-<Student_id>.ipynb\n",
    "       |__MIDS-W261-HW-04-<Student_id>.pdf\n",
    "       |__some other hw4 file\n",
    "    etc..\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\">\n",
    "# 2 Useful References\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "* See async and live lectures for this week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\">\n",
    "# 3 HW Problems\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5.4\"></a> \n",
    "# PHASE 2\n",
    "----------\n",
    "\n",
    "# HW 5.4   \n",
    "## Full-scale experiment on Google N-gram data on the CLOUD\n",
    "__ Once you are happy with your test results __ proceed to generating  your results on the Google n-grams dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.4.0  <a name=\"5.4.0\"></a> Run systems tests on the CLOUD  (PHASE 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Repeat HW5.3.0 (using the same small data sources that were used in HW5.3.0) on ** the cloud** (e.g., AltaScale / AWS/ SoftLayer/ Azure). Make sure all tests give correct results! Good luck out there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nileshbhoyar/.conda/envs/py27/bin/python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing buildStripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile buildStripes.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import re\n",
    "import mrjob\n",
    "import json\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from itertools import  combinations\n",
    "class MRbuildStripes(MRJob):\n",
    "  \n",
    "  #START SUDENT CODE531_STRIPES\n",
    "    SORT_VALUES = True\n",
    "   \n",
    "    #def __init__(self, *args, **kwargs):\n",
    "    #    super(MRjoins, self).__init__(*args, **kwargs)\n",
    "        \n",
    "  \n",
    "               \n",
    "    \n",
    "        \n",
    "    def mapper(self, _, recs):\n",
    "        self.increment_counter('Execution Counts', 'mapper calls', 1)\n",
    "        fields = recs.split(\"\\t\")\n",
    "        \n",
    "        products = fields[0].lower().replace('\\n','').split()\n",
    "        for i, term in enumerate(products):\n",
    "                # Create a new stripe for each term\n",
    "                stripe = {}\n",
    "\n",
    "                for j, token in enumerate(products):\n",
    "                    # Don't count the term's co-occurrence with itself\n",
    "                    if i != j:\n",
    "                        x = stripe.get(token,None)\n",
    "                        if x == None:\n",
    "                            stripe[token] = int( fields[1])\n",
    "                        else:\n",
    "                            stripe[token] += int(fields[1])\n",
    "\n",
    "                # Emit the term and the stripe\n",
    "                yield term, stripe\n",
    "    \n",
    "    def combiner(self, word, stripes):\n",
    "        yield word, self.combine_stripes(stripes)\n",
    "\n",
    "    def combine_stripes(self, stripes):\n",
    "        combined_stripe = {}\n",
    "\n",
    "        for stripe in stripes:\n",
    "            for key, value in stripe.iteritems():\n",
    "                if key in combined_stripe:\n",
    "                    combined_stripe[key] += int(value)\n",
    "                else:\n",
    "                    combined_stripe[key] = int(value)\n",
    "\n",
    "        return combined_stripe\n",
    "    def reducer(self,key, records):\n",
    "        yield key, self.combine_stripes(records)\n",
    "        \n",
    "    def steps(self):  #pipeline of Map-Reduce jobs\n",
    "        step = MRStep( \n",
    "                    mapper=self.mapper,       # STEP 1: word count step\n",
    "                    combiner = self.combiner,\n",
    "                    reducer=self.reducer\n",
    "                    )\n",
    "        return [step]\n",
    "            \n",
    "\n",
    "\n",
    "  #END SUDENT CODE531_STRIPES\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "  MRbuildStripes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing invertedIndex.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile invertedIndex.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import JSONProtocol\n",
    "class MRinvertedIndex(MRJob):\n",
    "    INPUT_PROTOCOL = JSONProtocol\n",
    "    SORT_VALUES = True\n",
    "#START SUDENT CODE531_INV_INDEX\n",
    "    def mapper_normalize_transpose(self, word, rate_stripe):\n",
    "\n",
    "        # First compute the magnitude for the vector.\n",
    "\n",
    "        #magnitude = math.sqrt(sum([value ** 2 for value in rate_stripe.itervalues()]))\n",
    "\n",
    "        # Divide each value in the vector by the magnitude to normalize.\n",
    "        length = len(rate_stripe)\n",
    "        for key, value in rate_stripe.iteritems():\n",
    "            #normalized_value = value / magnitude\n",
    "            yield key, { word: length}\n",
    "    def combiner_normalize_transpose(self, word, transpose_stripes):\n",
    "        yield word, self.combine_stripes(transpose_stripes)\n",
    "    def reducer_normalize_transpose(self, word, transpose_stripes):\n",
    "        yield word, self.combine_stripes(transpose_stripes)\n",
    "    def combine_stripes(self, stripes):\n",
    "        combined_stripe = {}\n",
    "\n",
    "        for stripe in stripes:\n",
    "            for key, value in stripe.iteritems():\n",
    "                if key in combined_stripe:\n",
    "                    combined_stripe[key] += value\n",
    "                else:\n",
    "                    combined_stripe[key] = value\n",
    "\n",
    "        return combined_stripe\n",
    "\n",
    "        \n",
    "    def steps(self):\n",
    "\n",
    "        transpose_step = MRStep(\n",
    "            mapper = self.mapper_normalize_transpose,\n",
    "            combiner = self.combiner_normalize_transpose,\n",
    "            reducer = self.reducer_normalize_transpose)\n",
    "        return [transpose_step]\n",
    "\n",
    "#END SUDENT CODE531_INV_INDEX\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRinvertedIndex.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing similarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile similarity.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import JSONProtocol\n",
    "\n",
    "class MRsimilarity(MRJob):\n",
    "    INPUT_PROTOCOL = JSONProtocol\n",
    "    SORT_VALUES = True\n",
    "\n",
    "#START SUDENT CODE531_SIMILARITY\n",
    "    def mapper_jaccard(self, word, rate_stripe):\n",
    "        #get all words and lengths\n",
    "#We will emit stripes for each word vector here.  These stripes will\n",
    "#be used in combiner to find the common length i.e. words occuring together\n",
    "        nonzero_keys = [key for key, value in rate_stripe.iteritems() if value != 0]\n",
    "\n",
    "    \n",
    "\n",
    "        sorted_keys = sorted(nonzero_keys)\n",
    "        # N * N  complexity matrix calculation\n",
    "        #We are going over each record to find out the common occureances\n",
    "        for i in range(0, len(sorted_keys)):\n",
    "            left_label = sorted_keys[i]\n",
    "\n",
    "            stripe = {}\n",
    "\n",
    "            for j in range(i + 1, len(sorted_keys)):\n",
    "                right_label = sorted_keys[j]\n",
    "                stripe[right_label] = 1\n",
    "\n",
    "            yield left_label, stripe\n",
    "\n",
    "     \n",
    "\n",
    "        for key in sorted_keys:\n",
    "            yield '*',{key:1}\n",
    "            #{u'DocC': 1}\n",
    "        #yield '*', { key: 1 for key in sorted_keys }\n",
    "    def combiner_jaccard(self, left_label, partial_stripes):\n",
    "        yield left_label, self.combine_stripes(partial_stripes)\n",
    "\n",
    "#find out the jaccard values.\n",
    "    def reducer_jaccard(self, left_label, partial_stripes):\n",
    "        total_stripe = self.combine_stripes(partial_stripes)\n",
    "        #this stores the total length of each word Vector\n",
    "        if left_label == '*':\n",
    "            self.total_counts = total_stripe\n",
    "            return\n",
    "\n",
    "        for right_label, intersection_size in total_stripe.iteritems():\n",
    "            coordinate = (left_label, right_label)\n",
    "            union_size = self.total_counts[left_label] + self.total_counts[right_label]\n",
    "\n",
    "            \n",
    "            jaccard_distance = float(intersection_size)/float(union_size - intersection_size) #jaccard\n",
    "            dice_coef = (float(intersection_size) * 2 )/float(union_size ) #dice Coefficient\n",
    "            final = {}\n",
    "            final['jaccard'] = jaccard_distance\n",
    "            final['dice'] = dice_coef\n",
    "            yield coordinate, final\n",
    "\n",
    "#in-memory combiner\n",
    "    def combine_stripes(self, stripes):\n",
    "        combined_stripe = {}\n",
    "\n",
    "        for stripe in stripes:\n",
    "            for key, value in stripe.iteritems():\n",
    "                if key in combined_stripe:\n",
    "                    combined_stripe[key] += value\n",
    "                else:\n",
    "                    combined_stripe[key] = value\n",
    "\n",
    "        return combined_stripe\n",
    "\n",
    "   \n",
    "    def steps(self):\n",
    "         distance_step = MRStep(\n",
    "                mapper = self.mapper_jaccard,\n",
    "                combiner = self.combiner_jaccard,\n",
    "                reducer = self.reducer_jaccard,\n",
    "                jobconf = {\n",
    "                    'mapreduce.job.reduces': 1\n",
    "                    \n",
    "                })\n",
    "         return [distance_step]\n",
    "#END SUDENT CODE531_SIMILARITY\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    MRsimilarity.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n",
    "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\n",
    "A Biography of General George\t92\t90\t74\n",
    "A Case Study in Government\t102\t102\t78\n",
    "A Case Study of Female\t447\t447\t327\n",
    "A Case Study of Limited\t55\t55\t43\n",
    "A Child's Christmas in Wales\t1099\t1061\t866\n",
    "A Circumstantial Narrative of the\t62\t62\t50\n",
    "A City by the Sea\t62\t60\t49\n",
    "A Collection of Fairy Tales\t123\t117\t80\n",
    "A Collection of Forms of\t116\t103\t82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing atlas-boon-systems-test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile atlas-boon-systems-test.txt\n",
    "atlas boon\t50\t50\t50\n",
    "boon cava dipped\t10\t10\t10\n",
    "atlas dipped\t15\t15\t15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#copy all files on local folder\n",
    "\n",
    "!rm -r /home/nileshbhoyar/data\n",
    "!hdfs dfs -copyToLocal hdfs:///user/cendylin/filtered-5Grams/ /home/nileshbhoyar/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /tmp/buildStripes.nileshbhoyar.20170623.130721.532942\n",
      "Running step 1 of 1...\n",
      "Counters: 1\n",
      "\tExecution Counts\n",
      "\t\tmapper calls=10\n",
      "Counters: 1\n",
      "\tExecution Counts\n",
      "\t\tmapper calls=10\n",
      "Streaming final output from /tmp/buildStripes.nileshbhoyar.20170623.130721.532942/output...\n",
      "Removing temp directory /tmp/buildStripes.nileshbhoyar.20170623.130721.532942...\n",
      "\"forms\"\t{\"a\":116,\"of\":232,\"collection\":116}\n",
      "\"general\"\t{\"a\":92,\"of\":92,\"george\":92,\"biography\":92}\n",
      "\"george\"\t{\"a\":92,\"of\":92,\"biography\":92,\"general\":92}\n",
      "\"government\"\t{\"a\":102,\"case\":102,\"study\":102,\"in\":102}\n",
      "\"in\"\t{\"a\":1201,\"case\":102,\"government\":102,\"study\":102,\"child's\":1099,\"wales\":1099,\"christmas\":1099}\n",
      "\"limited\"\t{\"a\":55,\"case\":55,\"study\":55,\"of\":55}\n",
      "\"narrative\"\t{\"a\":62,\"of\":62,\"the\":62,\"circumstantial\":62}\n",
      "\"of\"\t{\"a\":1011,\"case\":502,\"circumstantial\":62,\"george\":92,\"limited\":55,\"of\":232,\"tales\":123,\"collection\":355,\"general\":92,\"forms\":232,\"female\":447,\"narrative\":62,\"study\":502,\"fairy\":123,\"the\":62,\"biography\":92}\n",
      "\"religious\"\t{\"a\":59,\"bill\":59,\"for\":59,\"establishing\":59}\n",
      "\"sea\"\t{\"a\":62,\"city\":62,\"the\":62,\"by\":62}\n",
      "\"study\"\t{\"a\":604,\"case\":604,\"limited\":55,\"government\":102,\"of\":502,\"female\":447,\"in\":102}\n",
      "\"tales\"\t{\"a\":123,\"of\":123,\"fairy\":123,\"collection\":123}\n",
      "\"the\"\t{\"a\":124,\"city\":62,\"circumstantial\":62,\"of\":62,\"sea\":62,\"narrative\":62,\"by\":62}\n",
      "\"wales\"\t{\"a\":1099,\"in\":1099,\"christmas\":1099,\"child's\":1099}\n",
      "\"a\"\t{\"limited\":55,\"sea\":62,\"general\":92,\"female\":447,\"in\":1201,\"religious\":59,\"george\":92,\"biography\":92,\"city\":62,\"for\":59,\"tales\":123,\"child's\":1099,\"forms\":116,\"wales\":1099,\"christmas\":1099,\"government\":102,\"collection\":239,\"by\":62,\"case\":604,\"circumstantial\":62,\"fairy\":123,\"of\":1011,\"study\":604,\"bill\":59,\"establishing\":59,\"narrative\":62,\"the\":124}\n",
      "\"bill\"\t{\"a\":59,\"religious\":59,\"for\":59,\"establishing\":59}\n",
      "\"biography\"\t{\"a\":92,\"of\":92,\"george\":92,\"general\":92}\n",
      "\"by\"\t{\"a\":62,\"city\":62,\"the\":62,\"sea\":62}\n",
      "\"case\"\t{\"a\":604,\"limited\":55,\"government\":102,\"of\":502,\"study\":604,\"female\":447,\"in\":102}\n",
      "\"child's\"\t{\"a\":1099,\"wales\":1099,\"christmas\":1099,\"in\":1099}\n",
      "\"christmas\"\t{\"a\":1099,\"wales\":1099,\"in\":1099,\"child's\":1099}\n",
      "\"circumstantial\"\t{\"a\":62,\"of\":62,\"the\":62,\"narrative\":62}\n",
      "\"city\"\t{\"a\":62,\"the\":62,\"by\":62,\"sea\":62}\n",
      "\"collection\"\t{\"a\":239,\"of\":355,\"fairy\":123,\"tales\":123,\"forms\":116}\n",
      "\"establishing\"\t{\"a\":59,\"bill\":59,\"religious\":59,\"for\":59}\n",
      "\"fairy\"\t{\"a\":123,\"of\":123,\"tales\":123,\"collection\":123}\n",
      "\"female\"\t{\"a\":447,\"case\":447,\"study\":447,\"of\":447}\n",
      "\"for\"\t{\"a\":59,\"bill\":59,\"religious\":59,\"establishing\":59}\n"
     ]
    }
   ],
   "source": [
    "#!hdfs dfs rm --recursive systems_test_stripes_1\n",
    "!python buildStripes.py -r local googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt > systems_test_stripes_1\n",
    "!cat systems_test_stripes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: Unknown command\n",
      "Did you mean -rm?  This command begins with a dash.\n",
      "No configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /tmp/buildStripes.nileshbhoyar.20170623.130723.480051\n",
      "Running step 1 of 1...\n",
      "Counters: 1\n",
      "\tExecution Counts\n",
      "\t\tmapper calls=3\n",
      "Counters: 1\n",
      "\tExecution Counts\n",
      "\t\tmapper calls=3\n",
      "Streaming final output from /tmp/buildStripes.nileshbhoyar.20170623.130723.480051/output...\n",
      "Removing temp directory /tmp/buildStripes.nileshbhoyar.20170623.130723.480051...\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Make Stripes from ngrams for systems test 2\n",
    "###########################################################################\n",
    "\n",
    "!hdfs dfs rm --recursive systems_test_stripes_2\n",
    "!python buildStripes.py -r local atlas-boon-systems-test.txt > systems_test_stripes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DocA\"\t{\"X\":20, \"Y\":30, \"Z\":5}\r\n",
      "\"DocB\"\t{\"X\":100, \"Y\":20}\r\n",
      "\"DocC\"\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\r\n"
     ]
    }
   ],
   "source": [
    "with open(\"systems_test_stripes_3\", \"w\") as f:\n",
    "    f.writelines([\n",
    "        '\"DocA\"\\t{\"X\":20, \"Y\":30, \"Z\":5}\\n',\n",
    "        '\"DocB\"\\t{\"X\":100, \"Y\":20}\\n',  \n",
    "        '\"DocC\"\\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\\n'\n",
    "    ])\n",
    "!cat systems_test_stripes_3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /tmp/invertedIndex.nileshbhoyar.20170623.130724.961909\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/invertedIndex.nileshbhoyar.20170623.130724.961909/output...\n",
      "Removing temp directory /tmp/invertedIndex.nileshbhoyar.20170623.130724.961909...\n",
      "No configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /tmp/invertedIndex.nileshbhoyar.20170623.130726.185050\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/invertedIndex.nileshbhoyar.20170623.130726.185050/output...\n",
      "Removing temp directory /tmp/invertedIndex.nileshbhoyar.20170623.130726.185050...\n",
      "No configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /tmp/invertedIndex.nileshbhoyar.20170623.130727.411979\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/invertedIndex.nileshbhoyar.20170623.130727.411979/output...\n",
      "Removing temp directory /tmp/invertedIndex.nileshbhoyar.20170623.130727.411979...\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndex.py -r local systems_test_stripes_1 > systems_test_index_1\n",
    "!python invertedIndex.py -r local systems_test_stripes_2 > systems_test_index_2\n",
    "!python invertedIndex.py -r local systems_test_stripes_3 > systems_test_index_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  1  - Inverted Index\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "             \"a\" |          bill 4 |     biography 4 |            by 4\n",
      "          \"bill\" |            a 27 |  establishing 4 |           for 4\n",
      "     \"biography\" |            a 27 |       general 4 |        george 4\n",
      "            \"by\" |            a 27 |          city 4 |           sea 4\n",
      "          \"case\" |            a 27 |        female 4 |    government 4\n",
      "       \"child's\" |            a 27 |     christmas 4 |            in 7\n",
      "     \"christmas\" |            a 27 |       child's 4 |            in 7\n",
      "\"circumstantial\" |            a 27 |     narrative 4 |           of 16\n",
      "          \"city\" |            a 27 |            by 4 |           sea 4\n",
      "    \"collection\" |            a 27 |         fairy 4 |         forms 3\n",
      "  \"establishing\" |            a 27 |          bill 4 |           for 4\n",
      "         \"fairy\" |            a 27 |    collection 5 |           of 16\n",
      "        \"female\" |            a 27 |          case 7 |           of 16\n",
      "           \"for\" |            a 27 |          bill 4 |  establishing 4\n",
      "         \"forms\" |            a 27 |    collection 5 |           of 16\n",
      "       \"general\" |            a 27 |     biography 4 |        george 4\n",
      "        \"george\" |            a 27 |     biography 4 |       general 4\n",
      "    \"government\" |            a 27 |          case 7 |            in 7\n",
      "            \"in\" |            a 27 |          case 7 |       child's 4\n",
      "       \"limited\" |            a 27 |          case 7 |           of 16\n",
      "     \"narrative\" |            a 27 |circumstantial 4 |           of 16\n",
      "            \"of\" |            a 27 |     biography 4 |          case 7\n",
      "     \"religious\" |            a 27 |          bill 4 |  establishing 4\n",
      "           \"sea\" |            a 27 |            by 4 |          city 4\n",
      "         \"study\" |            a 27 |          case 7 |        female 4\n",
      "         \"tales\" |            a 27 |    collection 5 |         fairy 4\n",
      "           \"the\" |            a 27 |            by 4 |circumstantial 4\n",
      "         \"wales\" |            a 27 |       child's 4 |     christmas 4\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  2  - Inverted Index\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "         \"atlas\" |          boon 3 |        dipped 3 |                \n",
      "          \"boon\" |         atlas 2 |          cava 2 |        dipped 3\n",
      "          \"cava\" |          boon 3 |        dipped 3 |                \n",
      "        \"dipped\" |         atlas 2 |          boon 3 |          cava 2\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  3  - Inverted Index\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "             \"M\" |          DocC 4 |                 |                \n",
      "             \"N\" |          DocC 4 |                 |                \n",
      "             \"X\" |          DocA 3 |          DocB 2 |                \n",
      "             \"Y\" |          DocA 3 |          DocB 2 |          DocC 4\n",
      "             \"Z\" |          DocA 3 |          DocC 4 |                \n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Pretty print systems tests for generating Inverted Index\n",
    "##########################################################\n",
    "\n",
    "import json\n",
    "\n",
    "for i in range(1,4):\n",
    "    print \"—\"*100\n",
    "    print \"Systems test \",i,\" - Inverted Index\"\n",
    "    print \"—\"*100  \n",
    "    with open(\"systems_test_index_\"+str(i),\"r\") as f:\n",
    "        lines = sorted(f.readlines())\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            word, doc_list = line.split(\"\\t\")\n",
    "            doc_dict = json.loads(doc_list)\n",
    "            stripe=[]\n",
    "            for doc in doc_dict:\n",
    "                stripe.append([doc, doc_dict[doc]])\n",
    "            stripe=sorted(stripe)\n",
    "            stripe.extend([[\"\",\"\"] for _ in xrange(3 - len(stripe))])\n",
    "\n",
    "            print \"{0:>16} |{1:>16} |{2:>16} |{3:>16}\".format(\n",
    "              (word), stripe[0][0]+\" \"+str(stripe[0][1]), stripe[1][0]+\" \"+str(stripe[1][1]), stripe[2][0]+\" \"+str(stripe[2][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/similarity.nileshbhoyar.20170623.130728.790107\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nileshbhoyar/tmp/mrjob/similarity.nileshbhoyar.20170623.130728.790107/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob3854595593789649028.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_2477\n",
      "  Submitted application application_1497906899862_2477\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_2477/\n",
      "  Running job: job_1497906899862_2477\n",
      "  Job job_1497906899862_2477 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_2477 completed successfully\n",
      "  Output directory: hdfs:///user/nileshbhoyar/tmp/mrjob/similarity.nileshbhoyar.20170623.130728.790107/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2868\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=27736\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2211\n",
      "\t\tFILE: Number of bytes written=405587\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3248\n",
      "\t\tHDFS: Number of bytes written=27736\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=32415744\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=14517760\n",
      "\t\tTotal time spent by all map tasks (ms)=21104\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=63312\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5671\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=28355\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=21104\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5671\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3940\n",
      "\t\tCombine input records=318\n",
      "\t\tCombine output records=56\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=217\n",
      "\t\tInput split bytes=380\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=13073\n",
      "\t\tMap output materialized bytes=2391\n",
      "\t\tMap output records=318\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1896153088\n",
      "\t\tReduce input groups=29\n",
      "\t\tReduce input records=56\n",
      "\t\tReduce output records=378\n",
      "\t\tReduce shuffle bytes=2391\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=112\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7754387456\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nileshbhoyar/tmp/mrjob/similarity.nileshbhoyar.20170623.130728.790107/output...\n",
      "Removing HDFS temp directory hdfs:///user/nileshbhoyar/tmp/mrjob/similarity.nileshbhoyar.20170623.130728.790107...\n",
      "Removing temp directory /tmp/similarity.nileshbhoyar.20170623.130728.790107...\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop systems_test_index_1 --cmdenv PATH=/opt/anaconda/bin:$PATH  > systems_test_similarities_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/similarity.nileshbhoyar.20170623.130845.345104\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nileshbhoyar/tmp/mrjob/similarity.nileshbhoyar.20170623.130845.345104/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob4878511891400749099.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_2478\n",
      "  Submitted application application_1497906899862_2478\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_2478/\n",
      "  Running job: job_1497906899862_2478\n",
      "  Job job_1497906899862_2478 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_2478 completed successfully\n",
      "  Output directory: hdfs:///user/nileshbhoyar/tmp/mrjob/similarity.nileshbhoyar.20170623.130845.345104/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=167\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=236\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=123\n",
      "\t\tFILE: Number of bytes written=401284\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=547\n",
      "\t\tHDFS: Number of bytes written=236\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=75010560\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9515520\n",
      "\t\tTotal time spent by all map tasks (ms)=48835\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=146505\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3717\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=18585\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=48835\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3717\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3300\n",
      "\t\tCombine input records=18\n",
      "\t\tCombine output records=8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=252\n",
      "\t\tInput split bytes=380\n",
      "\t\tMap input records=5\n",
      "\t\tMap output bytes=299\n",
      "\t\tMap output materialized bytes=176\n",
      "\t\tMap output records=18\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1921343488\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=176\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=16\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7754878976\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nileshbhoyar/tmp/mrjob/similarity.nileshbhoyar.20170623.130845.345104/output...\n",
      "Removing HDFS temp directory hdfs:///user/nileshbhoyar/tmp/mrjob/similarity.nileshbhoyar.20170623.130845.345104...\n",
      "Removing temp directory /tmp/similarity.nileshbhoyar.20170623.130845.345104...\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop systems_test_index_3 --cmdenv PATH=/opt/anaconda/bin:$PATH > systems_test_similarities_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/similarity.nileshbhoyar.20170623.131246.496488\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nileshbhoyar/tmp/mrjob/similarity.nileshbhoyar.20170623.131246.496488/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob1668193155493510697.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497906899862_2480\n",
      "  Submitted application application_1497906899862_2480\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_2480/\n",
      "  Running job: job_1497906899862_2480\n",
      "  Job job_1497906899862_2480 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_2480 completed successfully\n",
      "  Output directory: hdfs:///user/nileshbhoyar/tmp/mrjob/similarity.nileshbhoyar.20170623.131246.496488/output\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=206\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=375\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=157\n",
      "\t\tFILE: Number of bytes written=401358\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=586\n",
      "\t\tHDFS: Number of bytes written=375\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=95953920\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17264640\n",
      "\t\tTotal time spent by all map tasks (ms)=62470\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=187410\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6744\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=33720\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=62470\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6744\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=4400\n",
      "\t\tCombine input records=20\n",
      "\t\tCombine output records=9\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=315\n",
      "\t\tInput split bytes=380\n",
      "\t\tMap input records=4\n",
      "\t\tMap output bytes=380\n",
      "\t\tMap output materialized bytes=216\n",
      "\t\tMap output records=20\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=1930522624\n",
      "\t\tReduce input groups=5\n",
      "\t\tReduce input records=9\n",
      "\t\tReduce output records=6\n",
      "\t\tReduce shuffle bytes=216\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=18\n",
      "\t\tTotal committed heap usage (bytes)=5218762752\n",
      "\t\tVirtual memory (bytes) snapshot=7780347904\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nileshbhoyar/tmp/mrjob/similarity.nileshbhoyar.20170623.131246.496488/output...\n",
      "Removing HDFS temp directory hdfs:///user/nileshbhoyar/tmp/mrjob/similarity.nileshbhoyar.20170623.131246.496488...\n",
      "Removing temp directory /tmp/similarity.nileshbhoyar.20170623.131246.496488...\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop systems_test_index_2 --cmdenv PATH=/opt/anaconda/bin:$PATH > systems_test_similarities_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  3  - Similarity measures\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "           pair |        jaccard|           Dice\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "[\"DocA\", \"DocB\"] |       0.666667|       0.800000\n",
      "[\"DocA\", \"DocC\"] |       0.400000|       0.571429\n",
      "[\"DocB\", \"DocC\"] |       0.200000|       0.333333\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  2  - Similarity measures\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "           pair |        jaccard|           Dice\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "[\"atlas\", \"dipped\"] |       0.250000|       0.400000\n",
      "[\"atlas\", \"boon\"] |       0.250000|       0.400000\n",
      "[\"atlas\", \"cava\"] |       1.000000|       1.000000\n",
      "[\"boon\", \"dipped\"] |       0.500000|       0.666667\n",
      "[\"boon\", \"cava\"] |       0.250000|       0.400000\n",
      "[\"cava\", \"dipped\"] |       0.250000|       0.400000\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  1  - Similarity measures\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "           pair |        jaccard|           Dice\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "[\"a\", \"limited\"] |       0.107143|       0.193548\n",
      "   [\"a\", \"sea\"] |       0.107143|       0.193548\n",
      "[\"a\", \"general\"] |       0.107143|       0.193548\n",
      "[\"a\", \"female\"] |       0.107143|       0.193548\n",
      "    [\"a\", \"in\"] |       0.214286|       0.352941\n",
      "[\"a\", \"religious\"] |       0.107143|       0.193548\n",
      "[\"a\", \"george\"] |       0.107143|       0.193548\n",
      "[\"a\", \"biography\"] |       0.107143|       0.193548\n",
      "  [\"a\", \"city\"] |       0.107143|       0.193548\n",
      "   [\"a\", \"for\"] |       0.107143|       0.193548\n",
      " [\"a\", \"tales\"] |       0.107143|       0.193548\n",
      "[\"a\", \"government\"] |       0.107143|       0.193548\n",
      "   [\"a\", \"the\"] |       0.214286|       0.352941\n",
      " [\"a\", \"forms\"] |       0.071429|       0.133333\n",
      " [\"a\", \"wales\"] |       0.107143|       0.193548\n",
      "[\"a\", \"christmas\"] |       0.107143|       0.193548\n",
      "[\"a\", \"child's\"] |       0.107143|       0.193548\n",
      "[\"a\", \"collection\"] |       0.142857|       0.250000\n",
      "    [\"a\", \"by\"] |       0.107143|       0.193548\n",
      "  [\"a\", \"case\"] |       0.214286|       0.352941\n",
      "[\"a\", \"circumstantial\"] |       0.107143|       0.193548\n",
      "    [\"a\", \"of\"] |       0.535714|       0.697674\n",
      " [\"a\", \"study\"] |       0.214286|       0.352941\n",
      "  [\"a\", \"bill\"] |       0.107143|       0.193548\n",
      "[\"a\", \"establishing\"] |       0.107143|       0.193548\n",
      "[\"a\", \"narrative\"] |       0.107143|       0.193548\n",
      " [\"a\", \"fairy\"] |       0.107143|       0.193548\n",
      "[\"bill\", \"limited\"] |       0.142857|       0.250000\n",
      "[\"bill\", \"sea\"] |       0.142857|       0.250000\n",
      "[\"bill\", \"general\"] |       0.142857|       0.250000\n",
      "[\"bill\", \"female\"] |       0.142857|       0.250000\n",
      " [\"bill\", \"in\"] |       0.100000|       0.181818\n",
      "[\"bill\", \"religious\"] |       0.600000|       0.750000\n",
      "[\"bill\", \"george\"] |       0.142857|       0.250000\n",
      "[\"bill\", \"biography\"] |       0.142857|       0.250000\n",
      "[\"bill\", \"city\"] |       0.142857|       0.250000\n",
      "[\"bill\", \"for\"] |       0.600000|       0.750000\n",
      "[\"bill\", \"tales\"] |       0.142857|       0.250000\n",
      "[\"bill\", \"government\"] |       0.142857|       0.250000\n",
      "[\"bill\", \"the\"] |       0.100000|       0.181818\n",
      "[\"bill\", \"forms\"] |       0.166667|       0.285714\n",
      "[\"bill\", \"wales\"] |       0.142857|       0.250000\n",
      "[\"bill\", \"christmas\"] |       0.142857|       0.250000\n",
      "[\"bill\", \"child's\"] |       0.142857|       0.250000\n",
      "[\"bill\", \"collection\"] |       0.125000|       0.222222\n",
      " [\"bill\", \"by\"] |       0.142857|       0.250000\n",
      "[\"bill\", \"case\"] |       0.100000|       0.181818\n",
      "[\"bill\", \"circumstantial\"] |       0.142857|       0.250000\n",
      " [\"bill\", \"of\"] |       0.052632|       0.100000\n",
      "[\"bill\", \"study\"] |       0.100000|       0.181818\n",
      "[\"bill\", \"establishing\"] |       0.600000|       0.750000\n",
      "[\"bill\", \"narrative\"] |       0.142857|       0.250000\n",
      "[\"bill\", \"fairy\"] |       0.142857|       0.250000\n",
      "[\"biography\", \"limited\"] |       0.333333|       0.500000\n",
      "[\"biography\", \"female\"] |       0.333333|       0.500000\n",
      "[\"biography\", \"general\"] |       0.600000|       0.750000\n",
      "[\"biography\", \"sea\"] |       0.142857|       0.250000\n",
      "[\"biography\", \"in\"] |       0.100000|       0.181818\n",
      "[\"biography\", \"religious\"] |       0.142857|       0.250000\n",
      "[\"biography\", \"george\"] |       0.600000|       0.750000\n",
      "[\"biography\", \"city\"] |       0.142857|       0.250000\n",
      "[\"biography\", \"for\"] |       0.142857|       0.250000\n",
      "[\"biography\", \"tales\"] |       0.333333|       0.500000\n",
      "[\"biography\", \"government\"] |       0.142857|       0.250000\n",
      "[\"biography\", \"the\"] |       0.222222|       0.363636\n",
      "[\"biography\", \"forms\"] |       0.400000|       0.571429\n",
      "[\"biography\", \"wales\"] |       0.142857|       0.250000\n",
      "[\"biography\", \"christmas\"] |       0.142857|       0.250000\n",
      "[\"biography\", \"child's\"] |       0.142857|       0.250000\n",
      "[\"biography\", \"collection\"] |       0.285714|       0.444444\n",
      "[\"biography\", \"by\"] |       0.142857|       0.250000\n",
      "[\"biography\", \"case\"] |       0.222222|       0.363636\n",
      "[\"biography\", \"circumstantial\"] |       0.333333|       0.500000\n",
      "[\"biography\", \"of\"] |       0.250000|       0.400000\n",
      "[\"biography\", \"study\"] |       0.222222|       0.363636\n",
      "[\"biography\", \"establishing\"] |       0.142857|       0.250000\n",
      "[\"biography\", \"narrative\"] |       0.333333|       0.500000\n",
      "[\"biography\", \"fairy\"] |       0.333333|       0.500000\n",
      "[\"by\", \"limited\"] |       0.142857|       0.250000\n",
      "  [\"by\", \"sea\"] |       0.600000|       0.750000\n",
      "[\"by\", \"general\"] |       0.142857|       0.250000\n",
      "[\"by\", \"female\"] |       0.142857|       0.250000\n",
      "   [\"by\", \"in\"] |       0.100000|       0.181818\n",
      "[\"by\", \"religious\"] |       0.142857|       0.250000\n",
      "[\"by\", \"george\"] |       0.142857|       0.250000\n",
      " [\"by\", \"city\"] |       0.600000|       0.750000\n",
      "  [\"by\", \"for\"] |       0.142857|       0.250000\n",
      "[\"by\", \"tales\"] |       0.142857|       0.250000\n",
      "[\"by\", \"child's\"] |       0.142857|       0.250000\n",
      "[\"by\", \"forms\"] |       0.166667|       0.285714\n",
      "[\"by\", \"wales\"] |       0.142857|       0.250000\n",
      "[\"by\", \"christmas\"] |       0.142857|       0.250000\n",
      "[\"by\", \"government\"] |       0.142857|       0.250000\n",
      "[\"by\", \"collection\"] |       0.125000|       0.222222\n",
      "  [\"by\", \"the\"] |       0.375000|       0.545455\n",
      " [\"by\", \"case\"] |       0.100000|       0.181818\n",
      "[\"by\", \"circumstantial\"] |       0.333333|       0.500000\n",
      "   [\"by\", \"of\"] |       0.111111|       0.200000\n",
      "[\"by\", \"study\"] |       0.100000|       0.181818\n",
      "[\"by\", \"establishing\"] |       0.142857|       0.250000\n",
      "[\"by\", \"narrative\"] |       0.333333|       0.500000\n",
      "[\"by\", \"fairy\"] |       0.142857|       0.250000\n",
      "[\"case\", \"limited\"] |       0.375000|       0.545455\n",
      "[\"case\", \"female\"] |       0.375000|       0.545455\n",
      "[\"case\", \"general\"] |       0.222222|       0.363636\n",
      "[\"case\", \"sea\"] |       0.100000|       0.181818\n",
      " [\"case\", \"in\"] |       0.272727|       0.428571\n",
      "[\"case\", \"religious\"] |       0.100000|       0.181818\n",
      "[\"case\", \"george\"] |       0.222222|       0.363636\n",
      "[\"case\", \"city\"] |       0.100000|       0.181818\n",
      "[\"case\", \"for\"] |       0.100000|       0.181818\n",
      "[\"case\", \"tales\"] |       0.222222|       0.363636\n",
      "[\"case\", \"child's\"] |       0.222222|       0.363636\n",
      "[\"case\", \"forms\"] |       0.250000|       0.400000\n",
      "[\"case\", \"wales\"] |       0.222222|       0.363636\n",
      "[\"case\", \"christmas\"] |       0.222222|       0.363636\n",
      "[\"case\", \"government\"] |       0.375000|       0.545455\n",
      "[\"case\", \"collection\"] |       0.200000|       0.333333\n",
      "[\"case\", \"the\"] |       0.166667|       0.285714\n",
      "[\"case\", \"circumstantial\"] |       0.222222|       0.363636\n",
      " [\"case\", \"of\"] |       0.277778|       0.434783\n",
      "[\"case\", \"study\"] |       0.750000|       0.857143\n",
      "[\"case\", \"establishing\"] |       0.100000|       0.181818\n",
      "[\"case\", \"narrative\"] |       0.222222|       0.363636\n",
      "[\"case\", \"fairy\"] |       0.222222|       0.363636\n",
      "[\"child's\", \"limited\"] |       0.142857|       0.250000\n",
      "[\"child's\", \"female\"] |       0.142857|       0.250000\n",
      "[\"child's\", \"general\"] |       0.142857|       0.250000\n",
      "[\"child's\", \"sea\"] |       0.142857|       0.250000\n",
      "[\"child's\", \"in\"] |       0.375000|       0.545455\n",
      "[\"child's\", \"religious\"] |       0.142857|       0.250000\n",
      "[\"child's\", \"george\"] |       0.142857|       0.250000\n",
      "[\"child's\", \"city\"] |       0.142857|       0.250000\n",
      "[\"child's\", \"for\"] |       0.142857|       0.250000\n",
      "[\"child's\", \"tales\"] |       0.142857|       0.250000\n",
      "[\"child's\", \"forms\"] |       0.166667|       0.285714\n",
      "[\"child's\", \"wales\"] |       0.600000|       0.750000\n",
      "[\"child's\", \"christmas\"] |       0.600000|       0.750000\n",
      "[\"child's\", \"government\"] |       0.333333|       0.500000\n",
      "[\"child's\", \"collection\"] |       0.125000|       0.222222\n",
      "[\"child's\", \"fairy\"] |       0.142857|       0.250000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"child's\", \"circumstantial\"] |       0.142857|       0.250000\n",
      "[\"child's\", \"of\"] |       0.052632|       0.100000\n",
      "[\"child's\", \"study\"] |       0.222222|       0.363636\n",
      "[\"child's\", \"establishing\"] |       0.142857|       0.250000\n",
      "[\"child's\", \"narrative\"] |       0.142857|       0.250000\n",
      "[\"child's\", \"the\"] |       0.100000|       0.181818\n",
      "[\"christmas\", \"limited\"] |       0.142857|       0.250000\n",
      "[\"christmas\", \"circumstantial\"] |       0.142857|       0.250000\n",
      "[\"christmas\", \"sea\"] |       0.142857|       0.250000\n",
      "[\"christmas\", \"for\"] |       0.142857|       0.250000\n",
      "[\"christmas\", \"city\"] |       0.142857|       0.250000\n",
      "[\"christmas\", \"of\"] |       0.052632|       0.100000\n",
      "[\"christmas\", \"study\"] |       0.222222|       0.363636\n",
      "[\"christmas\", \"government\"] |       0.333333|       0.500000\n",
      "[\"christmas\", \"collection\"] |       0.125000|       0.222222\n",
      "[\"christmas\", \"establishing\"] |       0.142857|       0.250000\n",
      "[\"christmas\", \"forms\"] |       0.166667|       0.285714\n",
      "[\"christmas\", \"narrative\"] |       0.142857|       0.250000\n",
      "[\"christmas\", \"tales\"] |       0.142857|       0.250000\n",
      "[\"christmas\", \"female\"] |       0.142857|       0.250000\n",
      "[\"christmas\", \"in\"] |       0.375000|       0.545455\n",
      "[\"christmas\", \"wales\"] |       0.600000|       0.750000\n",
      "[\"christmas\", \"the\"] |       0.100000|       0.181818\n",
      "[\"christmas\", \"general\"] |       0.142857|       0.250000\n",
      "[\"christmas\", \"religious\"] |       0.142857|       0.250000\n",
      "[\"christmas\", \"george\"] |       0.142857|       0.250000\n",
      "[\"christmas\", \"fairy\"] |       0.142857|       0.250000\n",
      "[\"circumstantial\", \"city\"] |       0.333333|       0.500000\n",
      "[\"circumstantial\", \"limited\"] |       0.333333|       0.500000\n",
      "[\"circumstantial\", \"sea\"] |       0.333333|       0.500000\n",
      "[\"circumstantial\", \"for\"] |       0.142857|       0.250000\n",
      "[\"circumstantial\", \"government\"] |       0.142857|       0.250000\n",
      "[\"circumstantial\", \"of\"] |       0.250000|       0.400000\n",
      "[\"circumstantial\", \"study\"] |       0.222222|       0.363636\n",
      "[\"circumstantial\", \"narrative\"] |       0.600000|       0.750000\n",
      "[\"circumstantial\", \"collection\"] |       0.285714|       0.444444\n",
      "[\"circumstantial\", \"establishing\"] |       0.142857|       0.250000\n",
      "[\"circumstantial\", \"forms\"] |       0.400000|       0.571429\n",
      "[\"circumstantial\", \"tales\"] |       0.333333|       0.500000\n",
      "[\"circumstantial\", \"female\"] |       0.333333|       0.500000\n",
      "[\"circumstantial\", \"in\"] |       0.100000|       0.181818\n",
      "[\"circumstantial\", \"wales\"] |       0.142857|       0.250000\n",
      "[\"circumstantial\", \"the\"] |       0.375000|       0.545455\n",
      "[\"circumstantial\", \"general\"] |       0.333333|       0.500000\n",
      "[\"circumstantial\", \"religious\"] |       0.142857|       0.250000\n",
      "[\"circumstantial\", \"george\"] |       0.333333|       0.500000\n",
      "[\"circumstantial\", \"fairy\"] |       0.333333|       0.500000\n",
      "[\"city\", \"limited\"] |       0.142857|       0.250000\n",
      "[\"city\", \"sea\"] |       0.600000|       0.750000\n",
      "[\"city\", \"for\"] |       0.142857|       0.250000\n",
      "[\"city\", \"government\"] |       0.142857|       0.250000\n",
      " [\"city\", \"of\"] |       0.111111|       0.200000\n",
      "[\"city\", \"study\"] |       0.100000|       0.181818\n",
      "[\"city\", \"narrative\"] |       0.333333|       0.500000\n",
      "[\"city\", \"collection\"] |       0.125000|       0.222222\n",
      "[\"city\", \"establishing\"] |       0.142857|       0.250000\n",
      "[\"city\", \"forms\"] |       0.166667|       0.285714\n",
      "[\"city\", \"tales\"] |       0.142857|       0.250000\n",
      "[\"city\", \"female\"] |       0.142857|       0.250000\n",
      " [\"city\", \"in\"] |       0.100000|       0.181818\n",
      "[\"city\", \"wales\"] |       0.142857|       0.250000\n",
      "[\"city\", \"the\"] |       0.375000|       0.545455\n",
      "[\"city\", \"general\"] |       0.142857|       0.250000\n",
      "[\"city\", \"religious\"] |       0.142857|       0.250000\n",
      "[\"city\", \"george\"] |       0.142857|       0.250000\n",
      "[\"city\", \"fairy\"] |       0.142857|       0.250000\n",
      "[\"collection\", \"limited\"] |       0.285714|       0.444444\n",
      "[\"collection\", \"sea\"] |       0.125000|       0.222222\n",
      "[\"collection\", \"for\"] |       0.125000|       0.222222\n",
      "[\"collection\", \"government\"] |       0.125000|       0.222222\n",
      "[\"collection\", \"of\"] |       0.312500|       0.476190\n",
      "[\"collection\", \"tales\"] |       0.500000|       0.666667\n",
      "[\"collection\", \"narrative\"] |       0.285714|       0.444444\n",
      "[\"collection\", \"religious\"] |       0.125000|       0.222222\n",
      "[\"collection\", \"general\"] |       0.285714|       0.444444\n",
      "[\"collection\", \"forms\"] |       0.333333|       0.500000\n",
      "[\"collection\", \"female\"] |       0.285714|       0.444444\n",
      "[\"collection\", \"in\"] |       0.090909|       0.166667\n",
      "[\"collection\", \"study\"] |       0.200000|       0.333333\n",
      "[\"collection\", \"wales\"] |       0.125000|       0.222222\n",
      "[\"collection\", \"establishing\"] |       0.125000|       0.222222\n",
      "[\"collection\", \"the\"] |       0.200000|       0.333333\n",
      "[\"collection\", \"george\"] |       0.285714|       0.444444\n",
      "[\"collection\", \"fairy\"] |       0.500000|       0.666667\n",
      "[\"establishing\", \"limited\"] |       0.142857|       0.250000\n",
      "[\"establishing\", \"sea\"] |       0.142857|       0.250000\n",
      "[\"establishing\", \"for\"] |       0.600000|       0.750000\n",
      "[\"establishing\", \"government\"] |       0.142857|       0.250000\n",
      "[\"establishing\", \"of\"] |       0.052632|       0.100000\n",
      "[\"establishing\", \"study\"] |       0.100000|       0.181818\n",
      "[\"establishing\", \"narrative\"] |       0.142857|       0.250000\n",
      "[\"establishing\", \"general\"] |       0.142857|       0.250000\n",
      "[\"establishing\", \"forms\"] |       0.166667|       0.285714\n",
      "[\"establishing\", \"tales\"] |       0.142857|       0.250000\n",
      "[\"establishing\", \"female\"] |       0.142857|       0.250000\n",
      "[\"establishing\", \"in\"] |       0.100000|       0.181818\n",
      "[\"establishing\", \"wales\"] |       0.142857|       0.250000\n",
      "[\"establishing\", \"the\"] |       0.100000|       0.181818\n",
      "[\"establishing\", \"religious\"] |       0.600000|       0.750000\n",
      "[\"establishing\", \"george\"] |       0.142857|       0.250000\n",
      "[\"establishing\", \"fairy\"] |       0.142857|       0.250000\n",
      "[\"fairy\", \"limited\"] |       0.333333|       0.500000\n",
      "[\"fairy\", \"sea\"] |       0.142857|       0.250000\n",
      "[\"fairy\", \"for\"] |       0.142857|       0.250000\n",
      "[\"fairy\", \"government\"] |       0.142857|       0.250000\n",
      "[\"fairy\", \"of\"] |       0.250000|       0.400000\n",
      "[\"fairy\", \"tales\"] |       0.600000|       0.750000\n",
      "[\"fairy\", \"narrative\"] |       0.333333|       0.500000\n",
      "[\"fairy\", \"religious\"] |       0.142857|       0.250000\n",
      "[\"fairy\", \"general\"] |       0.333333|       0.500000\n",
      "[\"fairy\", \"forms\"] |       0.750000|       0.857143\n",
      "[\"fairy\", \"female\"] |       0.333333|       0.500000\n",
      "[\"fairy\", \"in\"] |       0.100000|       0.181818\n",
      "[\"fairy\", \"study\"] |       0.222222|       0.363636\n",
      "[\"fairy\", \"wales\"] |       0.142857|       0.250000\n",
      "[\"fairy\", \"the\"] |       0.222222|       0.363636\n",
      "[\"fairy\", \"george\"] |       0.333333|       0.500000\n",
      "[\"female\", \"limited\"] |       1.000000|       1.000000\n",
      "[\"female\", \"for\"] |       0.142857|       0.250000\n",
      "[\"female\", \"government\"] |       0.600000|       0.750000\n",
      "[\"female\", \"of\"] |       0.250000|       0.400000\n",
      "[\"female\", \"study\"] |       0.375000|       0.545455\n",
      "[\"female\", \"narrative\"] |       0.333333|       0.500000\n",
      "[\"female\", \"general\"] |       0.333333|       0.500000\n",
      "[\"female\", \"forms\"] |       0.400000|       0.571429\n",
      "[\"female\", \"tales\"] |       0.333333|       0.500000\n",
      "[\"female\", \"sea\"] |       0.142857|       0.250000\n",
      "[\"female\", \"in\"] |       0.375000|       0.545455\n",
      "[\"female\", \"wales\"] |       0.142857|       0.250000\n",
      "[\"female\", \"the\"] |       0.222222|       0.363636\n",
      "[\"female\", \"religious\"] |       0.142857|       0.250000\n",
      "[\"female\", \"george\"] |       0.333333|       0.500000\n",
      "[\"for\", \"limited\"] |       0.142857|       0.250000\n",
      "[\"for\", \"government\"] |       0.142857|       0.250000\n",
      "  [\"for\", \"of\"] |       0.052632|       0.100000\n",
      "[\"for\", \"study\"] |       0.100000|       0.181818\n",
      "[\"for\", \"narrative\"] |       0.142857|       0.250000\n",
      "[\"for\", \"general\"] |       0.142857|       0.250000\n",
      "[\"for\", \"forms\"] |       0.166667|       0.285714\n",
      "[\"for\", \"tales\"] |       0.142857|       0.250000\n",
      " [\"for\", \"sea\"] |       0.142857|       0.250000\n",
      "  [\"for\", \"in\"] |       0.100000|       0.181818\n",
      "[\"for\", \"wales\"] |       0.142857|       0.250000\n",
      " [\"for\", \"the\"] |       0.100000|       0.181818\n",
      "[\"for\", \"religious\"] |       0.600000|       0.750000\n",
      "[\"for\", \"george\"] |       0.142857|       0.250000\n",
      "[\"forms\", \"limited\"] |       0.400000|       0.571429\n",
      "[\"forms\", \"government\"] |       0.166667|       0.285714\n",
      "[\"forms\", \"of\"] |       0.187500|       0.315789\n",
      "[\"forms\", \"study\"] |       0.250000|       0.400000\n",
      "[\"forms\", \"in\"] |       0.111111|       0.200000\n",
      "[\"forms\", \"general\"] |       0.400000|       0.571429\n",
      "[\"forms\", \"tales\"] |       0.750000|       0.857143\n",
      "[\"forms\", \"sea\"] |       0.166667|       0.285714\n",
      "[\"forms\", \"narrative\"] |       0.400000|       0.571429\n",
      "[\"forms\", \"wales\"] |       0.166667|       0.285714\n",
      "[\"forms\", \"the\"] |       0.250000|       0.400000\n",
      "[\"forms\", \"religious\"] |       0.166667|       0.285714\n",
      "[\"forms\", \"george\"] |       0.400000|       0.571429\n",
      "[\"general\", \"limited\"] |       0.333333|       0.500000\n",
      "[\"general\", \"government\"] |       0.142857|       0.250000\n",
      "[\"general\", \"of\"] |       0.250000|       0.400000\n",
      "[\"general\", \"study\"] |       0.222222|       0.363636\n",
      "[\"general\", \"in\"] |       0.100000|       0.181818\n",
      "[\"general\", \"tales\"] |       0.333333|       0.500000\n",
      "[\"general\", \"sea\"] |       0.142857|       0.250000\n",
      "[\"general\", \"narrative\"] |       0.333333|       0.500000\n",
      "[\"general\", \"wales\"] |       0.142857|       0.250000\n",
      "[\"general\", \"the\"] |       0.222222|       0.363636\n",
      "[\"general\", \"religious\"] |       0.142857|       0.250000\n",
      "[\"general\", \"george\"] |       0.600000|       0.750000\n",
      "[\"george\", \"limited\"] |       0.333333|       0.500000\n",
      "[\"george\", \"government\"] |       0.142857|       0.250000\n",
      "[\"george\", \"of\"] |       0.250000|       0.400000\n",
      "[\"george\", \"study\"] |       0.222222|       0.363636\n",
      "[\"george\", \"in\"] |       0.100000|       0.181818\n",
      "[\"george\", \"sea\"] |       0.142857|       0.250000\n",
      "[\"george\", \"narrative\"] |       0.333333|       0.500000\n",
      "[\"george\", \"wales\"] |       0.142857|       0.250000\n",
      "[\"george\", \"the\"] |       0.222222|       0.363636\n",
      "[\"george\", \"religious\"] |       0.142857|       0.250000\n",
      "[\"george\", \"tales\"] |       0.333333|       0.500000\n",
      "[\"government\", \"limited\"] |       0.600000|       0.750000\n",
      "[\"government\", \"of\"] |       0.176471|       0.300000\n",
      "[\"government\", \"study\"] |       0.375000|       0.545455\n",
      "[\"government\", \"in\"] |       0.375000|       0.545455\n",
      "[\"government\", \"sea\"] |       0.142857|       0.250000\n",
      "[\"government\", \"narrative\"] |       0.142857|       0.250000\n",
      "[\"government\", \"wales\"] |       0.333333|       0.500000\n",
      "[\"government\", \"the\"] |       0.100000|       0.181818\n",
      "[\"government\", \"religious\"] |       0.142857|       0.250000\n",
      "[\"government\", \"tales\"] |       0.142857|       0.250000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"in\", \"limited\"] |       0.375000|       0.545455\n",
      "   [\"in\", \"of\"] |       0.150000|       0.260870\n",
      "[\"in\", \"study\"] |       0.272727|       0.428571\n",
      "  [\"in\", \"sea\"] |       0.100000|       0.181818\n",
      "[\"in\", \"narrative\"] |       0.100000|       0.181818\n",
      "[\"in\", \"wales\"] |       0.375000|       0.545455\n",
      "  [\"in\", \"the\"] |       0.076923|       0.142857\n",
      "[\"in\", \"religious\"] |       0.100000|       0.181818\n",
      "[\"in\", \"tales\"] |       0.100000|       0.181818\n",
      "[\"limited\", \"of\"] |       0.250000|       0.400000\n",
      "[\"limited\", \"study\"] |       0.375000|       0.545455\n",
      "[\"limited\", \"sea\"] |       0.142857|       0.250000\n",
      "[\"limited\", \"narrative\"] |       0.333333|       0.500000\n",
      "[\"limited\", \"wales\"] |       0.142857|       0.250000\n",
      "[\"limited\", \"the\"] |       0.222222|       0.363636\n",
      "[\"limited\", \"religious\"] |       0.142857|       0.250000\n",
      "[\"limited\", \"tales\"] |       0.333333|       0.500000\n",
      "[\"narrative\", \"of\"] |       0.250000|       0.400000\n",
      "[\"narrative\", \"study\"] |       0.222222|       0.363636\n",
      "[\"narrative\", \"sea\"] |       0.333333|       0.500000\n",
      "[\"narrative\", \"wales\"] |       0.142857|       0.250000\n",
      "[\"narrative\", \"the\"] |       0.375000|       0.545455\n",
      "[\"narrative\", \"religious\"] |       0.142857|       0.250000\n",
      "[\"narrative\", \"tales\"] |       0.333333|       0.500000\n",
      "[\"of\", \"study\"] |       0.277778|       0.434783\n",
      "  [\"of\", \"sea\"] |       0.111111|       0.200000\n",
      "[\"of\", \"wales\"] |       0.052632|       0.100000\n",
      "  [\"of\", \"the\"] |       0.210526|       0.347826\n",
      "[\"of\", \"religious\"] |       0.052632|       0.100000\n",
      "[\"of\", \"tales\"] |       0.250000|       0.400000\n",
      "[\"religious\", \"wales\"] |       0.142857|       0.250000\n",
      "[\"religious\", \"the\"] |       0.100000|       0.181818\n",
      "[\"religious\", \"study\"] |       0.100000|       0.181818\n",
      "[\"religious\", \"sea\"] |       0.142857|       0.250000\n",
      "[\"religious\", \"tales\"] |       0.142857|       0.250000\n",
      "[\"sea\", \"wales\"] |       0.142857|       0.250000\n",
      " [\"sea\", \"the\"] |       0.375000|       0.545455\n",
      "[\"sea\", \"study\"] |       0.100000|       0.181818\n",
      "[\"sea\", \"tales\"] |       0.142857|       0.250000\n",
      "[\"study\", \"wales\"] |       0.222222|       0.363636\n",
      "[\"study\", \"tales\"] |       0.222222|       0.363636\n",
      "[\"study\", \"the\"] |       0.166667|       0.285714\n",
      "[\"tales\", \"wales\"] |       0.142857|       0.250000\n",
      "[\"tales\", \"the\"] |       0.222222|       0.363636\n",
      "[\"the\", \"wales\"] |       0.100000|       0.181818\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Pretty print systems tests\n",
    "############################################\n",
    "\n",
    "import json\n",
    "for i in range(3,0,-1):\n",
    "  print '—'*110\n",
    "  print \"Systems test \",i,\" - Similarity measures\"\n",
    "  print '—'*110\n",
    "  print \"{0:>15} |{1:>15}|{2:>15}\".format(\n",
    "           \"pair\",  \"jaccard\",\"Dice\")\n",
    "  print '-'*110\n",
    "\n",
    "  with open(\"systems_test_similarities_\"+str(i),\"r\") as f:\n",
    "      lines = f.readlines()\n",
    "      for line in lines:\n",
    "          line = line.strip()\n",
    "          pair,stripe = line.split(\"\\t\")\n",
    "          stripe = json.loads(stripe)\n",
    "     \n",
    "          print \"{0:>15} |{1:>15f}|{2:>15f}\".format(\n",
    "              pair,float(stripe['jaccard']),float(stripe['dice'] ))\n",
    "#          print \"{0:>15f} |{1:>15} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "#               float(avg), stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]))\n",
    "          #print \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.4.1 <a name=\"5.4.1\"></a>Full-scale experiment: EDA of Google n-grams dataset (PHASE 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- A. Longest 5-gram (number of characters)\n",
    "- B. Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- C. 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- D. Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.4.1 - A. Longest 5-gram (number of characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing longest5gram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile longest5gram.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "class longest5gram(MRJob):\n",
    "    \n",
    "    # START STUDENT CODE 5.4.1.A\n",
    "    \n",
    "\n",
    "    def mapper(self, _,recs):\n",
    "        fields = recs.split(\"\\t\")\n",
    "        ngram_length = len(fields[0])\n",
    "        yield None, (ngram_length, fields[0])\n",
    "\n",
    "   \n",
    "    def combiner(self, _, pairs):\n",
    "        yield None, self.get_longest_ngram(pairs)\n",
    "\n",
    "   \n",
    "    def reducer(self, _, pairs):\n",
    "        ngram_length, ngram = self.get_longest_ngram(pairs)\n",
    "        yield None,(ngram_length, ngram)\n",
    "    def reducer_second(self, _,pairs):\n",
    "        ngram_length, ngram = self.get_longest_ngram(pairs)\n",
    "        yield ngram_length, ngram\n",
    "        #yield None,pairs\n",
    "    \n",
    "    def get_longest_ngram(self, pairs):\n",
    "        longest_ngram = None\n",
    "        longest_ngram_length = 0\n",
    "\n",
    "        for ngram_length, ngram in pairs:\n",
    "            if ngram_length > longest_ngram_length:\n",
    "                longest_ngram = ngram\n",
    "                longest_ngram_length = ngram_length\n",
    "            elif ngram_length == longest_ngram_length and ngram < longest_ngram:\n",
    "                longest_ngram = ngram\n",
    "\n",
    "        return longest_ngram_length, longest_ngram\n",
    "    def steps(self):  #pipeline of Map-Reduce jobs\n",
    "     \n",
    "        return [\n",
    "            MRStep( \n",
    "                    mapper=self.mapper,       \n",
    "                    combiner=self.combiner,\n",
    "                    reducer=self.reducer,\n",
    "                    jobconf = {\n",
    "                    'mapreduce.job.reduces': 5,\n",
    "                    'mapreduce.reduce.cpu.vcores':8\n",
    "                }\n",
    "                \n",
    "            ),\n",
    "            MRStep(reducer=self.reducer_second,\n",
    "                  jobconf = {\n",
    "                    'mapreduce.job.reduces': 1,\n",
    "                    'mapreduce.reduce.cpu.vcores':8\n",
    "                }\n",
    "                  ) # Step 2: most frequent word\n",
    "        ]\n",
    "\n",
    "\n",
    "    # END STUDENT CODE 5.4.1.A\n",
    "    \n",
    "if __name__ == '__main__' and sys.argv[0].find('ipykernel') == -1:\n",
    "    longest5gram().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/longest5gram.nileshbhoyar.20170623.131503.863777\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/longest5gram.nileshbhoyar.20170623.131503.863777/output...\n",
      "33\t\"A BILL FOR ESTABLISHING RELIGIOUS\"\n",
      "Removing temp directory /tmp/longest5gram.nileshbhoyar.20170623.131503.863777...\n",
      "CPU times: user 33 ms, sys: 5 ms, total: 38 ms\n",
      "Wall time: 1.24 s\n"
     ]
    }
   ],
   "source": [
    "#!python longest5gram.py -r hadoop \"hdfs:///user/cendylin/filtered-5Grams/*.txt\" > outputlongest.txt\n",
    "%time !python longest5gram.py -r local \"googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/longest5gram.nileshbhoyar.20170623.131510.745767\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nileshbhoyar/tmp/mrjob/longest5gram.nileshbhoyar.20170623.131510.745767/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob2027824696222385117.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1497906899862_2482\n",
      "  Submitted application application_1497906899862_2482\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_2482/\n",
      "  Running job: job_1497906899862_2482\n",
      "  Job job_1497906899862_2482 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 4% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 15% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 19% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 29% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 44% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 46% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 76% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 79% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 86% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 89% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 94% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 96% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 60%\n",
      "   map 100% reduce 80%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_2482 completed successfully\n",
      "  Output directory: hdfs:///user/nileshbhoyar/tmp/mrjob/longest5gram.nileshbhoyar.20170623.131510.745767/step-output/0000\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=174\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=9018\n",
      "\t\tFILE: Number of bytes written=25934837\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2156101116\n",
      "\t\tHDFS: Number of bytes written=174\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=585\n",
      "\t\tHDFS: Number of write operations=10\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=3\n",
      "\t\tLaunched map tasks=193\n",
      "\t\tLaunched reduce tasks=5\n",
      "\t\tOther local map tasks=2\n",
      "\t\tRack-local map tasks=191\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18747078144\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=58260480\n",
      "\t\tTotal time spent by all map tasks (ms)=12205129\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=36615387\n",
      "\t\tTotal time spent by all reduce tasks (ms)=22758\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=113790\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12205129\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=182064\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=6374400\n",
      "\t\tCombine input records=58682266\n",
      "\t\tCombine output records=188\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=49938\n",
      "\t\tInput split bytes=32000\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=2331730493\n",
      "\t\tMap output materialized bytes=30109\n",
      "\t\tMap output records=58682266\n",
      "\t\tMerged Map outputs=950\n",
      "\t\tPhysical memory (bytes) snapshot=155563237376\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=188\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=30109\n",
      "\t\tShuffled Maps =950\n",
      "\t\tSpilled Records=376\n",
      "\t\tTotal committed heap usage (bytes)=308326957056\n",
      "\t\tVirtual memory (bytes) snapshot=434536435712\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob8859370033760825087.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 5\n",
      "  number of splits:6\n",
      "  Submitting tokens for job: job_1497906899862_2483\n",
      "  Submitted application application_1497906899862_2483\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_2483/\n",
      "  Running job: job_1497906899862_2483\n",
      "  Job job_1497906899862_2483 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_2483 completed successfully\n",
      "  Output directory: hdfs:///user/nileshbhoyar/tmp/mrjob/longest5gram.nileshbhoyar.20170623.131510.745767/output\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=261\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=166\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=200\n",
      "\t\tFILE: Number of bytes written=926584\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1419\n",
      "\t\tHDFS: Number of bytes written=166\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=21\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=6\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=4\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=156412416\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=29946880\n",
      "\t\tTotal time spent by all map tasks (ms)=101831\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=305493\n",
      "\t\tTotal time spent by all reduce tasks (ms)=11698\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=58490\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=101831\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=93584\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=8710\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=676\n",
      "\t\tInput split bytes=1158\n",
      "\t\tMap input records=1\n",
      "\t\tMap output bytes=175\n",
      "\t\tMap output materialized bytes=276\n",
      "\t\tMap output records=1\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tPhysical memory (bytes) snapshot=5088751616\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=276\n",
      "\t\tShuffled Maps =6\n",
      "\t\tSpilled Records=2\n",
      "\t\tTotal committed heap usage (bytes)=11489247232\n",
      "\t\tVirtual memory (bytes) snapshot=16522362880\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nileshbhoyar/tmp/mrjob/longest5gram.nileshbhoyar.20170623.131510.745767/output...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing HDFS temp directory hdfs:///user/nileshbhoyar/tmp/mrjob/longest5gram.nileshbhoyar.20170623.131510.745767...\n",
      "Removing temp directory /tmp/longest5gram.nileshbhoyar.20170623.131510.745767...\n",
      "CPU times: user 8.65 s, sys: 1.87 s, total: 10.5 s\n",
      "Wall time: 6min 22s\n"
     ]
    }
   ],
   "source": [
    "#!python longest5gram.py -r hadoop \"hdfs:///user/cendylin/filtered-5Grams/*.txt\" > outputlongest.txt\n",
    "%time !python longest5gram.py -r hadoop \"hdfs:///user/cendylin/filtered-5Grams/\" > outputlongest.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Stats: \n",
    "#### Step1\n",
    "##### No of maps 190\n",
    "##### No of Reducers  10\n",
    "#### Total time for haddop jobs 1mins, 19sec\n",
    "#### Step 2\n",
    "##### No of reducers 1\n",
    "##### No of maps 2\n",
    "##### Total time 17 sec\n",
    "\n",
    "Step 1\n",
    "http://rm-ia.s3s.altiscale.com:19888/jobhistory/job/job_1497906899862_1907\n",
    "\n",
    "Step 2\n",
    "http://rm-ia.s3s.altiscale.com:19888/jobhistory/job/job_1497906899862_1909"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\t\"AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat outputlongest.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/nileshbhoyar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.4.1 - B. Top 10 most frequent words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mostFrequentWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mostFrequentWords.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "#from mrjob.protocol import  RawValueProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "class mostFrequentWords(MRJob):\n",
    "    INPUT_PROTOCOL = RawProtocol\n",
    "    \n",
    "   \n",
    "    \n",
    "    # START STUDENT CODE 5.4.1.B\n",
    "    INPUT_PROTOCOL = RawProtocol\n",
    "    MRJob.SORT_VALUES = True\n",
    "    \n",
    "    def mapper(self, key, value):\n",
    "        words = re.findall(\"\\w+\", key.lower())\n",
    "        #words =word_tokenize(key.lower())\n",
    "\n",
    "        for word in words:\n",
    "            yield word,1\n",
    "\n",
    "  \n",
    "    def combiner(self, word, counts):\n",
    "      \n",
    "         yield word, sum(counts)   \n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "\n",
    "         yield word, sum(counts)\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep( \n",
    "                    mapper=self.mapper,\n",
    "                    combiner=self.combiner,\n",
    "                    reducer=self.reducer,\n",
    "                    jobconf = {\n",
    "                    'mapreduce.job.reduces': 40,\n",
    "                    'mapreduce.reduce.cpu.vcores':8\n",
    "                    }\n",
    "                  ),]\n",
    "  \n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    mostFrequentWords.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /tmp/mostFrequentWords.nileshbhoyar.20170623.132133.734214\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/mostFrequentWords.nileshbhoyar.20170623.132133.734214/output...\n",
      "Removing temp directory /tmp/mostFrequentWords.nileshbhoyar.20170623.132133.734214...\n",
      "\"collection\"\t2\n",
      "\"in\"\t2\n",
      "\"forms\"\t1\n",
      "\"bill\"\t1\n",
      "\"s\"\t1\n",
      "\"sea\"\t1\n",
      "\"george\"\t1\n",
      "\"limited\"\t1\n",
      "\"the\"\t2\n",
      "\"for\"\t1\n",
      "\"circumstantial\"\t1\n",
      "\"by\"\t1\n",
      "\"case\"\t3\n",
      "\"establishing\"\t1\n",
      "\"child\"\t1\n",
      "\"biography\"\t1\n",
      "\"a\"\t10\n",
      "\"wales\"\t1\n",
      "\"of\"\t7\n",
      "\"narrative\"\t1\n",
      "\"city\"\t1\n",
      "\"government\"\t1\n",
      "\"christmas\"\t1\n",
      "\"general\"\t1\n",
      "\"female\"\t1\n",
      "\"religious\"\t1\n",
      "\"tales\"\t1\n",
      "\"fairy\"\t1\n",
      "\"study\"\t3\n"
     ]
    }
   ],
   "source": [
    "!python mostFrequentWords.py -r local \"googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\" > mostfrequent.txt\n",
    "!cat mostfrequent.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `hdfs:/user/nileshbhoyar/output': File exists\n",
      "17/06/23 13:47:50 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/nileshbhoyar/output/HW541' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/nileshbhoyar/.Trash/Current\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/mostFrequentWords.nileshbhoyar.20170623.134751.553162\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nileshbhoyar/tmp/mrjob/mostFrequentWords.nileshbhoyar.20170623.134751.553162/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob940645363447370569.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1497906899862_2490\n",
      "  Submitted application application_1497906899862_2490\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_2490/\n",
      "  Running job: job_1497906899862_2490\n",
      "  Job job_1497906899862_2490 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 4% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 15% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 19% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 29% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 44% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 46% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 76% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 79% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 86% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 89% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 94% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 96% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 5%\n",
      "   map 100% reduce 13%\n",
      "   map 100% reduce 17%\n",
      "   map 100% reduce 22%\n",
      "   map 100% reduce 25%\n",
      "   map 100% reduce 28%\n",
      "   map 100% reduce 32%\n",
      "   map 100% reduce 40%\n",
      "   map 100% reduce 45%\n",
      "   map 100% reduce 47%\n",
      "   map 100% reduce 50%\n",
      "   map 100% reduce 52%\n",
      "   map 100% reduce 60%\n",
      "   map 100% reduce 68%\n",
      "   map 100% reduce 77%\n",
      "   map 100% reduce 80%\n",
      "   map 100% reduce 85%\n",
      "   map 100% reduce 88%\n",
      "   map 100% reduce 93%\n",
      "   map 100% reduce 98%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_2490 completed successfully\n",
      "  Output directory: hdfs:///user/nileshbhoyar/output/HW541/\n",
      "Counters: 52\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3501797\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=17176520\n",
      "\t\tFILE: Number of bytes written=120350082\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2156101116\n",
      "\t\tHDFS: Number of bytes written=3501797\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=690\n",
      "\t\tHDFS: Number of write operations=80\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=3\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=193\n",
      "\t\tLaunched reduce tasks=40\n",
      "\t\tOther local map tasks=2\n",
      "\t\tRack-local map tasks=191\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=52870878720\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=836377600\n",
      "\t\tTotal time spent by all map tasks (ms)=34421145\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=103263435\n",
      "\t\tTotal time spent by all reduce tasks (ms)=326710\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1633550\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=34421145\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2613680\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=11462100\n",
      "\t\tCombine input records=293931261\n",
      "\t\tCombine output records=6701544\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=164415\n",
      "\t\tInput split bytes=32000\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=3038517455\n",
      "\t\tMap output materialized bytes=72362812\n",
      "\t\tMap output records=293931261\n",
      "\t\tMerged Map outputs=7600\n",
      "\t\tPhysical memory (bytes) snapshot=168982286336\n",
      "\t\tReduce input groups=254733\n",
      "\t\tReduce input records=6701544\n",
      "\t\tReduce output records=254733\n",
      "\t\tReduce shuffle bytes=72362812\n",
      "\t\tShuffled Maps =7600\n",
      "\t\tSpilled Records=13403088\n",
      "\t\tTotal committed heap usage (bytes)=378521780224\n",
      "\t\tVirtual memory (bytes) snapshot=551894294528\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/nileshbhoyar/tmp/mrjob/mostFrequentWords.nileshbhoyar.20170623.134751.553162...\n",
      "Removing temp directory /tmp/mostFrequentWords.nileshbhoyar.20170623.134751.553162...\n",
      "CPU times: user 11.3 s, sys: 2.73 s, total: 14 s\n",
      "Wall time: 8min 38s\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir hdfs:///user/nileshbhoyar/output/\n",
    "!hdfs dfs -rm -r hdfs:///user/nileshbhoyar/output/HW541/\n",
    "%time !python mostFrequentWords.py \\\n",
    "        -r hadoop \\\n",
    "        --output-dir=\"hdfs:///user/nileshbhoyar/output/HW541/\"\\\n",
    "        --no-output \\\n",
    "        \"hdfs:///user/cendylin/filtered-5Grams/\" \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm frequentwords.txt\n",
    "!hdfs dfs -cat hdfs:///user/nileshbhoyar/output/HW541/* > frequentwords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sort -k2nr -k1 frequentwords.txt > frequentwords.txt_sorted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"the\"\t27502442\r\n",
      "\"of\"\t18191779\r\n",
      "\"to\"\t12075972\r\n",
      "\"in\"\t7881254\r\n",
      "\"a\"\t7853623\r\n",
      "\"and\"\t7767901\r\n",
      "\"that\"\t4327887\r\n",
      "\"is\"\t3847383\r\n",
      "\"be\"\t3288734\r\n",
      "\"for\"\t2763614\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 frequentwords.txt_sorted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!head -10000 frequentwords.txt_sorted.txt > vocabulary.txt\n",
    "!head -10000 frequentwords.txt_sorted.txt | tail -1000 > basis.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most frequent words MR stats\n",
    "    \n",
    "    \n",
    "    \n",
    "__Step 1:__   \n",
    "\n",
    "    RUNNING for 2mins, 51sec \n",
    "    Launched map tasks=190 \n",
    "    Launched reduce tasks=3 sec   \n",
    "\n",
    "    Job Name:\tstreamjob8293326753705703331.jar\n",
    "    User Name:\tnileshbhoyar\n",
    "    Queue:\tberkeley\n",
    "    State:\tSUCCEEDED\n",
    "    Uberized:\tfalse\n",
    "    Submitted:\tThu Jun 22 14:13:24 UTC 2017\n",
    "    Started:\tThu Jun 22 14:16:05 UTC 2017\n",
    "    Finished:\tThu Jun 22 14:25:40 UTC 2017\n",
    "    Elapsed:\t9mins, 35sec\n",
    "    Diagnostics:\t\n",
    "    Average Map Time\t2mins, 51sec\n",
    "    Average Shuffle Time\t3sec\n",
    "    Average Merge Time\t0sec\n",
    "    Average Reduce Time\t3sec\n",
    "http://rm-ia.s3s.altiscale.com:19888/jobhistory/job/job_1497906899862_1912/mapreduce/job/job_1497906899862_1912"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.4.1 - C. 20 Most/Least densely appearing words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mostLeastDenseWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mostLeastDenseWords.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division\n",
    "import re\n",
    "import numpy as np\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class mostLeastDenseWords(MRJob):\n",
    "    \n",
    "    # START STUDENT CODE 5.4.1.C\n",
    "    def mapper1(self, _, line):\n",
    "        line = line.strip()\n",
    "        tabs = line.split('\\t')\n",
    "        words = tabs[0].split(' ')\n",
    "        for word1 in words:\n",
    "            yield ''.join(words),word1\n",
    "                    \n",
    "    def reducer1(self, key , values):\n",
    "        word2list={}\n",
    "        for value in values:\n",
    "            if value not in word2list:\n",
    "                word2list[value]=1\n",
    "            else:\n",
    "                word2list[value]+=1\n",
    "        yield key,word2list\n",
    "            \n",
    "    def mapper2(self, key , values):\n",
    "        for v in values:\n",
    "            yield v,(values[v],1)\n",
    "                    \n",
    "    def combiner2(self, key , values):\n",
    "        tf=0\n",
    "        idf=0\n",
    "        for pair in values:\n",
    "            tf+=pair[0]\n",
    "            idf+=pair[1]\n",
    "        yield key,(tf,idf)\n",
    "        \n",
    "    def reducer2(self, key , values):\n",
    "        tf=0\n",
    "        idf=0\n",
    "        for pair in values:\n",
    "            tf+=pair[0]\n",
    "            idf+=pair[1]\n",
    "        yield key,(tf/idf)\n",
    "\n",
    "    def steps(self):\n",
    "        JOBCONF_STEP1 = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keypartitioner.options':'-nrk2',\n",
    "            'mapreduce.partition.keycomparator.options':'-nrk2',\n",
    "           \n",
    "            \n",
    "            'partitioner':'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "        }\n",
    "        return [MRStep(jobconf=JOBCONF_STEP1,mapper=self.mapper1,reducer=self.reducer1\n",
    "                      ),\n",
    "                MRStep(mapper=self.mapper2,combiner=self.combiner2,reducer=self.reducer2,jobconf={\n",
    "                    'mapred.map.tasks': 190,\n",
    "                    'mapreduce.reduce.cpu.vcores':8\n",
    "                        }\n",
    "                      )\n",
    "               ]\n",
    "    # END STUDENT CODE 5.4.1.C\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    mostLeastDenseWords.run()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    a = 'time elapsed:', str(elapsed_time)\n",
    "    logging.warning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the test data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove `mostLeastDenseWords.txt': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/mostLeastDenseWords.nileshbhoyar.20170623.132140.228575\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/mostLeastDenseWords.nileshbhoyar.20170623.132140.228575/output...\n",
      "Removing temp directory /tmp/mostLeastDenseWords.nileshbhoyar.20170623.132140.228575...\n",
      "WARNING:root:('time elapsed:', '2.57423710823')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!rm mostLeastDenseWords.txt\n",
    "!python mostLeastDenseWords.py -r local \"googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\" > mostLeastDenseWords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"of\"\t1.1666666667\n",
      "\"the\"\t1.0\n",
      "\"in\"\t1.0\n",
      "\"by\"\t1.0\n",
      "\"Wales\"\t1.0\n",
      "\"Tales\"\t1.0\n",
      "\"Study\"\t1.0\n",
      "\"Sea\"\t1.0\n",
      "\"RELIGIOUS\"\t1.0\n",
      "\"Narrative\"\t1.0\n",
      "\"Limited\"\t1.0\n",
      "\"Government\"\t1.0\n",
      "\"George\"\t1.0\n",
      "\"General\"\t1.0\n",
      "\"Forms\"\t1.0\n",
      "\"Female\"\t1.0\n",
      "\"Fairy\"\t1.0\n",
      "\"FOR\"\t1.0\n",
      "\"ESTABLISHING\"\t1.0\n",
      "\"Collection\"\t1.0\n",
      "\"of\"\t1.1666666667\n",
      "\"the\"\t1.0\n",
      "\"in\"\t1.0\n",
      "\"by\"\t1.0\n",
      "\"Wales\"\t1.0\n",
      "\"Tales\"\t1.0\n",
      "\"Study\"\t1.0\n",
      "\"Sea\"\t1.0\n",
      "\"RELIGIOUS\"\t1.0\n",
      "\"Narrative\"\t1.0\n",
      "\"Limited\"\t1.0\n",
      "\"Government\"\t1.0\n",
      "\"George\"\t1.0\n",
      "\"General\"\t1.0\n",
      "\"Forms\"\t1.0\n",
      "\"Female\"\t1.0\n",
      "\"Fairy\"\t1.0\n",
      "\"FOR\"\t1.0\n",
      "\"ESTABLISHING\"\t1.0\n",
      "\"Collection\"\t1.0\n"
     ]
    }
   ],
   "source": [
    "!cat mostLeastDenseWords.txt | sort -nrk2 | head -20\n",
    "!cat mostLeastDenseWords.txt | sort -nrk2 | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `hdfs:/user/nileshbhoyar/output': File exists\n",
      "17/06/23 13:21:47 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/nileshbhoyar/output/HW541C' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/nileshbhoyar/.Trash/Current\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/mostLeastDenseWords.nileshbhoyar.20170623.132148.216070\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nileshbhoyar/tmp/mrjob/mostLeastDenseWords.nileshbhoyar.20170623.132148.216070/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.map.tasks: mapreduce.job.maps\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob1973423015615906961.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1497906899862_2484\n",
      "  Submitted application application_1497906899862_2484\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_2484/\n",
      "  Running job: job_1497906899862_2484\n",
      "  Job job_1497906899862_2484 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 4% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 15% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 44% reduce 0%\n",
      "   map 46% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 76% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 79% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 89% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 94% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 96% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 7%\n",
      "   map 100% reduce 9%\n",
      "   map 100% reduce 11%\n",
      "   map 100% reduce 12%\n",
      "   map 100% reduce 15%\n",
      "   map 100% reduce 17%\n",
      "   map 100% reduce 18%\n",
      "   map 100% reduce 21%\n",
      "   map 100% reduce 25%\n",
      "   map 100% reduce 29%\n",
      "   map 100% reduce 34%\n",
      "   map 100% reduce 40%\n",
      "   map 100% reduce 48%\n",
      "   map 100% reduce 56%\n",
      "   map 100% reduce 61%\n",
      "   map 100% reduce 63%\n",
      "   map 100% reduce 65%\n",
      "   map 100% reduce 66%\n",
      "   map 100% reduce 67%\n",
      "   map 100% reduce 68%\n",
      "   map 100% reduce 69%\n",
      "   map 100% reduce 70%\n",
      "   map 100% reduce 71%\n",
      "   map 100% reduce 72%\n",
      "   map 100% reduce 73%\n",
      "   map 100% reduce 74%\n",
      "   map 100% reduce 75%\n",
      "   map 100% reduce 76%\n",
      "   map 100% reduce 77%\n",
      "   map 100% reduce 78%\n",
      "   map 100% reduce 79%\n",
      "   map 100% reduce 80%\n",
      "   map 100% reduce 81%\n",
      "   map 100% reduce 82%\n",
      "   map 100% reduce 83%\n",
      "   map 100% reduce 84%\n",
      "   map 100% reduce 85%\n",
      "   map 100% reduce 86%\n",
      "   map 100% reduce 87%\n",
      "   map 100% reduce 88%\n",
      "   map 100% reduce 89%\n",
      "   map 100% reduce 90%\n",
      "   map 100% reduce 91%\n",
      "   map 100% reduce 92%\n",
      "   map 100% reduce 93%\n",
      "   map 100% reduce 94%\n",
      "   map 100% reduce 95%\n",
      "   map 100% reduce 96%\n",
      "   map 100% reduce 97%\n",
      "   map 100% reduce 98%\n",
      "   map 100% reduce 99%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_2484 completed successfully\n",
      "  Output directory: hdfs:///user/nileshbhoyar/tmp/mrjob/mostLeastDenseWords.nileshbhoyar.20170623.132148.216070/step-output/0000\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4795509007\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3142217937\n",
      "\t\tFILE: Number of bytes written=6739677464\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2156101116\n",
      "\t\tHDFS: Number of bytes written=4795509007\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=660\n",
      "\t\tHDFS: Number of write operations=60\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=3\n",
      "\t\tLaunched map tasks=193\n",
      "\t\tLaunched reduce tasks=30\n",
      "\t\tOther local map tasks=2\n",
      "\t\tRack-local map tasks=191\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=21623717376\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=23829153280\n",
      "\t\tTotal time spent by all map tasks (ms)=14077941\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=42233823\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9308263\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=46541315\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14077941\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=37233052\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=15378870\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=264985\n",
      "\t\tInput split bytes=32000\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=9413166910\n",
      "\t\tMap output materialized bytes=3568044227\n",
      "\t\tMap output records=293411330\n",
      "\t\tMerged Map outputs=5700\n",
      "\t\tPhysical memory (bytes) snapshot=181705306112\n",
      "\t\tReduce input groups=58681272\n",
      "\t\tReduce input records=293411330\n",
      "\t\tReduce output records=58681272\n",
      "\t\tReduce shuffle bytes=3568044227\n",
      "\t\tShuffled Maps =5700\n",
      "\t\tSpilled Records=586822660\n",
      "\t\tTotal committed heap usage (bytes)=361058271232\n",
      "\t\tVirtual memory (bytes) snapshot=518039519232\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.map.tasks: mapreduce.job.maps\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob958509063857636388.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 30\n",
      "  number of splits:210\n",
      "  Submitting tokens for job: job_1497906899862_2487\n",
      "  Submitted application application_1497906899862_2487\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_2487/\n",
      "  Running job: job_1497906899862_2487\n",
      "  Job job_1497906899862_2487 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 4% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 15% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 19% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 29% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 38% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 44% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 46% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 76% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 79% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 86% reduce 0%\n",
      "   map 87% reduce 0%\n",
      "   map 88% reduce 0%\n",
      "   map 89% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 94% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 96% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 63%\n",
      "   map 100% reduce 67%\n",
      "   map 100% reduce 68%\n",
      "   map 100% reduce 69%\n",
      "   map 100% reduce 70%\n",
      "   map 100% reduce 71%\n",
      "   map 100% reduce 72%\n",
      "   map 100% reduce 73%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   map 100% reduce 74%\n",
      "   map 100% reduce 75%\n",
      "   map 100% reduce 76%\n",
      "   map 100% reduce 77%\n",
      "   map 100% reduce 78%\n",
      "   map 100% reduce 79%\n",
      "   map 100% reduce 80%\n",
      "   map 100% reduce 81%\n",
      "   map 100% reduce 82%\n",
      "   map 100% reduce 83%\n",
      "   map 100% reduce 84%\n",
      "   map 100% reduce 85%\n",
      "   map 100% reduce 86%\n",
      "   map 100% reduce 87%\n",
      "   map 100% reduce 88%\n",
      "   map 100% reduce 89%\n",
      "   map 100% reduce 90%\n",
      "   map 100% reduce 91%\n",
      "   map 100% reduce 92%\n",
      "   map 100% reduce 93%\n",
      "   map 100% reduce 94%\n",
      "   map 100% reduce 95%\n",
      "   map 100% reduce 96%\n",
      "   map 100% reduce 97%\n",
      "   map 100% reduce 98%\n",
      "   map 100% reduce 99%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_2487 completed successfully\n",
      "  Output directory: hdfs:///user/nileshbhoyar/output/HW541C/\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=4805773657\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5412281\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=24748610\n",
      "\t\tFILE: Number of bytes written=123678120\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4805815657\n",
      "\t\tHDFS: Number of bytes written=5412281\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=633\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=2\n",
      "\t\tLaunched map tasks=212\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=212\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=102647370240\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=995402240\n",
      "\t\tTotal time spent by all map tasks (ms)=66827715\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=200483145\n",
      "\t\tTotal time spent by all reduce tasks (ms)=388829\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1944145\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=66827715\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3110632\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=38944620\n",
      "\t\tCombine input records=288860781\n",
      "\t\tCombine output records=7769009\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=167763\n",
      "\t\tInput split bytes=42000\n",
      "\t\tMap input records=58681272\n",
      "\t\tMap output bytes=4151933892\n",
      "\t\tMap output materialized bytes=70906134\n",
      "\t\tMap output records=288860781\n",
      "\t\tMerged Map outputs=210\n",
      "\t\tPhysical memory (bytes) snapshot=171257028608\n",
      "\t\tReduce input groups=343019\n",
      "\t\tReduce input records=7769009\n",
      "\t\tReduce output records=343019\n",
      "\t\tReduce shuffle bytes=70906134\n",
      "\t\tShuffled Maps =210\n",
      "\t\tSpilled Records=15538018\n",
      "\t\tTotal committed heap usage (bytes)=328451227648\n",
      "\t\tVirtual memory (bytes) snapshot=465282498560\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/nileshbhoyar/tmp/mrjob/mostLeastDenseWords.nileshbhoyar.20170623.132148.216070...\n",
      "Removing temp directory /tmp/mostLeastDenseWords.nileshbhoyar.20170623.132148.216070...\n",
      "WARNING:root:('time elapsed:', '1552.66501188')\n",
      "CPU times: user 34.5 s, sys: 7.89 s, total: 42.4 s\n",
      "Wall time: 25min 52s\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir hdfs:///user/nileshbhoyar/output/\n",
    "!hdfs dfs -rm -r hdfs:///user/nileshbhoyar/output/HW541C/\n",
    "%time !python mostLeastDenseWords.py \\\n",
    "        -r hadoop \\\n",
    "        --output-dir=\"hdfs:///user/nileshbhoyar/output/HW541C/\"\\\n",
    "        --no-output \\\n",
    "        \"hdfs:///user/cendylin/filtered-5Grams/\" \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 nileshbhoyar users          0 2017-06-23 13:47 hdfs:/user/nileshbhoyar/output/HW541C/_SUCCESS\r\n",
      "-rw-r--r--   3 nileshbhoyar users    5412281 2017-06-23 13:47 hdfs:/user/nileshbhoyar/output/HW541C/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls hdfs:///user/nileshbhoyar/output/HW541C/\n",
    "!hdfs dfs -cat hdfs:/user/nileshbhoyar/output/HW541C/part-* > final_density.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sort -k2nr final_density.txt > sorted_density.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"IOOO\"\t5.0\r\n",
      "\"MeO\"\t5.0\r\n",
      "\"OMe\"\t5.0\r\n",
      "\"OOOOOO\"\t5.0\r\n",
      "\"PIC\"\t5.0\r\n",
      "\"Quintile\"\t5.0\r\n",
      "\"llll\"\t5.0\r\n",
      "\"nnn\"\t5.0\r\n",
      "\"oooooooooooooooo\"\t5.0\r\n",
      "\"xxxx\"\t5.0\r\n",
      "\"xxxxxxxx\"\t5.0\r\n",
      "\"OOOOO\"\t4.2857142857142856\r\n",
      "\"CHOH\"\t4.0\r\n",
      "\"oooooooo\"\t4.0\r\n",
      "\"ooooo\"\t3.875\r\n",
      "\"IIII\"\t3.6666666666666665\r\n",
      "\"oooooo\"\t3.5\r\n",
      "\"ARCHIE\"\t3.0\r\n",
      "\"Cosine\"\t3.0\r\n",
      "\"Crystalloid\"\t3.0\r\n"
     ]
    }
   ],
   "source": [
    "!head  -20  sorted_density.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word density MR stats\n",
    "\n",
    "    ec2_instance_type: m3.xlarge\n",
    "    num_ec2_instances: 15\n",
    "    \n",
    "__Step 1:__ \n",
    "\n",
    "    Job Name:\tstreamjob3040269206354443932.jar\n",
    "    User Name:\tnileshbhoyar\n",
    "    Queue:\tberkeley\n",
    "    State:\tSUCCEEDED\n",
    "    Uberized:\tfalse\n",
    "    Submitted:\tThu Jun 22 17:47:41 UTC 2017\n",
    "    Started:\tThu Jun 22 17:47:49 UTC 2017\n",
    "    Finished:\tThu Jun 22 17:54:08 UTC 2017\n",
    "    Elapsed:\t6mins, 19sec\n",
    "    Diagnostics:\t\n",
    "    Average Map Time\t1mins, 10sec\n",
    "    Average Shuffle Time\t4sec\n",
    "    Average Merge Time\t7sec\n",
    "    Average Reduce Time\t4mins, 9sec\n",
    " \n",
    "http://rm-ia.s3s.altiscale.com:19888/jobhistory/job/job_1497906899862_2043/mapreduce/job/job_1497906899862_2043\n",
    "\n",
    "step 2\n",
    "\n",
    "    Job Name:\tstreamjob810535679113074807.jar\n",
    "    User Name:\tnileshbhoyar\n",
    "    Queue:\tberkeley\n",
    "    State:\tSUCCEEDED\n",
    "    Uberized:\tfalse\n",
    "    Submitted:\tThu Jun 22 17:54:18 UTC 2017\n",
    "    Started:\tThu Jun 22 17:54:28 UTC 2017\n",
    "    Finished:\tThu Jun 22 18:08:51 UTC 2017\n",
    "    Elapsed:\t14mins, 22sec\n",
    "    Diagnostics:\t\n",
    "    Average Map Time\t5mins, 25sec\n",
    "    Average Shuffle Time\t3sec\n",
    "    Average Merge Time\t5sec\n",
    "    Average Reduce Time\t5mins, 36sec\n",
    " \n",
    " http://rm-ia.s3s.altiscale.com:19888/jobhistory/job/job_1497906899862_2049/mapreduce/job/job_1497906899862_2049"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.4.1 - D. Distribution of 5-gram sizes (character length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing distribution.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile distribution.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class distribution(MRJob):\n",
    "    counter=0\n",
    "    # START STUDENT CODE 5.4.1.D\n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        tabs = line.split('\\t')\n",
    "        words = tabs[0]\n",
    "        yield len(words),1\n",
    "    def combiner(self,key,value):\n",
    "        yield key,sum(value)\n",
    "    def reducer(self, key , value):\n",
    "        yield key,sum(value)\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,combiner=self.combiner,reducer=self.reducer,jobconf={\n",
    "            'mapred.map.tasks': 190,\n",
    "\t\t    'mapreduce.reduce.cpu.vcores':8\n",
    "        })]\n",
    "    # END STUDENT CODE 5.4.1.D\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    distribution.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: Unknown command\n",
      "Did you mean -rm?  This command begins with a dash.\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/distribution.nileshbhoyar.20170623.140355.472491\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nileshbhoyar/tmp/mrjob/distribution.nileshbhoyar.20170623.140355.472491/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.map.tasks: mapreduce.job.maps\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob9034149022049265941.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:375\n",
      "  Submitting tokens for job: job_1497906899862_2492\n",
      "  Submitted application application_1497906899862_2492\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_2492/\n",
      "  Running job: job_1497906899862_2492\n",
      "  Job job_1497906899862_2492 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 9% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 77% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 94% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_2492 completed successfully\n",
      "  Output directory: hdfs:///user/nileshbhoyar/tmp/mrjob/distribution.nileshbhoyar.20170623.140355.472491/output\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=70500\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=45\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=68\n",
      "\t\tFILE: Number of bytes written=49943659\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=159000\n",
      "\t\tHDFS: Number of bytes written=45\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=1128\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=375\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=375\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=12486535680\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10923520\n",
      "\t\tTotal time spent by all map tasks (ms)=8129255\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=24387765\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4267\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=21335\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8129255\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=34136\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=384830\n",
      "\t\tCombine input records=10\n",
      "\t\tCombine output records=10\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=31988\n",
      "\t\tInput split bytes=88500\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=50\n",
      "\t\tMap output materialized bytes=6070\n",
      "\t\tMap output records=10\n",
      "\t\tMerged Map outputs=375\n",
      "\t\tPhysical memory (bytes) snapshot=298987986944\n",
      "\t\tReduce input groups=9\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=9\n",
      "\t\tReduce shuffle bytes=6070\n",
      "\t\tShuffled Maps =375\n",
      "\t\tSpilled Records=20\n",
      "\t\tTotal committed heap usage (bytes)=589941440512\n",
      "\t\tVirtual memory (bytes) snapshot=827733069824\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nileshbhoyar/tmp/mrjob/distribution.nileshbhoyar.20170623.140355.472491/output...\n",
      "Removing HDFS temp directory hdfs:///user/nileshbhoyar/tmp/mrjob/distribution.nileshbhoyar.20170623.140355.472491...\n",
      "Removing temp directory /tmp/distribution.nileshbhoyar.20170623.140355.472491...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!hdfs dfs rm --recursive systems_test_distribution\n",
    "!python distribution.py -r hadoop googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt > systems_test_distribution.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\t2\r\n",
      "29\t1\r\n",
      "28\t1\r\n",
      "27\t1\r\n",
      "26\t1\r\n",
      "24\t1\r\n",
      "23\t1\r\n",
      "22\t1\r\n",
      "17\t1\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!cat systems_test_distribution.txt | sort -nrk1,1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the test data set:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On the full data set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `hdfs:/user/nileshbhoyar/output': File exists\n",
      "rm: `hdfs:///user/nileshbhoyar/output/HW541D/': No such file or directory\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/distribution.nileshbhoyar.20170623.140551.232466\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nileshbhoyar/tmp/mrjob/distribution.nileshbhoyar.20170623.140551.232466/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.map.tasks: mapreduce.job.maps\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob366480745319286911.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1497906899862_2493\n",
      "  Submitted application application_1497906899862_2493\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_2493/\n",
      "  Running job: job_1497906899862_2493\n",
      "  Job job_1497906899862_2493 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 15% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 22% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 26% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 51% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 53% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 80% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 94% reduce 0%\n",
      "   map 95% reduce 0%\n",
      "   map 96% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_2493 completed successfully\n",
      "  Output directory: hdfs:///user/nileshbhoyar/output/HW541D/\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=624\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=29003\n",
      "\t\tFILE: Number of bytes written=25441030\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2156101116\n",
      "\t\tHDFS: Number of bytes written=624\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=573\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=191\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=2\n",
      "\t\tRack-local map tasks=189\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=15845016576\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=12195840\n",
      "\t\tTotal time spent by all map tasks (ms)=10315766\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=30947298\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4764\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=23820\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10315766\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=38112\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3528460\n",
      "\t\tCombine input records=58682266\n",
      "\t\tCombine output records=9172\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=42452\n",
      "\t\tInput split bytes=32000\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=293411210\n",
      "\t\tMap output materialized bytes=77954\n",
      "\t\tMap output records=58682266\n",
      "\t\tMerged Map outputs=190\n",
      "\t\tPhysical memory (bytes) snapshot=154452697088\n",
      "\t\tReduce input groups=80\n",
      "\t\tReduce input records=9172\n",
      "\t\tReduce output records=80\n",
      "\t\tReduce shuffle bytes=77954\n",
      "\t\tShuffled Maps =190\n",
      "\t\tSpilled Records=18344\n",
      "\t\tTotal committed heap usage (bytes)=299992875008\n",
      "\t\tVirtual memory (bytes) snapshot=421201391616\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/nileshbhoyar/tmp/mrjob/distribution.nileshbhoyar.20170623.140551.232466...\n",
      "Removing temp directory /tmp/distribution.nileshbhoyar.20170623.140551.232466...\n",
      "CPU times: user 2.62 s, sys: 568 ms, total: 3.19 s\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "!hdfs dfs -mkdir hdfs:///user/nileshbhoyar/output/\n",
    "!hdfs dfs -rm -r hdfs:///user/nileshbhoyar/output/HW541D/\n",
    "%time !python distribution.py \\\n",
    "        -r hadoop \\\n",
    "        --output-dir=\"hdfs:///user/nileshbhoyar/output/HW541D/\"\\\n",
    "        --no-output \\\n",
    "        \"hdfs:///user/cendylin/filtered-5Grams/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm full_distribution.txt\n",
    "!hdfs dfs -cat hdfs:///user/nileshbhoyar/output/HW541D/part* |sort -nk1,1 > full_distribution.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/oAAAG1CAYAAABJd48xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRoqTslCwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0ad22b8d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "results_A = []\n",
    "for line in open(\"full_distribution.txt\").readlines():\n",
    "    line = line.strip()\n",
    "    X,Y = line.split(\"\\t\")\n",
    "    results_A.append([int(X),int(Y)])\n",
    "\n",
    "items = (np.array(results_A)[::-1].T)\n",
    "fig = pl.figure(figsize=(17,7))\n",
    "ax = pl.subplot(111)\n",
    "width=0.8\n",
    "ax.bar(range(len(items[0])), items[1], width=width)\n",
    "ax.set_xticks(np.arange(len(items[0])) + width/2)\n",
    "ax.set_xticklabels(items[0], rotation=90)\n",
    "\n",
    "pl.title(\"Distributions of 5 Gram lengths\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution MRJob stats\n",
    "\n",
    "    Job Name:\tstreamjob1829799781120004028.jar\n",
    "    User Name:\tnileshbhoyar\n",
    "    Queue:\tberkeley\n",
    "    State:\tSUCCEEDED\n",
    "    Uberized:\tfalse\n",
    "    Submitted:\tThu Jun 22 17:40:18 UTC 2017\n",
    "    Started:\tThu Jun 22 17:40:26 UTC 2017\n",
    "    Finished:\tThu Jun 22 17:41:55 UTC 2017\n",
    "    Elapsed:\t1mins, 28sec\n",
    "    Diagnostics:\t\n",
    "    Average Map Time\t54sec\n",
    "    Average Shuffle Time\t3sec\n",
    "    Average Merge Time\t0sec\n",
    "    Average Reduce Time\t1sec\n",
    "    http://rm-ia.s3s.altiscale.com:19888/jobhistory/job/job_1497906899862_2038/mapreduce/job/job_1497906899862_2038"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/oAAAG1CAYAAABJd48xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRoqTslCwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0ad206b990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "results_A = []\n",
    "for line in open(\"full_distribution.txt\").readlines():\n",
    "    line = line.strip()\n",
    "    X,Y = line.split(\"\\t\")\n",
    "    results_A.append([int(X),int(Y)])\n",
    "\n",
    "items = (np.array(results_A)[::-1].T)\n",
    "fig = pl.figure(figsize=(17,7))\n",
    "ax = pl.subplot(111)\n",
    "width=0.8\n",
    "ax.bar(range(len(items[0])), items[1], width=width)\n",
    "ax.set_xticks(np.arange(len(items[0])) + width/2)\n",
    "ax.set_xticklabels(items[0], rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "pl.title(\"Distributions of 5 Gram lengths\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Run times\n",
    "    Job Name:\tstreamjob1829799781120004028.jar\n",
    "    User Name:\tnileshbhoyar\n",
    "    Queue:\tberkeley\n",
    "    State:\tSUCCEEDED\n",
    "    Uberized:\tfalse\n",
    "    Submitted:\tThu Jun 22 17:40:18 UTC 2017\n",
    "    Started:\tThu Jun 22 17:40:26 UTC 2017\n",
    "    Finished:\tThu Jun 22 17:41:55 UTC 2017\n",
    "    Elapsed:\t1mins, 28sec\n",
    "    Diagnostics:\t\n",
    "    Average Map Time\t54sec\n",
    "    Average Shuffle Time\t3sec\n",
    "    Average Merge Time\t0sec\n",
    "    Average Reduce Time\t1sec\n",
    "    http://rm-ia.s3s.altiscale.com:19888/jobhistory/job/job_1497906899862_2038/mapreduce/job/job_1497906899862_2038"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.4.2 <a name=\"5.4.2\"></a>OPTIONAL Question: log-log plots (PHASE 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "- https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "- https://en.wikipedia.org/wiki/Power_law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.5  <a name=\"5.5\"></a> Synonym detection over 2Gig of Data with extra Preprocessing steps (HW5.3 plus some preprocessing)   (Phase 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "For the remainder of this assignment please feel free to eliminate stop words from your analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    "> from nltk.corpus import stopwords\n",
    " stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: A large subset of the Google n-grams dataset as was described above\n",
    "\n",
    "For each HW 5.4 -5.5.1 Please unit test and system test your code with respect \n",
    "to SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations for the \n",
    "SYSTEMS TEST DATASET. Then show the results you get with your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment we will focus on developing methods for detecting synonyms, using the Google 5-grams dataset. At a high level:\n",
    "\n",
    "\n",
    "1. remove stopwords\n",
    "2. get 10,0000 most frequent\n",
    "3. get 1000 (9001-10000) features\n",
    "3. build stripes\n",
    "\n",
    "To accomplish this you must script two main tasks using MRJob:\n",
    "\n",
    "\n",
    "__TASK (1)__ Build stripes for the most frequent 10,000 words using cooccurence information based on\n",
    "the words ranked from 9001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "\n",
    "__TASK (2)__ Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "#### Design notes for TASK (1)\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for TASK (2).\n",
    "\n",
    "#### Design notes for _TASK (2)_\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. \n",
    "\n",
    "Please report the size of the cluster used and the amount of time it takes to run for the index construction task and for the synonym calculation task. How many pairs need to be processed (HINT: use the posting list length to calculate directly)? Report your  Cluster configuration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example MR stats: (report times!)\n",
    "    took ~11 minutes on 5 m3.xlarge nodes\n",
    "    Data-local map tasks=188\n",
    "\tLaunched map tasks=190\n",
    "\tLaunched reduce tasks=15\n",
    "\tOther local map tasks=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOP word removal MR job from output of HW5.4.1 - B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing stopremoval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stopremoval.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    " \n",
    "STOP_WORDS =['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "             'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he',\n",
    "             'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n",
    "             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "             'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', \n",
    "             'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', \n",
    "             'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', \n",
    "             'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off',\n",
    "             'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', \n",
    "             'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only',\n",
    "             'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    " \n",
    "class MRSTOPRemoval(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "         fields = line.split(\"\\t\")\n",
    "         word = re.findall(\"\\w+\", fields[0])\n",
    "         if word[0] not in STOP_WORDS:\n",
    "                yield word[0] ,int(fields[1])\n",
    "            \n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep( \n",
    "                    mapper=self.mapper,\n",
    "                    \n",
    "                  ),]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRSTOPRemoval.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/stopremoval.nileshbhoyar.20170623.140824.017998\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/stopremoval.nileshbhoyar.20170623.140824.017998/output...\n",
      "Removing temp directory /tmp/stopremoval.nileshbhoyar.20170623.140824.017998...\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 2.69 s\n"
     ]
    }
   ],
   "source": [
    "%time !python stopremoval.py -r local frequentwords.txt_sorted.txt > frequentwords_final.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sort  -k2nr -k1 frequentwords_final.txt > frequentwords_final_sorted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!head -10000 frequentwords_final_sorted.txt > vocabulary.txt\n",
    "!head -10000 frequentwords_final_sorted.txt | tail -1000 > features.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting buildStripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile buildStripes.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import re\n",
    "import mrjob\n",
    "import csv\n",
    "import json\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from itertools import  combinations\n",
    "class MRbuildStripes(MRJob):\n",
    "  \n",
    "  #START SUDENT CODE531_STRIPES\n",
    "    SORT_VALUES = True\n",
    "   \n",
    "    #def __init__(self, *args, **kwargs):\n",
    "    #    super(MRjoins, self).__init__(*args, **kwargs)\n",
    "        \n",
    "  \n",
    "    def mapper_init(self):\n",
    "        self.basis = set()\n",
    "        self.vocabulary = set()\n",
    "\n",
    "        with open('features.txt', 'r') as basis_file:\n",
    "            self.features = set([row[0] for row in csv.reader(basis_file, delimiter = '\\t')])\n",
    "\n",
    "        with open('vocabulary.txt', 'r') as vocabulary_file:\n",
    "            self.vocabulary = set([row[0] for row in csv.reader(vocabulary_file, delimiter = '\\t')])\n",
    "           \n",
    "    \n",
    "        \n",
    "    def mapper(self, _, recs):\n",
    "        self.increment_counter('Execution Counts', 'mapper calls', 1)\n",
    "        fields = recs.split(\"\\t\")\n",
    "        \n",
    "        products = fields[0].lower().replace('\\n','').split()\n",
    "        for i, term in enumerate(products):\n",
    "                # Create a new stripe for each term\n",
    "                stripe = {}\n",
    "                # check if term is in vocab\n",
    "                if term not in self.vocabulary:\n",
    "                        continue\n",
    "                for j, token in enumerate(products):\n",
    "                    \n",
    "                    if i != j:\n",
    "                        if token not in self.features:\n",
    "                            continue\n",
    "                        x = stripe.get(token,None)\n",
    "                        \n",
    "                        if x == None:\n",
    "                            stripe[token] = int( fields[1])\n",
    "                        else:\n",
    "                            stripe[token] += int(fields[1])\n",
    "\n",
    "                # Emit the term and the stripe\n",
    "                if len(stripe) > 0:\n",
    "                    yield term, stripe\n",
    "    \n",
    "    def combiner(self, word, stripes):\n",
    "        yield word, self.combine_stripes(stripes)\n",
    "\n",
    "    def combine_stripes(self, stripes):\n",
    "        combined_stripe = {}\n",
    "\n",
    "        for stripe in stripes:\n",
    "            for key, value in stripe.iteritems():\n",
    "                if key in combined_stripe:\n",
    "                    combined_stripe[key] += int(value)\n",
    "                else:\n",
    "                    combined_stripe[key] = int(value)\n",
    "\n",
    "        return combined_stripe\n",
    "    def reducer(self,key, records):\n",
    "        yield key, self.combine_stripes(records)\n",
    "        \n",
    "    def steps(self):  #pipeline of Map-Reduce jobs\n",
    "        step = MRStep( \n",
    "                    mapper_init=self.mapper_init,\n",
    "                    mapper=self.mapper,       \n",
    "                    combiner = self.combiner,\n",
    "                    reducer=self.reducer,\n",
    "                    jobconf={\n",
    "                          'mapreduce.job.reduces': 20,\n",
    "                           'mapred.map.tasks': 60,\n",
    "                    }\n",
    "                    )\n",
    "        return [step]\n",
    "            \n",
    "\n",
    "\n",
    "  #END SUDENT CODE531_STRIPES\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "  MRbuildStripes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: Unknown command\n",
      "Did you mean -rm?  This command begins with a dash.\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/buildStripes.nileshbhoyar.20170623.140828.416067\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nileshbhoyar/tmp/mrjob/buildStripes.nileshbhoyar.20170623.140828.416067/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.map.tasks: mapreduce.job.maps\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob2633475190638288861.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 1\n",
      "  number of splits:67\n",
      "  Submitting tokens for job: job_1497906899862_2494\n",
      "  Submitted application application_1497906899862_2494\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_2494/\n",
      "  Running job: job_1497906899862_2494\n",
      "  Job job_1497906899862_2494 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 34% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 42% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 15%\n",
      "   map 100% reduce 30%\n",
      "   map 100% reduce 60%\n",
      "   map 100% reduce 80%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_2494 completed successfully\n",
      "  Output directory: hdfs:///user/nileshbhoyar/tmp/mrjob/buildStripes.nileshbhoyar.20170623.140828.416067/output\n",
      "Counters: 51\n",
      "\tExecution Counts\n",
      "\t\tmapper calls=3\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2278\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=400\n",
      "\t\tFILE: Number of bytes written=11725889\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=15611\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=261\n",
      "\t\tHDFS: Number of write operations=40\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=68\n",
      "\t\tLaunched reduce tasks=20\n",
      "\t\tRack-local map tasks=68\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2797478400\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=194163200\n",
      "\t\tTotal time spent by all map tasks (ms)=1821275\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5463825\n",
      "\t\tTotal time spent by all reduce tasks (ms)=75845\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=379225\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=1821275\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=75845\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=83570\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=7226\n",
      "\t\tInput split bytes=13333\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=0\n",
      "\t\tMap output materialized bytes=21440\n",
      "\t\tMap output records=0\n",
      "\t\tMerged Map outputs=1340\n",
      "\t\tPhysical memory (bytes) snapshot=59336208384\n",
      "\t\tReduce input groups=0\n",
      "\t\tReduce input records=0\n",
      "\t\tReduce output records=0\n",
      "\t\tReduce shuffle bytes=21440\n",
      "\t\tShuffled Maps =1340\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=146701025280\n",
      "\t\tVirtual memory (bytes) snapshot=214275899392\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/nileshbhoyar/tmp/mrjob/buildStripes.nileshbhoyar.20170623.140828.416067/output...\n",
      "Removing HDFS temp directory hdfs:///user/nileshbhoyar/tmp/mrjob/buildStripes.nileshbhoyar.20170623.140828.416067...\n",
      "Removing temp directory /tmp/buildStripes.nileshbhoyar.20170623.140828.416067...\n"
     ]
    }
   ],
   "source": [
    "#unit test\n",
    "!hdfs dfs rm --recursive systems_test_stripes_2\n",
    "!python buildStripes.py -r hadoop atlas-boon-systems-test.txt --file features.txt --file vocabulary.txt  > systems_test_stripes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting invertedIndex.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile invertedIndex.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import JSONProtocol\n",
    "class MRinvertedIndex(MRJob):\n",
    "    INPUT_PROTOCOL = JSONProtocol\n",
    "    SORT_VALUES = True\n",
    "#START SUDENT CODE531_INV_INDEX\n",
    "    def mapper_normalize_transpose(self, word, rate_stripe):\n",
    "\n",
    "\n",
    "\n",
    "        # Divide each value in the vector by the magnitude to normalize.\n",
    "        length = len(rate_stripe)\n",
    "        for key, value in rate_stripe.iteritems():\n",
    "            #normalized_value = value / magnitude\n",
    "            yield key, { word: length}\n",
    "    def combiner_normalize_transpose(self, word, transpose_stripes):\n",
    "        yield word, self.combine_stripes(transpose_stripes)\n",
    "    def reducer_normalize_transpose(self, word, transpose_stripes):\n",
    "        yield word, self.combine_stripes(transpose_stripes)\n",
    "    def combine_stripes(self, stripes):\n",
    "        combined_stripe = {}\n",
    "\n",
    "        for stripe in stripes:\n",
    "            for key, value in stripe.iteritems():\n",
    "                if key in combined_stripe:\n",
    "                    combined_stripe[key] += value\n",
    "                else:\n",
    "                    combined_stripe[key] = value\n",
    "\n",
    "        return combined_stripe\n",
    "\n",
    "        \n",
    "    def steps(self):\n",
    "\n",
    "        transpose_step = MRStep(\n",
    "            mapper = self.mapper_normalize_transpose,\n",
    "            combiner = self.combiner_normalize_transpose,\n",
    "            reducer = self.reducer_normalize_transpose,\n",
    "            jobconf = {\n",
    "                    'mapreduce.job.reduces': 10\n",
    "                })\n",
    "        return [transpose_step]\n",
    "\n",
    "#END SUDENT CODE531_INV_INDEX\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRinvertedIndex.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting similarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile similarity.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import JSONProtocol\n",
    "\n",
    "class MRsimilarity(MRJob):\n",
    "    INPUT_PROTOCOL = JSONProtocol\n",
    " \n",
    "    SORT_VALUES = True\n",
    "    def makeKeyn(self,key, num_reducers):\n",
    "        byteof = lambda char: int(format(ord(char), 'b'), 2)\n",
    "        current_hash = 0\n",
    "        for c in key:\n",
    "            current_hash = (current_hash * 31 + byteof(c))\n",
    "        return current_hash % num_reducers\n",
    "    \n",
    "\n",
    "#START SUDENT CODE531_SIMILARITY\n",
    "    def mapper_jaccard(self, word, rate_stripe):\n",
    "        #get all words and lengths\n",
    "#We will emit stripes for each word vector here.  These stripes will\n",
    "#be used in combiner to find the common length i.e. words occuring together\n",
    "        nonzero_keys = [key for key, value in rate_stripe.iteritems() if value != 0]\n",
    "\n",
    "       \n",
    "\n",
    "        sorted_keys = sorted(nonzero_keys)\n",
    "        mydict = {}\n",
    "        for key,value in rate_stripe.iteritems():\n",
    "            mydict[key] = str(value)\n",
    "        #yield None,mydict\n",
    "        # N * N  complexity matrix calculation\n",
    "        #We are going over each record to find out the common occureances\n",
    "        for i in range(0, len(sorted_keys)):\n",
    "            \n",
    "            left_label = sorted_keys[i]\n",
    "\n",
    "            stripe = {}\n",
    "\n",
    "            for j in range(i + 1, len(sorted_keys)):\n",
    "                right_label = sorted_keys[j]\n",
    "                rkey = right_label+'-'+mydict[right_label]\n",
    "                stripe[rkey] = 1\n",
    "            nkey =  left_label+'-'+mydict[left_label]\n",
    "            if len(stripe) > 0:\n",
    "                yield self.makeKeyn(left_label,10),(nkey,stripe)\n",
    "\n",
    "     \n",
    "\n",
    "        for key in sorted_keys:\n",
    "            break\n",
    "            yield '*',{key:1}\n",
    "            #{u'DocC': 1}\n",
    "        #yield '*', { key: 1 for key in sorted_keys }\n",
    "    def combiner_jaccard(self, left_label, partial_stripes):\n",
    "         #yield left_label,partial_stripes\n",
    "        for key in partial_stripes:\n",
    "            #mydict[key] = str(value)\n",
    "            \n",
    "           \n",
    "            yield key[0],[self.combine_stripes(key[1])]\n",
    "        #yield left_label, self.combine_stripes(partial_stripes)\n",
    "\n",
    "#find out the jaccard values.\n",
    "    def reducer_jaccard(self, left_labeltotal, partial_stripes):\n",
    "        self.increment_counter('reducers_custom', 'counter_name', 1)\n",
    "    \n",
    "        \n",
    "        total_stripe = self.combine_stripess(partial_stripes)\n",
    "#         yield left_labeltotal , total_stripe\n",
    "#         return\n",
    "        #this stores the total length of each word Vector\n",
    "        if left_labeltotal == '*':\n",
    "            self.total_counts = total_stripe\n",
    "            return\n",
    "        \n",
    "        for right_label, intersection_size in total_stripe.iteritems():\n",
    "            left_label,left_total = left_labeltotal.split('-')\n",
    "            right_label,right_total = right_label.split('-')\n",
    "            coordinate = (left_label, right_label)\n",
    "            #union_size = self.total_counts[left_label] + self.total_counts[right_label]\n",
    "\n",
    "            union_size = float(left_total) + float(right_total)\n",
    "            jaccard_distance = float(intersection_size)/float(union_size - intersection_size) #jaccard\n",
    "            dice_coef = (float(intersection_size) * 2 )/float(union_size ) #dice Coefficient\n",
    "            #final = {}\n",
    "            #final['jaccard'] = jaccard_distance\n",
    "            #final['dice'] = dice_coef\n",
    "            #final['average'] = (float(jaccard_distance) + float(dice_coef)) /2\n",
    "            #if (final['jaccard'] == final['dice'] ) and final['dice'] == 1:\n",
    "            #    continue\n",
    "            #else:\n",
    "            yield  coordinate, ( jaccard_distance,dice_coef,(float(jaccard_distance) + float(dice_coef)) /2)\n",
    "           \n",
    "\n",
    "#in-memory combiner\n",
    "    def combine_stripes(self, stripes):\n",
    "        combined_stripe = {}\n",
    "        \n",
    "        #for stripe in stripes:\n",
    "        for key, value in stripes.iteritems():\n",
    "                if key in combined_stripe:\n",
    "                    combined_stripe[key] += value\n",
    "                else:\n",
    "                    combined_stripe[key] = value\n",
    "\n",
    "        return combined_stripe\n",
    "\n",
    "    def combine_stripess(self, stripes_all):\n",
    "        combined_stripe = {}\n",
    "        \n",
    "        for stripe in stripes_all:\n",
    "            for stripes in stripe:\n",
    "                for key, value in stripes.iteritems():\n",
    "                    if key in combined_stripe:\n",
    "                        combined_stripe[key] += value\n",
    "                    else:\n",
    "                        combined_stripe[key] = value\n",
    "\n",
    "        return combined_stripe\n",
    "    def steps(self):\n",
    "        JOBCONF_STEP1 = {\n",
    "            'stream.num.map.output.key.fields':3,\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            #'stream.map.output.field.separator':\"\\t\",\n",
    "            'mapreduce.partition.keypartitioner.options':'-k1,1',\n",
    "            #'mapreduce.partition.keycomparator.options':'-k3,-3nr',\n",
    "            'mapred.reduce.tasks': 10,\n",
    "            'partitioner':'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner',\n",
    "            'mapreduce.job.maps':20,\n",
    "        \n",
    "            }\n",
    "        distance_step = MRStep(\n",
    "                mapper = self.mapper_jaccard,\n",
    "                combiner = self.combiner_jaccard,\n",
    "                reducer = self.reducer_jaccard,\n",
    "          \n",
    "                jobconf= JOBCONF_STEP1\n",
    "                 )\n",
    "             \n",
    "        return [distance_step]\n",
    "#END SUDENT CODE531_SIMILARITY\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    MRsimilarity.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DocA\"\t{\"X\":20, \"Y\":30, \"Z\":5}\r\n",
      "\"DocB\"\t{\"X\":100, \"Y\":20}\r\n",
      "\"DocC\"\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\r\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# Stripes for systems test 3 (given, no need to build stripes)\n",
    "########################################################################\n",
    "\n",
    "with open(\"systems_test_stripes_3\", \"w\") as f:\n",
    "    f.writelines([\n",
    "        '\"DocA\"\\t{\"X\":20, \"Y\":30, \"Z\":5}\\n',\n",
    "        '\"DocB\"\\t{\"X\":100, \"Y\":20}\\n',  \n",
    "        '\"DocC\"\\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\\n'\n",
    "    ])\n",
    "!cat systems_test_stripes_3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /tmp/invertedIndex.nileshbhoyar.20170623.141044.832312\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/invertedIndex.nileshbhoyar.20170623.141044.832312/output...\n",
      "Removing temp directory /tmp/invertedIndex.nileshbhoyar.20170623.141044.832312...\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndex.py -r local systems_test_stripes_3 > systems_test_index_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  3  - Inverted Index\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "             \"M\" |          DocC 4 |                 |                \n",
      "             \"N\" |          DocC 4 |                 |                \n",
      "             \"X\" |          DocA 3 |          DocB 2 |                \n",
      "             \"Y\" |          DocA 3 |          DocB 2 |          DocC 4\n",
      "             \"Z\" |          DocA 3 |          DocC 4 |                \n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Pretty print systems tests for generating Inverted Index\n",
    "##########################################################\n",
    "\n",
    "import json\n",
    "\n",
    "for i in range(3,4):\n",
    "    print \"—\"*100\n",
    "    print \"Systems test \",i,\" - Inverted Index\"\n",
    "    print \"—\"*100  \n",
    "    with open(\"systems_test_index_\"+str(i),\"r\") as f:\n",
    "        lines = sorted(f.readlines())\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            word, doc_list = line.split(\"\\t\")\n",
    "            doc_dict = json.loads(doc_list)\n",
    "            stripe=[]\n",
    "            for doc in doc_dict:\n",
    "                stripe.append([doc, doc_dict[doc]])\n",
    "            stripe=sorted(stripe)\n",
    "            stripe.extend([[\"\",\"\"] for _ in xrange(3 - len(stripe))])\n",
    "\n",
    "            print \"{0:>16} |{1:>16} |{2:>16} |{3:>16}\".format(\n",
    "              (word), stripe[0][0]+\" \"+str(stripe[0][1]), stripe[1][0]+\" \"+str(stripe[1][1]), stripe[2][0]+\" \"+str(stripe[2][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"DocA\", \"DocB\"]\t{\"dice\": 0.80000000000000004, \"jaccard\": 0.66666666666666663}\r\n",
      "[\"DocA\", \"DocC\"]\t{\"dice\": 0.5714285714285714, \"jaccard\": 0.40000000000000002}\r\n",
      "[\"DocB\", \"DocC\"]\t{\"dice\": 0.33333333333333331, \"jaccard\": 0.20000000000000001}\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_similarities_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /tmp/similarity.nileshbhoyar.20170623.141046.325690\n",
      "Running step 1 of 1...\n",
      "Counters: 1\n",
      "\treducers_custom\n",
      "\t\tcounter_name=2\n",
      "Streaming final output from /tmp/similarity.nileshbhoyar.20170623.141046.325690/output...\n",
      "Removing temp directory /tmp/similarity.nileshbhoyar.20170623.141046.325690...\n",
      "[\"DocA\",\"DocB\"]\t[0.6666666667,0.8,0.7333333333]\n",
      "[\"DocA\",\"DocC\"]\t[0.4,0.5714285714,0.4857142857]\n",
      "[\"DocB\",\"DocC\"]\t[0.2,0.3333333333,0.2666666667]\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r local systems_test_index_3  > systems_test_similarities_3\n",
    "!cat systems_test_similarities_3|sort -k3nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  3  - Similarity measures\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "           pair |        jaccard|           Dice|        average\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "[\"DocB\",\"DocC\"] |       0.200000|       0.333333|   0.2666666667\n",
      "[\"DocA\",\"DocC\"] |       0.400000|       0.571429|   0.4857142857\n",
      "[\"DocA\",\"DocB\"] |       0.666667|       0.800000|   0.7333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(3,2,-1):\n",
    "  print '—'*110\n",
    "  print \"Systems test \",i,\" - Similarity measures\"\n",
    "  print '—'*110\n",
    "  print \"{0:>15} |{1:>15}|{2:>15}|{3:>15}\".format(\n",
    "           \"pair\",  \"jaccard\",\"Dice\",\"average\")\n",
    "  print '-'*110\n",
    "\n",
    "  with open(\"systems_test_similarities_\"+str(i),\"r\") as f:\n",
    "      lines = f.readlines()\n",
    "      for line in lines:\n",
    "          line = line.strip()\n",
    "          \n",
    "          pair,stripe = line.split(\"\\t\")\n",
    "        \n",
    "            \n",
    "          result = stripe[1:-1].split(',')\n",
    " \n",
    "          print \"{0:>15} |{1:>15f}|{2:>15f}|{3:>15}\".format(\n",
    "              pair,float(result[0]),float(result[1] ),float(result[2]))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run for all Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !mkdir /home/nileshbhoyar/output/HW55\n",
    "# !mkdir /home/nileshbhoyar/output/HW55/stripes\n",
    "# %time !python buildStripes.py \\\n",
    "#          -r local \\\n",
    "#          --output-dir=\"/home/nileshbhoyar/output/HW55/stripes\"\\\n",
    "#          --no-output \\\n",
    "#          --file features.txt\\\n",
    "#          --file vocabulary.txt\\\n",
    "#         \"/home/nileshbhoyar/data/googlebooks-eng-all-5gram-20090715-88-filtered.txt\" \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `hdfs:/user/nileshbhoyar/output': File exists\n",
      "mkdir: `hdfs:/user/nileshbhoyar/output/HW55': File exists\n",
      "17/06/23 14:10:54 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/nileshbhoyar/output/HW55/stripes' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/nileshbhoyar/.Trash/Current\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/buildStripes.nileshbhoyar.20170623.141055.106020\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nileshbhoyar/tmp/mrjob/buildStripes.nileshbhoyar.20170623.141055.106020/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.map.tasks: mapreduce.job.maps\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob9026761523756038603.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 190\n",
      "  number of splits:190\n",
      "  Submitting tokens for job: job_1497906899862_2495\n",
      "  Submitted application application_1497906899862_2495\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_2495/\n",
      "  Running job: job_1497906899862_2495\n",
      "  Job job_1497906899862_2495 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 2% reduce 0%\n",
      "   map 3% reduce 0%\n",
      "   map 4% reduce 0%\n",
      "   map 5% reduce 0%\n",
      "   map 6% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 8% reduce 0%\n",
      "   map 10% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 13% reduce 0%\n",
      "   map 14% reduce 0%\n",
      "   map 16% reduce 0%\n",
      "   map 17% reduce 0%\n",
      "   map 19% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 21% reduce 0%\n",
      "   map 23% reduce 0%\n",
      "   map 24% reduce 0%\n",
      "   map 25% reduce 0%\n",
      "   map 27% reduce 0%\n",
      "   map 28% reduce 0%\n",
      "   map 29% reduce 0%\n",
      "   map 30% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 35% reduce 0%\n",
      "   map 36% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 41% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 45% reduce 0%\n",
      "   map 47% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 54% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 56% reduce 0%\n",
      "   map 57% reduce 0%\n",
      "   map 58% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 60% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 63% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 65% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 73% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 81% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 83% reduce 0%\n",
      "   map 84% reduce 0%\n",
      "   map 86% reduce 0%\n",
      "   map 91% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 96% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 98% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 20%\n",
      "   map 100% reduce 45%\n",
      "   map 100% reduce 70%\n",
      "   map 100% reduce 95%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_2495 completed successfully\n",
      "  Output directory: hdfs:///user/nileshbhoyar/output/HW55/stripes/\n",
      "Counters: 53\n",
      "\tExecution Counts\n",
      "\t\tmapper calls=58682266\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9159402\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=13678797\n",
      "\t\tFILE: Number of bytes written=66024983\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2156101116\n",
      "\t\tHDFS: Number of bytes written=9159402\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=630\n",
      "\t\tHDFS: Number of write operations=40\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=191\n",
      "\t\tLaunched reduce tasks=20\n",
      "\t\tOther local map tasks=2\n",
      "\t\tRack-local map tasks=189\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=22989321216\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=298273280\n",
      "\t\tTotal time spent by all map tasks (ms)=14967006\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=44901018\n",
      "\t\tTotal time spent by all reduce tasks (ms)=116513\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=582565\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14967006\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=116513\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=4043960\n",
      "\t\tCombine input records=1572616\n",
      "\t\tCombine output records=655328\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=91995\n",
      "\t\tInput split bytes=32000\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=43103110\n",
      "\t\tMap output materialized bytes=24106506\n",
      "\t\tMap output records=1572616\n",
      "\t\tMerged Map outputs=3800\n",
      "\t\tPhysical memory (bytes) snapshot=160255143936\n",
      "\t\tReduce input groups=9999\n",
      "\t\tReduce input records=655328\n",
      "\t\tReduce output records=9999\n",
      "\t\tReduce shuffle bytes=24106506\n",
      "\t\tShuffled Maps =3800\n",
      "\t\tSpilled Records=1310656\n",
      "\t\tTotal committed heap usage (bytes)=337723260928\n",
      "\t\tVirtual memory (bytes) snapshot=484557623296\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/nileshbhoyar/tmp/mrjob/buildStripes.nileshbhoyar.20170623.141055.106020...\n",
      "Removing temp directory /tmp/buildStripes.nileshbhoyar.20170623.141055.106020...\n",
      "CPU times: user 3.47 s, sys: 851 ms, total: 4.33 s\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir hdfs:///user/nileshbhoyar/output\n",
    "!hdfs dfs -mkdir hdfs:///user/nileshbhoyar/output/HW55\n",
    "!hdfs dfs -rm -r hdfs:///user/nileshbhoyar/output/HW55/stripes\n",
    "%time !python buildStripes.py\\\n",
    "        -r hadoop \\\n",
    "        --output-dir=\"hdfs:///user/nileshbhoyar/output/HW55/stripes/\"\\\n",
    "        --no-output \\\n",
    "        --file features.txt\\\n",
    "        --file vocabulary.txt\\\n",
    "        \"hdfs:///user/cendylin/filtered-5Grams/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !mkdir /home/nileshbhoyar/output/HW55/index\n",
    "# %time !python invertedIndex.py\\\n",
    "#          -r local \\\n",
    "#          --output-dir=\"/home/nileshbhoyar/output/HW55/index\"\\\n",
    "#          --no-output \\\n",
    "#          --file features.txt\\\n",
    "#          --file vocabulary.txt\\\n",
    "#          \"/home/nileshbhoyar/output/HW55/stripes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/06/23 14:13:36 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/nileshbhoyar/output/HW55/index' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/nileshbhoyar/.Trash/Current\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/invertedIndex.nileshbhoyar.20170623.141337.092588\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nileshbhoyar/tmp/mrjob/invertedIndex.nileshbhoyar.20170623.141337.092588/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob4392068789191135455.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 20\n",
      "  number of splits:20\n",
      "  Submitting tokens for job: job_1497906899862_2496\n",
      "  Submitted application application_1497906899862_2496\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_2496/\n",
      "  Running job: job_1497906899862_2496\n",
      "  Job job_1497906899862_2496 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 20% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 70% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 30%\n",
      "   map 100% reduce 40%\n",
      "   map 100% reduce 90%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_2496 completed successfully\n",
      "  Output directory: hdfs:///user/nileshbhoyar/output/HW55/index/\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=9159402\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=8280089\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4558460\n",
      "\t\tFILE: Number of bytes written=11392910\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=9162122\n",
      "\t\tHDFS: Number of bytes written=8280089\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=90\n",
      "\t\tHDFS: Number of write operations=20\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=20\n",
      "\t\tLaunched reduce tasks=10\n",
      "\t\tRack-local map tasks=20\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=800793600\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=155957760\n",
      "\t\tTotal time spent by all map tasks (ms)=521350\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1564050\n",
      "\t\tTotal time spent by all reduce tasks (ms)=60921\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=304605\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=521350\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=60921\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=88300\n",
      "\t\tCombine input records=548971\n",
      "\t\tCombine output records=20000\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=2714\n",
      "\t\tInput split bytes=2720\n",
      "\t\tMap input records=9999\n",
      "\t\tMap output bytes=15370985\n",
      "\t\tMap output materialized bytes=2808030\n",
      "\t\tMap output records=548971\n",
      "\t\tMerged Map outputs=200\n",
      "\t\tPhysical memory (bytes) snapshot=19319091200\n",
      "\t\tReduce input groups=1000\n",
      "\t\tReduce input records=20000\n",
      "\t\tReduce output records=1000\n",
      "\t\tReduce shuffle bytes=2808030\n",
      "\t\tShuffled Maps =200\n",
      "\t\tSpilled Records=40000\n",
      "\t\tTotal committed heap usage (bytes)=52187627520\n",
      "\t\tVirtual memory (bytes) snapshot=77436125184\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing HDFS temp directory hdfs:///user/nileshbhoyar/tmp/mrjob/invertedIndex.nileshbhoyar.20170623.141337.092588...\n",
      "Removing temp directory /tmp/invertedIndex.nileshbhoyar.20170623.141337.092588...\n",
      "CPU times: user 1.8 s, sys: 373 ms, total: 2.17 s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/nileshbhoyar/output/HW55/index\n",
    "\n",
    "%time !python invertedIndex.py\\\n",
    "        -r hadoop \\\n",
    "        --output-dir=\"hdfs:///user/nileshbhoyar/output/HW55/index/\"\\\n",
    "        --no-output \\\n",
    "        --file features.txt\\\n",
    "        --file vocabulary.txt\\\n",
    "        \"hdfs:///user/nileshbhoyar/output/HW55/stripes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/06/23 14:14:58 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 5760 minutes, Emptier interval = 360 minutes.\n",
      "Moved: 'hdfs://nn-ia.s3s.altiscale.com:8020/user/nileshbhoyar/output/HW55/similarity' to trash at: hdfs://nn-ia.s3s.altiscale.com:8020/user/nileshbhoyar/.Trash/Current\n",
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/similarity.nileshbhoyar.20170623.141458.960607\n",
      "Using Hadoop version 2.7.3\n",
      "Copying local files to hdfs:///user/nileshbhoyar/tmp/mrjob/similarity.nileshbhoyar.20170623.141458.960607/files/...\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.3:\n",
      "The have been translated as follows\n",
      " mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar] /tmp/streamjob6901628871665240612.jar tmpDir=null\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Timeline service address: http://rm-ia.s3s.altiscale.com:8188/ws/v1/timeline/\n",
      "  Connecting to ResourceManager at rm-ia.s3s.altiscale.com/10.251.255.108:8032\n",
      "  Connecting to Application History server at rm-ia.s3s.altiscale.com/10.251.255.108:10200\n",
      "  Loaded native gpl library from the embedded binaries\n",
      "  Successfully loaded & initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]\n",
      "  Total input paths to process : 10\n",
      "  number of splits:24\n",
      "  Submitting tokens for job: job_1497906899862_2498\n",
      "  Submitted application application_1497906899862_2498\n",
      "  The url to track the job: http://rm-ia.s3s.altiscale.com:8088/proxy/application_1497906899862_2498/\n",
      "  Running job: job_1497906899862_2498\n",
      "  Job job_1497906899862_2498 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 1% reduce 0%\n",
      "   map 11% reduce 0%\n",
      "   map 18% reduce 0%\n",
      "   map 31% reduce 0%\n",
      "   map 32% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 39% reduce 0%\n",
      "   map 43% reduce 0%\n",
      "   map 48% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 52% reduce 0%\n",
      "   map 55% reduce 0%\n",
      "   map 59% reduce 0%\n",
      "   map 61% reduce 0%\n",
      "   map 62% reduce 0%\n",
      "   map 64% reduce 0%\n",
      "   map 66% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 68% reduce 0%\n",
      "   map 69% reduce 0%\n",
      "   map 71% reduce 0%\n",
      "   map 72% reduce 0%\n",
      "   map 74% reduce 0%\n",
      "   map 75% reduce 0%\n",
      "   map 76% reduce 0%\n",
      "   map 78% reduce 0%\n",
      "   map 79% reduce 0%\n",
      "   map 82% reduce 0%\n",
      "   map 85% reduce 0%\n",
      "   map 86% reduce 0%\n",
      "   map 90% reduce 0%\n",
      "   map 92% reduce 0%\n",
      "   map 93% reduce 0%\n",
      "   map 96% reduce 0%\n",
      "   map 97% reduce 0%\n",
      "   map 99% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 7%\n",
      "   map 100% reduce 33%\n",
      "   map 100% reduce 47%\n",
      "   map 100% reduce 54%\n",
      "   map 100% reduce 67%\n",
      "   map 100% reduce 68%\n",
      "   map 100% reduce 69%\n",
      "   map 100% reduce 70%\n",
      "   map 100% reduce 71%\n",
      "   map 100% reduce 72%\n",
      "   map 100% reduce 73%\n",
      "   map 100% reduce 74%\n",
      "   map 100% reduce 75%\n",
      "   map 100% reduce 76%\n",
      "   map 100% reduce 77%\n",
      "   map 100% reduce 78%\n",
      "   map 100% reduce 79%\n",
      "   map 100% reduce 80%\n",
      "   map 100% reduce 81%\n",
      "   map 100% reduce 82%\n",
      "   map 100% reduce 83%\n",
      "   map 100% reduce 84%\n",
      "   map 100% reduce 85%\n",
      "   map 100% reduce 86%\n",
      "   map 100% reduce 87%\n",
      "   map 100% reduce 88%\n",
      "   map 100% reduce 89%\n",
      "   map 100% reduce 90%\n",
      "   map 100% reduce 91%\n",
      "   map 100% reduce 92%\n",
      "   map 100% reduce 93%\n",
      "   map 100% reduce 94%\n",
      "   map 100% reduce 95%\n",
      "   map 100% reduce 96%\n",
      "   map 100% reduce 97%\n",
      "   map 100% reduce 98%\n",
      "   map 100% reduce 99%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497906899862_2498 completed successfully\n",
      "  Output directory: hdfs:///user/nileshbhoyar/output/HW55/similarity/\n",
      "Counters: 51\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=9714472\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2537863027\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1109482284\n",
      "\t\tFILE: Number of bytes written=2171953748\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=9717688\n",
      "\t\tHDFS: Number of bytes written=2537863027\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=102\n",
      "\t\tHDFS: Number of write operations=20\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=5\n",
      "\t\tLaunched map tasks=29\n",
      "\t\tLaunched reduce tasks=10\n",
      "\t\tRack-local map tasks=29\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10632354816\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9477611520\n",
      "\t\tTotal time spent by all map tasks (ms)=6922106\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=20766318\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3702192\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=18510960\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6922106\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3702192\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=9382560\n",
      "\t\tCombine input records=547971\n",
      "\t\tCombine output records=547971\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=19417\n",
      "\t\tInput split bytes=3216\n",
      "\t\tMap input records=1000\n",
      "\t\tMap output bytes=2638077492\n",
      "\t\tMap output materialized bytes=1057928394\n",
      "\t\tMap output records=547971\n",
      "\t\tMerged Map outputs=240\n",
      "\t\tPhysical memory (bytes) snapshot=27444760576\n",
      "\t\tReduce input groups=9996\n",
      "\t\tReduce input records=547971\n",
      "\t\tReduce output records=28459270\n",
      "\t\tReduce shuffle bytes=1057928394\n",
      "\t\tShuffled Maps =240\n",
      "\t\tSpilled Records=1095942\n",
      "\t\tTotal committed heap usage (bytes)=57501810688\n",
      "\t\tVirtual memory (bytes) snapshot=86251646976\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\treducers_custom\n",
      "\t\tcounter_name=9996\n",
      "Removing HDFS temp directory hdfs:///user/nileshbhoyar/tmp/mrjob/similarity.nileshbhoyar.20170623.141458.960607...\n",
      "Removing temp directory /tmp/similarity.nileshbhoyar.20170623.141458.960607...\n",
      "CPU times: user 17.3 s, sys: 4.09 s, total: 21.4 s\n",
      "Wall time: 12min 58s\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/nileshbhoyar/output/HW55/similarity\n",
    "%time !python similarity.py\\\n",
    "             -r hadoop \\\n",
    "             --output-dir=\"hdfs:///user/nileshbhoyar/output/HW55/similarity/\"\\\n",
    "             --no-output \"hdfs:///user/nileshbhoyar/output/HW55/index\"\n",
    "        \n",
    "# !hdfs dfs -rm -r hdfs:///user/nileshbhoyar/output/HW55/similarity\n",
    "# !rm -r /home/nileshbhoyar/output/HW55/similarity/\n",
    "# %time !python similarity.py\\\n",
    "#          -r local \\\n",
    "#          --output-dir=\"/home/nileshbhoyar/output/HW55/similarity/\"\\\n",
    "#          --no-output \\\n",
    "#          --file features.txt\\\n",
    "#          --file vocabulary.txt\\\n",
    "#          \"/home/nileshbhoyar/output/HW55/index/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm /home/nileshbhoyar/output/HW55/similarity/part-00000\n",
    "!hdfs dfs -cat hdfs:///user/nileshbhoyar/output/HW55/similarity/* > /home/nileshbhoyar/output/HW55/similarity/part-00000\n",
    "!sort -k4nr /home/nileshbhoyar/output/HW55/similarity/part-00000 > final_sorted.txt\n",
    "!head -1000 final_sorted.txt > result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "print \"{0:>15} |{1:>15}|{2:>15}|{3:>15}\".format(\n",
    "           \"pair\",  \"jaccard\",\"Dice\",\"average\")\n",
    "print '-'*110\n",
    "ndict = defaultdict(float)\n",
    "print \"{0:>15} |{1:>15}|{2:>15}|{3:>15}\".format(\n",
    "           \"pair\",  \"jaccard\",\"Dice\",\"average\")\n",
    "print '-'*110\n",
    "\n",
    "with open(\"result.txt\",\"r\") as f:\n",
    "      lines = f.readlines()\n",
    "      for line in lines:\n",
    "          line = line.strip()\n",
    "          \n",
    "          pair,stripe = line.split(\"\\t\")\n",
    "        \n",
    "            \n",
    "          result = stripe[1:-1].split(',')\n",
    " \n",
    "          print \"{0:>15} |{1:>15f}|{2:>15f}|{3:>15}\".format(\n",
    "              pair,float(result[0]),float(result[1] ),float(result[2]))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print \"***********************TOP 20 RECORDS****************************\"\n",
    "!head -20 final_sorted.txt\n",
    "print \"\\n\\n\"\n",
    "print \"***********************Bottom 20 Records****************************\"\n",
    "print \"\\n\\n\"\n",
    "!tail -20 final_sorted.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run time for Stripes Job\n",
    "\n",
    "    Map 190\n",
    "    Reducers 20\n",
    "\n",
    "    Job Name: \tstreamjob7789550624839722782.jar\n",
    "    User Name: \tnileshbhoyar\n",
    "    Queue: \tberkeley\n",
    "    State: \tSUCCEEDED\n",
    "    Uberized: \tfalse\n",
    "    Submitted: \tThu Jun 22 18:17:30 UTC 2017\n",
    "    Started: \tThu Jun 22 18:17:39 UTC 2017\n",
    "    Finished: \tThu Jun 22 18:19:38 UTC 2017\n",
    "    Elapsed: \t1mins, 59sec\n",
    "    Diagnostics: \t\n",
    "    Average Map Time \t1mins, 17sec\n",
    "    Average Shuffle Time \t3sec\n",
    "    Average Merge Time \t0sec\n",
    "    Average Reduce Time \t2sec \n",
    "    \n",
    " ## Run time for Inverted index job\n",
    " \n",
    "     NO OF MAPS 20\n",
    "     NO of REDUCERS 10\n",
    "     \n",
    "     Job Name: \tstreamjob6234810784634808372.jar\n",
    "    User Name: \tnileshbhoyar\n",
    "    Queue: \tberkeley\n",
    "    State: \tSUCCEEDED\n",
    "    Uberized: \tfalse\n",
    "    Submitted: \tThu Jun 22 18:20:11 UTC 2017\n",
    "    Started: \tThu Jun 22 18:20:19 UTC 2017\n",
    "    Finished: \tThu Jun 22 18:23:57 UTC 2017\n",
    "    Elapsed: \t3mins, 37sec\n",
    "    Diagnostics: \t\n",
    "    Average Map Time \t39sec\n",
    "    Average Shuffle Time \t11sec\n",
    "    Average Merge Time \t0sec\n",
    "    Average Reduce Time \t3sec \n",
    "    \n",
    "    \n",
    "## Run time for similarity jobs\n",
    "    \n",
    "    \n",
    "    Job Name:\tstreamjob5246956418942942580.jar\n",
    "    User Name:\tnileshbhoyar\n",
    "    Queue:\tberkeley\n",
    "    State:\tSUCCEEDED\n",
    "    Uberized:\tfalse\n",
    "    Submitted:\tThu Jun 22 18:24:25 UTC 2017\n",
    "    Started:\tThu Jun 22 18:25:40 UTC 2017\n",
    "    Finished:\tThu Jun 22 18:43:19 UTC 2017\n",
    "    Elapsed:\t17mins, 38sec\n",
    "    Diagnostics:\t\n",
    "    Average Map Time\t5mins, 38sec\n",
    "    Average Shuffle Time\t4sec\n",
    "    Average Merge Time\t1sec\n",
    "    Average Reduce Time\t8mins, 13sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top/Bottom 20 results - Similarity measures - sorted by cosine\n",
    "(From the entire data set)\n",
    "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "                          pair |         cosine |        jaccard |        overlap |           dice |        average\n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "                   cons - pros |       0.894427 |       0.800000 |       1.000000 |       0.888889 |       0.895829\n",
    "            forties - twenties |       0.816497 |       0.666667 |       1.000000 |       0.800000 |       0.820791\n",
    "                    own - time |       0.809510 |       0.670563 |       0.921168 |       0.802799 |       0.801010\n",
    "                 little - time |       0.784197 |       0.630621 |       0.926101 |       0.773473 |       0.778598\n",
    "                  found - time |       0.783434 |       0.636364 |       0.883788 |       0.777778 |       0.770341\n",
    "                 nova - scotia |       0.774597 |       0.600000 |       1.000000 |       0.750000 |       0.781149\n",
    "                   hong - kong |       0.769800 |       0.615385 |       0.888889 |       0.761905 |       0.758995\n",
    "                   life - time |       0.769666 |       0.608789 |       0.925081 |       0.756829 |       0.765091\n",
    "                  time - world |       0.755476 |       0.585049 |       0.937500 |       0.738209 |       0.754058\n",
    "                  means - time |       0.752181 |       0.587117 |       0.902597 |       0.739854 |       0.745437\n",
    "                   form - time |       0.749943 |       0.588418 |       0.876733 |       0.740885 |       0.738995\n",
    "       infarction - myocardial |       0.748331 |       0.560000 |       1.000000 |       0.717949 |       0.756570\n",
    "                 people - time |       0.745788 |       0.573577 |       0.923875 |       0.729010 |       0.743063\n",
    "                 angeles - los |       0.745499 |       0.586207 |       0.850000 |       0.739130 |       0.730209\n",
    "                  little - own |       0.739343 |       0.585834 |       0.767296 |       0.738834 |       0.707827\n",
    "                    life - own |       0.737053 |       0.582217 |       0.778502 |       0.735951 |       0.708430\n",
    "          anterior - posterior |       0.733388 |       0.576471 |       0.790323 |       0.731343 |       0.707881\n",
    "                  power - time |       0.719611 |       0.533623 |       0.933586 |       0.695898 |       0.720680\n",
    "              dearly - install |       0.707107 |       0.500000 |       1.000000 |       0.666667 |       0.718443\n",
    "                   found - own |       0.704802 |       0.544134 |       0.710949 |       0.704776 |       0.666165\n",
    "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "           arrival - essential |       0.008258 |       0.004098 |       0.009615 |       0.008163 |       0.007534\n",
    "         governments - surface |       0.008251 |       0.003534 |       0.014706 |       0.007042 |       0.008383\n",
    "                king - lesions |       0.008178 |       0.003106 |       0.017857 |       0.006192 |       0.008833\n",
    "              clinical - stood |       0.008178 |       0.003831 |       0.011905 |       0.007634 |       0.007887\n",
    "               till - validity |       0.008172 |       0.003367 |       0.015625 |       0.006711 |       0.008469\n",
    "            evidence - started |       0.008159 |       0.003802 |       0.012048 |       0.007576 |       0.007896\n",
    "               forces - record |       0.008152 |       0.003876 |       0.011364 |       0.007722 |       0.007778\n",
    "               primary - stone |       0.008146 |       0.004065 |       0.009091 |       0.008097 |       0.007350\n",
    "             beneath - federal |       0.008134 |       0.004082 |       0.008403 |       0.008130 |       0.007187\n",
    "                factors - rose |       0.008113 |       0.004032 |       0.009346 |       0.008032 |       0.007381\n",
    "           evening - functions |       0.008069 |       0.004049 |       0.008333 |       0.008065 |       0.007129\n",
    "                   bone - told |       0.008061 |       0.003704 |       0.012346 |       0.007380 |       0.007873\n",
    "             building - occurs |       0.008002 |       0.003891 |       0.010309 |       0.007752 |       0.007489\n",
    "                 company - fig |       0.007913 |       0.003257 |       0.015152 |       0.006494 |       0.008204\n",
    "               chronic - north |       0.007803 |       0.003268 |       0.014493 |       0.006515 |       0.008020\n",
    "             evaluation - king |       0.007650 |       0.003030 |       0.015625 |       0.006042 |       0.008087\n",
    "             resulting - stood |       0.007650 |       0.003663 |       0.010417 |       0.007299 |       0.007257\n",
    "                 agent - round |       0.007515 |       0.003289 |       0.012821 |       0.006557 |       0.007546\n",
    "         afterwards - analysis |       0.007387 |       0.003521 |       0.010204 |       0.007018 |       0.007032\n",
    "            posterior - spirit |       0.007156 |       0.002660 |       0.016129 |       0.005305 |       0.007812"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.6  <a name=\"5.6\"></a> Evaluation of synonyms that your discovered\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "In this part of the assignment you will evaluate the success of you synonym detector (developed in response to HW5.4).\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined by your measure in HW5.4, and use the synonyms function in the accompanying python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Calculate performance measures:\n",
    "$$Precision (P) = \\frac{TP}{TP + FP} $$  \n",
    "$$Recall (R) = \\frac{TP}{TP + FN} $$  \n",
    "$$F1 = \\frac{2 * ( precision * recall )}{precision + recall}$$\n",
    "\n",
    "\n",
    "We calculate Precision by counting the number of hits and dividing by the number of occurances in our top1000 (opportunities)   \n",
    "We calculate Recall by counting the number of hits, and dividing by the number of synonyms in wordnet (syns)\n",
    "\n",
    "\n",
    "Other diagnostic measures not implemented here:  https://en.wikipedia.org/wiki/F1_score#Diagnostic_Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Performance measures '''\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "import re\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n",
    "hits = []\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "\n",
    "TOTAL = 0\n",
    "flag = False # so we don't double count, but at the same time don't miss hits\n",
    "\n",
    "top1000sims = []\n",
    "lisst =[]\n",
    "with open(\"result.txt\",\"r\") as f:\n",
    "   for line in f.readlines():#\n",
    "\n",
    "       line = line.strip()\n",
    "       pair,stripe = line.split(\"\\t\")\n",
    "        \n",
    "            \n",
    "       result = stripe[1:-1].split(',')\n",
    "#        avg,lisst = line.split(\"\\t\")\n",
    "#        lisst = json.loads(lisst)\n",
    "       lisst.append(result[2])\n",
    "       top1000sims.append(pair)\n",
    "    \n",
    "\n",
    "measures = {}\n",
    "not_in_wordnet = []\n",
    "\n",
    "for line in top1000sims:\n",
    "    TOTAL += 1\n",
    "\n",
    "    #pair = line[0]\n",
    "    #words = pair.split(\" - \")\n",
    "    #rec = line[0]\n",
    "    #print line\n",
    "    #break\n",
    "    words = re.findall(\"\\w+\", line.lower())\n",
    "#     print words\n",
    "#     break\n",
    "   \n",
    "    for word in words:\n",
    "        if word not in measures:\n",
    "            measures[word] = {\"syns\":0,\"opps\": 0,\"hits\":0}\n",
    "        measures[word][\"opps\"] += 1 \n",
    "    \n",
    "    syns0 = synonyms(words[0])\n",
    "    measures[words[1]][\"syns\"] = len(syns0)\n",
    "    if len(syns0) == 0:\n",
    "        not_in_wordnet.append(words[0])\n",
    "        \n",
    "    if words[1] in syns0:\n",
    "        TP += 1\n",
    "        hits.append(line)\n",
    "        flag = True\n",
    "        measures[words[1]][\"hits\"] += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    syns1 = synonyms(words[1]) \n",
    "    measures[words[0]][\"syns\"] = len(syns1)\n",
    "    if len(syns1) == 0:\n",
    "        not_in_wordnet.append(words[1])\n",
    "\n",
    "    if words[0] in syns1:\n",
    "        if flag == False:\n",
    "            TP += 1\n",
    "            hits.append(line)\n",
    "            measures[words[0]][\"hits\"] += 1\n",
    "            \n",
    "    flag = False    \n",
    "\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for key in measures:\n",
    "    p,r,f = 0,0,0\n",
    "    if measures[key][\"hits\"] > 0 and measures[key][\"syns\"] > 0:\n",
    "        p = measures[key][\"hits\"]/measures[key][\"opps\"]\n",
    "        r = measures[key][\"hits\"]/measures[key][\"syns\"]\n",
    "        f = 2 * (p*r)/(p+r)\n",
    "    \n",
    "    # For calculating measures, only take into account words that have synonyms in wordnet\n",
    "    if measures[key][\"syns\"] > 0:\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        f1.append(f)\n",
    "\n",
    "    \n",
    "# Take the mean of each measure    \n",
    "print \"—\"*110    \n",
    "print \"Number of Hits:\",TP, \"out of top\",TOTAL\n",
    "print \"Number of words without synonyms:\",len(not_in_wordnet)\n",
    "print \"—\"*110 \n",
    "print \"Precision\\t\", np.mean(precision)\n",
    "print \"Recall\\t\\t\", np.mean(recall)\n",
    "print \"F1\\t\\t\", np.mean(f1)\n",
    "print \"—\"*110  \n",
    "\n",
    "print \"Words without synonyms:\"\n",
    "print \"-\"*100\n",
    "\n",
    "for word in not_in_wordnet:\n",
    "    print synonyms(word),word\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-67-0c3edb848036>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-67-0c3edb848036>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ——————————————————————————————————————————————————————————————————————————————————————————————————————————————\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Number of Hits: 31 out of top 1000\n",
    "Number of words without synonyms: 67\n",
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Precision\t0.0280214404967\n",
    "Recall\t\t0.0178598869579\n",
    "F1\t\t0.013965517619\n",
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Words without synonyms:\n",
    "----------------------------------------------------------------------------------------------------\n",
    "[] scotia\n",
    "[] hong\n",
    "[] kong\n",
    "[] angeles\n",
    "[] los\n",
    "[] nor\n",
    "[] themselves\n",
    "[] \n",
    "......."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.7  <a name=\"5.7\"></a> OPTIONAL: using different vocabulary subsets\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "Repeat HW5 using vocabulary words ranked from 8001,-10,000;  7001,-10,000; 6001,-10,000; 5001,-10,000; 3001,-10,000; and 1001,-10,000;\n",
    "Dont forget to report you Cluster configuration.\n",
    "\n",
    "Generate the following graphs:\n",
    "-- vocabulary size (X-Axis) versus CPU time for indexing\n",
    "-- vocabulary size (X-Axis) versus number of pairs processed\n",
    "-- vocabulary size (X-Axis) versus F1 measure, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.8  <a name=\"5.8\"></a> OPTIONAL: filter stopwords\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    "> from nltk.corpus import stopwords\n",
    ">> stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.9 <a name=\"5.9\"></a> OPTIONAL \n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "There are many good ways to build our synonym detectors, so for this optional homework, \n",
    "measure co-occurrence by (left/right/all) consecutive words only, \n",
    "or make stripes according to word co-occurrences with the accompanying \n",
    "2-, 3-, or 4-grams (note here that your output will no longer \n",
    "be interpretable as a network) inside of the 5-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.10 <a name=\"5.10\"></a> OPTIONAL \n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Once again, benchmark your top 10,000 associations (as in 5.5), this time for your\n",
    "results from 5.6. Has your detector improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "511px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
