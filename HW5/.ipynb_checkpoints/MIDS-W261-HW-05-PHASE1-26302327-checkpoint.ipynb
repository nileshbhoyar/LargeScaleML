{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#MIDS---w261-Machine-Learning-At-Scale\" data-toc-modified-id=\"MIDS---w261-Machine-Learning-At-Scale-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MIDS - w261 Machine Learning At Scale</a></div><div class=\"lev2 toc-item\"><a href=\"#Assignment---HW5\" data-toc-modified-id=\"Assignment---HW5-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Assignment - HW5</a></div><div class=\"lev1 toc-item\"><a href=\"#1-Instructions\" data-toc-modified-id=\"1-Instructions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>1 Instructions</a></div><div class=\"lev3 toc-item\"><a href=\"#INSTRUCTIONS-for-SUBMISSIONS\" data-toc-modified-id=\"INSTRUCTIONS-for-SUBMISSIONS-201\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>INSTRUCTIONS for SUBMISSIONS</a></div><div class=\"lev1 toc-item\"><a href=\"#HW-Problems\" data-toc-modified-id=\"HW-Problems-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>HW Problems</a></div><div class=\"lev2 toc-item\"><a href=\"#HW5.0--data-warehouse;-star-schema\" data-toc-modified-id=\"HW5.0--data-warehouse;-star-schema-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>HW5.0  data warehouse; star schema</a></div><div class=\"lev2 toc-item\"><a href=\"#HW5.1-Databases:-3NF;-denormalized\" data-toc-modified-id=\"HW5.1-Databases:-3NF;-denormalized-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>HW5.1 Databases: 3NF; denormalized</a></div><div class=\"lev2 toc-item\"><a href=\"#HW5.2--Memory-backed-map-side\" data-toc-modified-id=\"HW5.2--Memory-backed-map-side-33\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>HW5.2  Memory-backed map-side</a></div><div class=\"lev2 toc-item\"><a href=\"#HW5.2.1-(OPTIONAL)-Almost-stateless-reducer-side-join\" data-toc-modified-id=\"HW5.2.1-(OPTIONAL)-Almost-stateless-reducer-side-join-34\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>HW5.2.1 (OPTIONAL) Almost stateless reducer-side join</a></div><div class=\"lev2 toc-item\"><a href=\"#5.3-Pairwise-similarity----PHASE-1\" data-toc-modified-id=\"5.3-Pairwise-similarity----PHASE-1-35\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>5.3 Pairwise similarity  - PHASE 1</a></div><div class=\"lev3 toc-item\"><a href=\"#(1)-Using-the-systems-tests-data-sets,-write-mrjob-code-to-build-the-stripes\" data-toc-modified-id=\"(1)-Using-the-systems-tests-data-sets,-write-mrjob-code-to-build-the-stripes-351\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>(1) Using the systems tests data sets, write mrjob code to build the stripes</a></div><div class=\"lev3 toc-item\"><a href=\"#(2)-Write-mrjob-code-to-build-an-inverted-index-from-the-stripes\" data-toc-modified-id=\"(2)-Write-mrjob-code-to-build-an-inverted-index-from-the-stripes-352\"><span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;</span>(2) Write mrjob code to build an inverted index from the stripes</a></div><div class=\"lev3 toc-item\"><a href=\"#(3)-Using-two-(symmetric)-comparison-methods-of-your-choice-(e.g.,-correlations,-distances,-similarities),-pairwise-compare-all-stripes-(vectors),-and-output-to-a-file.\" data-toc-modified-id=\"(3)-Using-two-(symmetric)-comparison-methods-of-your-choice-(e.g.,-correlations,-distances,-similarities),-pairwise-compare-all-stripes-(vectors),-and-output-to-a-file.-353\"><span class=\"toc-item-num\">3.5.3&nbsp;&nbsp;</span>(3) Using two (symmetric) comparison methods of your choice (e.g., correlations, distances, similarities), pairwise compare all stripes (vectors), and output to a file.</a></div><div class=\"lev2 toc-item\"><a href=\"#HW5.3.1---Run-Systems-tests-locally-on-small-datasets-(PHASE1)\" data-toc-modified-id=\"HW5.3.1---Run-Systems-tests-locally-on-small-datasets-(PHASE1)-36\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>HW5.3.1   Run Systems tests locally on small datasets (PHASE1)</a></div><div class=\"lev4 toc-item\"><a href=\"#1:-unit/systems-first-10-lines\" data-toc-modified-id=\"1:-unit/systems-first-10-lines-3601\"><span class=\"toc-item-num\">3.6.0.1&nbsp;&nbsp;</span>1: unit/systems first-10-lines</a></div><div class=\"lev4 toc-item\"><a href=\"#2:-unit/systems-atlas-boon\" data-toc-modified-id=\"2:-unit/systems-atlas-boon-3602\"><span class=\"toc-item-num\">3.6.0.2&nbsp;&nbsp;</span>2: unit/systems atlas-boon</a></div><div class=\"lev4 toc-item\"><a href=\"#3:-unit/systems-stripe-docs-test\" data-toc-modified-id=\"3:-unit/systems-stripe-docs-test-3603\"><span class=\"toc-item-num\">3.6.0.3&nbsp;&nbsp;</span>3: unit/systems stripe-docs-test</a></div><div class=\"lev3 toc-item\"><a href=\"#(1)-build-stripes-for-all-the-test-data-sets---run-the-commands-and-insure-that-your-output-matches-the-output-below\" data-toc-modified-id=\"(1)-build-stripes-for-all-the-test-data-sets---run-the-commands-and-insure-that-your-output-matches-the-output-below-361\"><span class=\"toc-item-num\">3.6.1&nbsp;&nbsp;</span>(1) build stripes for all the test data sets - run the commands and insure that your output matches the output below</a></div><div class=\"lev3 toc-item\"><a href=\"#(2)-Build-Inverted-Index---run-the-commands-and-insure-that-your-output-matches-the-output-below\" data-toc-modified-id=\"(2)-Build-Inverted-Index---run-the-commands-and-insure-that-your-output-matches-the-output-below-362\"><span class=\"toc-item-num\">3.6.2&nbsp;&nbsp;</span>(2) Build Inverted Index - run the commands and insure that your output matches the output below</a></div><div class=\"lev3 toc-item\"><a href=\"#Inverted-Index\" data-toc-modified-id=\"Inverted-Index-363\"><span class=\"toc-item-num\">3.6.3&nbsp;&nbsp;</span>Inverted Index</a></div><div class=\"lev3 toc-item\"><a href=\"#(3)-Calculate-similarities---run-the-commands-and-insure-that-your-output-matches-the-output-below\" data-toc-modified-id=\"(3)-Calculate-similarities---run-the-commands-and-insure-that-your-output-matches-the-output-below-364\"><span class=\"toc-item-num\">3.6.4&nbsp;&nbsp;</span>(3) Calculate similarities - run the commands and insure that your output matches the output below</a></div><div class=\"lev4 toc-item\"><a href=\"#NOTE:-you-must-run-in-hadoop-mode-to-generate-sorted-similarities\" data-toc-modified-id=\"NOTE:-you-must-run-in-hadoop-mode-to-generate-sorted-similarities-3641\"><span class=\"toc-item-num\">3.6.4.1&nbsp;&nbsp;</span>NOTE: you must run in hadoop mode to generate sorted similarities</a></div><div class=\"lev3 toc-item\"><a href=\"#Pairwise-Similairity\" data-toc-modified-id=\"Pairwise-Similairity-365\"><span class=\"toc-item-num\">3.6.5&nbsp;&nbsp;</span>Pairwise Similairity</a></div><div class=\"lev1 toc-item\"><a href=\"#===-END-OF-PHASE-1-===\" data-toc-modified-id=\"===-END-OF-PHASE-1-===-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>=== END OF PHASE 1 ===</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW5\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  *Your Name Goes Here*   \n",
    "__Class:__ MIDS w261 (Section *Your Section Goes Here*, e.g., Fall 2016 Group 1)     \n",
    "__Email:__  *Your UC Berkeley Email Goes Here*@iSchool.Berkeley.edu     \n",
    "__StudentId__  26302327    __End of StudentId__     \n",
    "__Week:__   5\n",
    "\n",
    "__NOTE:__ please replace `1234567` with your student id above      \n",
    "__Due Time:__ HW is due the Tuesday of the following week by 8AM (West coast time). I.e., Tuesday, Feb 14, 2017 in the case of this homework. \n",
    "\n",
    "* __HW5 Phase 1__ \n",
    "This can be done on a local machine (with a unit test on the cloud such as AltaScale's PaaS or on AWS) and is due Tuesday, Week 6 by 8AM (West coast time). It will primarily focus on building a unit/systems and for pairwise similarity calculations pipeline (for stripe documents)\n",
    "\n",
    "* __HW5 Phase 2__ \n",
    "This will require the Altiscale cluster and will be due Tuesday, Feb 21 by 8AM (West coast time). \n",
    "The focus of  HW5 Phase 2  will be to scale up the unit/systems tests to the Google 5 gram corpus. \n",
    "\n",
    "# 1 Instructions\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale   \n",
    "DATSCIW261 ASSIGNMENT #5\n",
    "\n",
    "\n",
    "### INSTRUCTIONS for SUBMISSIONS \n",
    "Follow the instructions for submissions carefully.\n",
    "\n",
    "Each student has a `HW-<user>` repository for all assignments.   \n",
    "\n",
    "Push the following to your HW github repo into the master branch:\n",
    "* Your local HW5 directory. Your repo file structure should look like this:\n",
    "\n",
    "```\n",
    "HW-<user>\n",
    "    --HW3\n",
    "       |__MIDS-W261-HW-03-<Student_id>.ipynb\n",
    "       |__MIDS-W261-HW-03-<Student_id>.pdf\n",
    "       |__some other hw3 file\n",
    "    --HW4\n",
    "       |__MIDS-W261-HW-04-<Student_id>.ipynb\n",
    "       |__MIDS-W261-HW-04-<Student_id>.pdf\n",
    "       |__some other hw4 file\n",
    "    etc..\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.0  data warehouse; star schema\n",
    "\n",
    "- What is a data warehouse? What is a Star schema? When is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will understand this sytem from insurance industry example. \n",
    "\n",
    "\n",
    "#### DW\n",
    "Underwriting system book policies and collects premium from customers. They are transaction systems which gives current status of policies/booking system and also can give total active policies. \n",
    "system is good enough for underwriteres.\n",
    "\n",
    "For acturial calculation (people who decides the price/rate premium ) they would need historical records for individual customers, they will also need data for all claims ,market trend data and competitors data. So decision making people needs data from various sources not just single .\n",
    "\n",
    "DW system solves this problem.  DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data in one single place and are used for creating analytical reports for knowledge workers throughout the enterprise.\n",
    "\n",
    "\n",
    "#### Star Schema\n",
    "\n",
    "Star schema is one of the architecture in DW systems. A single fact record surrounded by multiple dimension records forming a star pattern. The fact record type sits in the middle of the schema at the many end of a number of one-to-many relationships. The dimension records are used to query the fact record.\n",
    "\n",
    "Related dimensions are grouped as columns in dimension tables, and the facts are stored as columns in a fact table. The star schema gets its name from its appearance: when drawn with the fact table in the center, it looks like a star or asterisk\n",
    "\n",
    "SAP/Oracle DW system supports this architecture.  for SAP BI sytems they are called as infocubes.\n",
    "![jobUI](starschema.png)\n",
    "\n",
    "\n",
    "#### when it is used?\n",
    "\n",
    "##### Querying Facts\n",
    "One or more facts are requested, along with the dimensional attributes that provide the desired context. The facts will be summarized in accordance with the dimensions present in the query. Dimension values are also used to limit the scope of the query, serving as the basis for filters or constraints on the data to be fetched and aggregated.\n",
    "\n",
    "##### Browsing Dimensions\n",
    "Browsing is the act of exploring the data within a dimension.Here we interact with dimensional schema.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.1 Databases: 3NF; denormalized \n",
    "\n",
    "- In the database world What is 3NF? Does machine learning use data in 3NF? If so why? \n",
    "- In what form does ML consume data?\n",
    "- Why would one use log files that are denormalized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### In the database world What is 3NF? Does machine learning use data in 3NF? If so why? \n",
    "\n",
    "Third normal form (3NF) is the third step in normalizing a database and it builds on the first and second normal forms, 1NF and 2NF.\n",
    "\n",
    "3NF states that all column reference in referenced data that are not dependent on the primary key should be removed. Another way of putting this is that only foreign key columns should be used to reference another table, and no other columns from the parent table should exist in the referenced table.\n",
    "\n",
    "ML tries to find out model that can be generalized for observed data of various kind.  For example, to predict the premium rate for particular insurance product we need premium/claims/market values housing/health and data from variety of sources.  Algorithm needs to study all features at same time. With 3NF, we will end up with reading thosands of tables to make single record while training time which is not feasible. Hence not ideal though technically we can make it possible by joining tables at runtime.\n",
    "\n",
    "#### In what form does ML consume data?\n",
    "ML consumes data in denormalized form i.e. in flattened file. \n",
    "\n",
    "This also helps in hadoop kind of databases where we fetch records in blocks of size 128MB or someitmes gigs. We get all data for each observation for data fetch.\n",
    "#### Why would one use log files that are denormalized?\n",
    "for ML we need each attributes/demension for particular observation. So we will use denormalized log files for that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.2  Memory-backed map-side\n",
    "\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Use the following tables for this HW and join based on the country code (third column of the transactions table and the second column of the Countries table:\n",
    "\n",
    "<pre>\n",
    "transactions.dat\n",
    "Alice Bob|$10|US\n",
    "Sam Sneed|$1|CA\n",
    "Jon Sneed|$20|CA\n",
    "Arnold Wesise|$400|UK\n",
    "Henry Bob|$2|US\n",
    "Yo Yo Ma|$2|CA\n",
    "Jon York|$44|CA\n",
    "Alex Ball|$5|UK\n",
    "Jim Davis|$66|JA\n",
    "\n",
    "Countries.dat\n",
    "United States|US\n",
    "Canada|CA\n",
    "United Kingdom|UK\n",
    "Italy|IT\n",
    "\n",
    "</pre>\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "- (1) Left joining Table Left with Table Right\n",
    "- (2) Right joining Table Left with Table Right\n",
    "- (3) Inner joining Table Left with Table Right\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left Table\n",
    "I am choosing transactions table on left side as for left join I will get records only in left table.  If I am analyzing the transactions I would prefer to see all transactions though some of them might be missing some data elements.\n",
    "\n",
    "Even SAP recommends to use left join in their training material for HANA Database. People rarely use right outer join in practice.\n",
    "\n",
    "We could have implement this using small table as left as well.  This would not make a difference though as we are doing all joins here.  but if situation would have been only for inner join then smaller table to use as left would make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting transactions.dat\n"
     ]
    }
   ],
   "source": [
    "%%writefile transactions.dat\n",
    "Alice Bob|$10|US\n",
    "Sam Sneed|$1|CA\n",
    "Jon Sneed|$20|CA\n",
    "Arnold Wesise|$400|UK\n",
    "Henry Bob|$2|US\n",
    "Yo Yo Ma|$2|CA\n",
    "Jon York|$44|CA\n",
    "Alex Ball|$5|UK\n",
    "Jim Davis|$66|JA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Countries.dat\n"
     ]
    }
   ],
   "source": [
    "%%writefile Countries.dat\n",
    "United States|US\n",
    "Canada|CA\n",
    "United Kingdom|UK\n",
    "Italy|IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRjoins.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRjoins.py\n",
    "#!/usr/bin/env python\n",
    "#START STUDENT CODE43\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MRjoins(MRJob):\n",
    "\n",
    "#    OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "    SORT_VALUES = True\n",
    "   \n",
    "    #def __init__(self, *args, **kwargs):\n",
    "    #    super(MRjoins, self).__init__(*args, **kwargs)\n",
    "    def configure_options(self):\n",
    "        super(MRjoins, self).configure_options()\n",
    "        self.add_passthrough_option('--join-type', type = 'string', default = 'left')      \n",
    "       \n",
    "    def mapper_init_leftjoin(self):\n",
    "        self.country = {}\n",
    "        with open('Countries.dat', 'r') as country:\n",
    "            for line in country:\n",
    "                tokens = line.split('|')\n",
    "                # the entry is the page id => ( url : count )\n",
    "                self.country[tokens[1].strip('\\n')] = tokens[0]\n",
    "            \n",
    "    \n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        self.increment_counter('Execution Counts', 'mapper calls', 1)\n",
    "\n",
    "        fields = line.split(\"|\")\n",
    "        if fields[2].strip('\\n') in self.country:\n",
    "            sline = line + '|'+ self.country[fields[2].strip('\\n')]\n",
    "            yield None,sline\n",
    "            \n",
    "        else:\n",
    "            yield None,line\n",
    "    def mapper_innerjoin(self, _, line):\n",
    "        self.increment_counter('Execution Counts', 'mapper calls', 1)\n",
    "\n",
    "        fields = line.split(\"|\")\n",
    "        if fields[2].strip('\\n') in self.country:\n",
    "            sline = line + '|'+ self.country[fields[2].strip('\\n')]\n",
    "            yield None,sline\n",
    "            \n",
    "    def mapper_rightjoin(self, _, line):\n",
    "        self.increment_counter('Execution Counts', 'mapper calls', 1)\n",
    "        fields = line.split(\"|\")\n",
    "        keys =fields[2].strip('\\n')\n",
    "        if keys in self.country:\n",
    "            sline = line + '|'+ self.country[keys]\n",
    "            yield None,sline  \n",
    "   \n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.country = {}\n",
    "        with open('Countries.dat', 'r') as f:\n",
    "            for line in f:\n",
    "                tokens = line.split('|')\n",
    "                # the entry is the page id => ( url : count )\n",
    "                self.country[tokens[1].strip('\\n')] = tokens[0]\n",
    "        \n",
    "    def reducer(self,key, records):\n",
    "        self.increment_counter('Execution Counts', 'reducer count', 1) \n",
    "        passedkeys = set()\n",
    "        for rec in records:\n",
    "            fields = rec.split(\"|\")\n",
    "            passedkeys.add(fields[2])\n",
    "            yield None,[fields[0],fields[1],fields[2],fields[3]]\n",
    "        for element in self.country.keys():\n",
    "            if element not in passedkeys:\n",
    "                yield None,[None,None,element,self.country[element]]\n",
    "        \n",
    "    def steps(self):  #pipeline of Map-Reduce jobs\n",
    "        JOBCONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.map.output.field.separator':'\\t',    \n",
    "            'mapreduce.partition.keycomparator.options': '-k1,1nr -k2',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "            \n",
    "        }\n",
    "        if self.options.join_type == 'left':\n",
    "             step = MRStep( mapper_init=self.mapper_init_leftjoin,\n",
    "                    mapper=self.mapper,       # STEP 1: word count step\n",
    "                    )\n",
    "        elif self.options.join_type == 'inner':\n",
    "             step = MRStep( mapper_init=self.mapper_init_leftjoin,\n",
    "                    mapper=self.mapper_innerjoin,       # STEP 1: word count step\n",
    "                    )\n",
    "        else:\n",
    "             step =  MRStep( \n",
    "                    mapper_init=self.mapper_init_leftjoin,\n",
    "                    mapper=self.mapper_rightjoin, \n",
    "                    reducer_init = self.reducer_init,\n",
    "                    reducer=self.reducer\n",
    "                    )\n",
    "        return [step]\n",
    "            \n",
    "            \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRjoins.run()\n",
    "\n",
    "\n",
    "#END STUDENT CODE43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_folder = 'hdfs://' + '/user/root/tmp/mrjob' + '/' + 'hw522'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///user/root/tmp/mrjob/hw522\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r $input_folder\n",
    "!hdfs dfs -mkdir $input_folder\n",
    "\n",
    "!hdfs dfs -copyFromLocal transactions.dat  $input_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   1 root supergroup        151 2017-06-12 21:28 hdfs:///user/root/tmp/mrjob/hw522/transactions.dat\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls $input_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from MRjoins import MRjoins\n",
    "def do_joins(jointype,hadoop):\n",
    "    if hadoop == 'X': #not in hadoop mode\n",
    "        mr_job = MRjoins(args=['transactions.dat',\"--file\" ,\"Countries.dat\",\"--join-type\",jointype])\n",
    "    else:\n",
    "        mr_job = MRjoins(args=['-r','hadoop','transactions.dat',\"--file\" ,\"Countries.dat\",\"--join-type\",jointype])\n",
    "    with mr_job.make_runner() as runner: \n",
    "    # Run MRJob\n",
    "        runner.run()\n",
    "    \n",
    "    # Write stream_output to file\n",
    "        count = 0\n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            print value\n",
    "            count += 1\n",
    "    print \"Total number of Records %d\"%count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"mrjob.compat\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo Yo Ma|$2|CA|Canada\n",
      "Jon York|$44|CA|Canada\n",
      "Alex Ball|$5|UK|United Kingdom\n",
      "Alice Bob|$10|US|United States\n",
      "Sam Sneed|$1|CA|Canada\n",
      "Jon Sneed|$20|CA|Canada\n",
      "Arnold Wesise|$400|UK|United Kingdom\n",
      "Henry Bob|$2|US|United States\n",
      "Total number of Records 8\n"
     ]
    }
   ],
   "source": [
    "#Inner join in hadoop mode\n",
    "do_joins(\"inner\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo Yo Ma|$2|CA|Canada\n",
      "Jon York|$44|CA|Canada\n",
      "Alex Ball|$5|UK|United Kingdom\n",
      "Jim Davis|$66|JA\n",
      "Alice Bob|$10|US|United States\n",
      "Sam Sneed|$1|CA|Canada\n",
      "Jon Sneed|$20|CA|Canada\n",
      "Arnold Wesise|$400|UK|United Kingdom\n",
      "Henry Bob|$2|US|United States\n",
      "Total number of Records 9\n"
     ]
    }
   ],
   "source": [
    "#lefjoin in hadoop mode\n",
    "do_joins(\"left\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Alex Ball', u'$5', u'UK', u'United Kingdom']\n",
      "[u'Alice Bob', u'$10', u'US', u'United States']\n",
      "[u'Arnold Wesise', u'$400', u'UK', u'United Kingdom']\n",
      "[u'Henry Bob', u'$2', u'US', u'United States']\n",
      "[u'Jon Sneed', u'$20', u'CA', u'Canada']\n",
      "[u'Jon York', u'$44', u'CA', u'Canada']\n",
      "[u'Sam Sneed', u'$1', u'CA', u'Canada']\n",
      "[u'Yo Yo Ma', u'$2', u'CA', u'Canada']\n",
      "[None, None, u'IT', u'Italy']\n",
      "Total number of Records 9\n"
     ]
    }
   ],
   "source": [
    "#right in hadoop mode\n",
    "do_joins(\"right\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Alex Ball', u'$5', u'UK', u'United Kingdom']\n",
      "[u'Alice Bob', u'$10', u'US', u'United States']\n",
      "[u'Arnold Wesise', u'$400', u'UK', u'United Kingdom']\n",
      "[u'Henry Bob', u'$2', u'US', u'United States']\n",
      "[u'Jon Sneed', u'$20', u'CA', u'Canada']\n",
      "[u'Jon York', u'$44', u'CA', u'Canada']\n",
      "[u'Sam Sneed', u'$1', u'CA', u'Canada']\n",
      "[u'Yo Yo Ma', u'$2', u'CA', u'Canada']\n",
      "[None, None, u'IT', u'Italy']\n",
      "Total number of Records 9\n"
     ]
    }
   ],
   "source": [
    "from MRjoins import MRjoins\n",
    "#mr_job = MRjoins(args=['-r','hadoop','hdfs:///user/root/tmp/mrjob/hw522/transactions.dat'])\n",
    "mr_job = MRjoins(args=['transactions.dat',\"--file\" ,\"Countries.dat\",\"--join-type\",\"right\"])\n",
    "\n",
    "with mr_job.make_runner() as runner: \n",
    "    # Run MRJob\n",
    "    runner.run()\n",
    "    \n",
    "    # Write stream_output to file\n",
    "    count = 0\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        print value\n",
    "        count += 1\n",
    "    print \"Total number of Records %d\"%count\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.2.1 (OPTIONAL) Almost stateless reducer-side join\n",
    "\n",
    "The following MRJob code, implements a reduce-side join for an inner join. The reducer is almost stateless, i.e., uses as little memory as possible. Use the tables from HW5.2 for this HW and join based on the country code (third column of the transactions table and the second column of the Countries table perform. Perform  an left, right, inner joins using the code provided below and report the number of rows resulting from:\n",
    "\n",
    "- (1) Left joining Table Left with Table Right\n",
    "- (2) Right joining Table Left with Table Right\n",
    "- (3) Inner joining Table Left with Table Right\n",
    "\n",
    "Again make smart decisions about which table should be the left table (i.e., crosscheck the code). \n",
    "\n",
    "__Some notes on the code__ \n",
    "Here, the mapper receives its set of input splits either from the transaction table or from the countries table and makes the appropriate transformations: splitting the line into fields, and emitting a key/value. The key is the join key - in this case, the country code field of both sets of records. The mapper knows which file and type of record it is receiving based on the length of the fields. The records it emits contain the join field as the key, which acts as the partitioning key; We use the SORT_VALUES option, which ensures the values are sorted as well. Then, we employ a trick to ensure that for each join key, country records are seen always before transaction records. We achieve this by adding an arbitrary key to the front of the value: 'A' for countries, 'B' for customers. This makes countries sort before customers for each and every join/partition key. After that trick, the join is simply a matter of storing countries ('A' records) and crossing this array with each customer record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducerjoins.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducerjoins.py\n",
    "import sys, os, re\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "\n",
    "  # Performs secondary sort\n",
    "  #    OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "  SORT_VALUES = True\n",
    "   \n",
    "    #def __init__(self, *args, **kwargs):\n",
    "    #    super(MRjoins, self).__init__(*args, **kwargs)\n",
    "  def configure_options(self):\n",
    "        super(MRJoin, self).configure_options()\n",
    "        self.add_passthrough_option('--join-type', type = 'string', default = 'left')      \n",
    "\n",
    "\n",
    "  def mapper(self, _, line):\n",
    "    splits = line.rstrip(\"\\n\").split(\"|\")\n",
    "\n",
    "    if len(splits) == 2: # country data\n",
    "      symbol = 'A' # make country sort before transaction data\n",
    "      country2digit = splits[1]\n",
    "      yield country2digit, [symbol, splits]\n",
    "    else: # person data\n",
    "      symbol = 'B'\n",
    "      country2digit = splits[2]\n",
    "      yield country2digit, [symbol, splits]\n",
    "\n",
    "  def reducer_inner(self, key, values):\n",
    "    countries = {}\n",
    "    for value in values:\n",
    "      if value[0] == 'A':\n",
    "        countries[value[1][1]] = value[1][0]\n",
    "      if value[0] == 'B':\n",
    "        if  key in countries:\n",
    "           yield key, countries[key] + value[1][0] + value[1][1] + value[1][2]\n",
    "\n",
    "  def reducer_left(self, key, values):\n",
    "    countries = {} # should come first, as they are sorted on artificia key 'A'\n",
    "    for value in values:\n",
    "      if value[0] == 'A':\n",
    "        countries[value[1][1]] = value[1][0]\n",
    "      if value[0] == 'B':\n",
    "        if  key in countries:\n",
    "           yield key, countries[key] + value[1][0] + value[1][1] + value[1][2]\n",
    "        else:\n",
    "            yield key,\"None\"+ value[1][0] + value[1][1] + value[1][2]\n",
    "           \n",
    "  def reducer_init(self):\n",
    "     self.countries = {}  #this is used in final method to do filtering on right join\n",
    "     self.passed_countries = set()\n",
    "  def reducer_right(self, key, values):\n",
    "   \n",
    "    for value in values:\n",
    "      if value[0] == 'A':\n",
    "        self.countries[value[1][1]] = value[1][0]\n",
    "      if value[0] == 'B':\n",
    "        if  key in self.countries:\n",
    "           yield key, self.countries[key] + value[1][0] + value[1][1] + value[1][2]\n",
    "           self.passed_countries.add(key)\n",
    "  def reducer_right_final(self):\n",
    "    for element in self.countries:\n",
    "            if element not in self.passed_countries:\n",
    "                yield element,self.countries[element]+\"None\" + \"None\"\n",
    "     \n",
    "           #yield key, [\"None\"] + value[1][0] + value[1][1] + value[1][2]      \n",
    "\n",
    "  def steps(self):  #pipeline of Map-Reduce jobs\n",
    "     \n",
    "        if self.options.join_type == 'left':\n",
    "             step = MRStep( \n",
    "                    mapper=self.mapper,       # STEP 1: word count step\n",
    "                    reducer=self.reducer_left \n",
    "                    )\n",
    "        elif self.options.join_type == 'inner':\n",
    "             step = MRStep(  mapper=self.mapper,       # STEP 1: word count step\n",
    "                    reducer=self.reducer_inner\n",
    "                    )\n",
    "        else:\n",
    "             step =  MRStep( \n",
    "                     mapper=self.mapper,       # STEP 1: word count step\n",
    "                    reducer=self.reducer_right,\n",
    "                    reducer_final = self.reducer_right_final,\n",
    "                    reducer_init = self.reducer_init,\n",
    "                  jobconf = {\n",
    "                    'mapreduce.job.reduces': 1\n",
    "                }\n",
    "                    )\n",
    "        return [step]\n",
    "if __name__ == '__main__':\n",
    "  MRJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from reducerjoins import MRJoin\n",
    "def right_run(jointype):\n",
    "   \n",
    "\n",
    "    mr_job = MRJoin(args=['transactions.dat',\"Countries.dat\",\"--join-type\",jointype])\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "    # Run MRJob\n",
    "        runner.run()\n",
    "    \n",
    "    # Write stream_output to file\n",
    "        count = 0\n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            print value\n",
    "            count += 1\n",
    "        print \"Total number of Records %d\"%count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CanadaJon Sneed$20CA\n",
      "CanadaJon York$44CA\n",
      "CanadaSam Sneed$1CA\n",
      "CanadaYo Yo Ma$2CA\n",
      "NoneJim Davis$66JA\n",
      "United KingdomAlex Ball$5UK\n",
      "United KingdomArnold Wesise$400UK\n",
      "United StatesAlice Bob$10US\n",
      "United StatesHenry Bob$2US\n",
      "Total number of Records 9\n"
     ]
    }
   ],
   "source": [
    "right_run(\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CanadaJon Sneed$20CA\n",
      "CanadaJon York$44CA\n",
      "CanadaSam Sneed$1CA\n",
      "CanadaYo Yo Ma$2CA\n",
      "United KingdomAlex Ball$5UK\n",
      "United KingdomArnold Wesise$400UK\n",
      "United StatesAlice Bob$10US\n",
      "United StatesHenry Bob$2US\n",
      "ItalyNoneNone\n",
      "Total number of Records 9\n"
     ]
    }
   ],
   "source": [
    "right_run(\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CanadaJon Sneed$20CA\n",
      "CanadaJon York$44CA\n",
      "CanadaSam Sneed$1CA\n",
      "CanadaYo Yo Ma$2CA\n",
      "United KingdomAlex Ball$5UK\n",
      "United KingdomArnold Wesise$400UK\n",
      "United StatesAlice Bob$10US\n",
      "United StatesHenry Bob$2US\n",
      "Total number of Records 8\n"
     ]
    }
   ],
   "source": [
    "right_run(\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Pairwise similarity  - PHASE 1\n",
    "\n",
    "In this part of the assignment we will focus on developing methods for detecting synonyms, using the Google 5-grams dataset. To accomplish this you must script two main tasks using MRJob:\n",
    "\n",
    "\n",
    "### (1) Using the systems tests data sets, write mrjob code to build the stripes\n",
    "### (2) Write mrjob code to build an inverted index from the stripes\n",
    "### (3) Using two (symmetric) comparison methods of your choice (e.g., correlations, distances, similarities), pairwise compare all stripes (vectors), and output to a file.   \n",
    "\n",
    "__==Design notes for (1)== __  \n",
    "For this task you will be able to modify the pattern we used in HW 3.2 (feel free to use the solution as reference). To total the word counts across the n-grams, output the support from the mappers using the total order inversion pattern:\n",
    "\n",
    "<*word,count>   \n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.   \n",
    "\n",
    "In addition to ensuring the determination of the total word counts, the mapper must also output co-occurrence counts for the pairs of words inside of each n-gram. Treat these words as a basket, as we have in HW 3, but count all stripes or pairs in both orders, i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "__==Design notes for (3)==__   \n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    " - Jaccard\n",
    " - Cosine similarity\n",
    " - Spearman correlation\n",
    " - Euclidean distance\n",
    " - Taxicab (Manhattan) distance\n",
    " - Shortest path graph distance (a graph, because our data is symmetric!)\n",
    " - Pearson correlation\n",
    " - Kendall correlation\n",
    " ...\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to parallelize than others, and do not perform more associations than is necessary, since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting buildStripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile buildStripes.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import re\n",
    "import mrjob\n",
    "import json\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from itertools import  combinations\n",
    "class MRbuildStripes(MRJob):\n",
    "  \n",
    "  #START SUDENT CODE531_STRIPES\n",
    "    SORT_VALUES = True\n",
    "   \n",
    "    #def __init__(self, *args, **kwargs):\n",
    "    #    super(MRjoins, self).__init__(*args, **kwargs)\n",
    "        \n",
    "  \n",
    "               \n",
    "    \n",
    "        \n",
    "    def mapper(self, _, recs):\n",
    "        self.increment_counter('Execution Counts', 'mapper calls', 1)\n",
    "        fields = recs.split(\"\\t\")\n",
    "        \n",
    "        products = fields[0].lower().replace('\\n','').split()\n",
    "        for i, term in enumerate(products):\n",
    "                # Create a new stripe for each term\n",
    "                stripe = {}\n",
    "\n",
    "                for j, token in enumerate(products):\n",
    "                    # Don't count the term's co-occurrence with itself\n",
    "                    if i != j:\n",
    "                        x = stripe.get(token,None)\n",
    "                        if x == None:\n",
    "                            stripe[token] = int( fields[1])\n",
    "                        else:\n",
    "                            stripe[token] += int(fields[1])\n",
    "\n",
    "                # Emit the term and the stripe\n",
    "                yield term, stripe\n",
    "    \n",
    "    def combiner(self, word, stripes):\n",
    "        yield word, self.combine_stripes(stripes)\n",
    "\n",
    "    def combine_stripes(self, stripes):\n",
    "        combined_stripe = {}\n",
    "\n",
    "        for stripe in stripes:\n",
    "            for key, value in stripe.iteritems():\n",
    "                if key in combined_stripe:\n",
    "                    combined_stripe[key] += int(value)\n",
    "                else:\n",
    "                    combined_stripe[key] = int(value)\n",
    "\n",
    "        return combined_stripe\n",
    "    def reducer(self,key, records):\n",
    "        yield key, self.combine_stripes(records)\n",
    "        \n",
    "    def steps(self):  #pipeline of Map-Reduce jobs\n",
    "        step = MRStep( \n",
    "                    mapper=self.mapper,       # STEP 1: word count step\n",
    "                    combiner = self.combiner,\n",
    "                    reducer=self.reducer\n",
    "                    )\n",
    "        return [step]\n",
    "            \n",
    "\n",
    "\n",
    "  #END SUDENT CODE531_STRIPES\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "  MRbuildStripes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a {u'limited': 55, u'female': 447, u'general': 92, u'sea': 62, u'in': 1201, u'religious': 59, u'george': 92, u'biography': 92, u'city': 62, u'for': 59, u'tales': 123, u'government': 102, u'the': 124, u'forms': 116, u'wales': 1099, u'christmas': 1099, u\"child's\": 1099, u'collection': 239, u'by': 62, u'case': 604, u'circumstantial': 62, u'of': 1011, u'study': 604, u'bill': 59, u'establishing': 59, u'narrative': 62, u'fairy': 123}\n",
      "bill {u'a': 59, u'religious': 59, u'for': 59, u'establishing': 59}\n",
      "biography {u'a': 92, u'of': 92, u'george': 92, u'general': 92}\n",
      "by {u'a': 62, u'city': 62, u'the': 62, u'sea': 62}\n",
      "case {u'a': 604, u'limited': 55, u'government': 102, u'of': 502, u'study': 604, u'female': 447, u'in': 102}\n",
      "child's {u'a': 1099, u'wales': 1099, u'christmas': 1099, u'in': 1099}\n",
      "christmas {u'a': 1099, u'wales': 1099, u\"child's\": 1099, u'in': 1099}\n",
      "circumstantial {u'a': 62, u'of': 62, u'the': 62, u'narrative': 62}\n",
      "city {u'a': 62, u'the': 62, u'by': 62, u'sea': 62}\n",
      "collection {u'a': 239, u'of': 355, u'fairy': 123, u'tales': 123, u'forms': 116}\n",
      "establishing {u'a': 59, u'bill': 59, u'religious': 59, u'for': 59}\n",
      "fairy {u'a': 123, u'of': 123, u'tales': 123, u'collection': 123}\n",
      "female {u'a': 447, u'case': 447, u'study': 447, u'of': 447}\n",
      "for {u'a': 59, u'bill': 59, u'religious': 59, u'establishing': 59}\n",
      "forms {u'a': 116, u'of': 232, u'collection': 116}\n",
      "general {u'a': 92, u'of': 92, u'george': 92, u'biography': 92}\n",
      "george {u'a': 92, u'of': 92, u'biography': 92, u'general': 92}\n",
      "government {u'a': 102, u'case': 102, u'study': 102, u'in': 102}\n",
      "in {u'a': 1201, u'case': 102, u\"child's\": 1099, u'study': 102, u'government': 102, u'wales': 1099, u'christmas': 1099}\n",
      "limited {u'a': 55, u'case': 55, u'study': 55, u'of': 55}\n",
      "narrative {u'a': 62, u'of': 62, u'the': 62, u'circumstantial': 62}\n",
      "of {u'a': 1011, u'case': 502, u'circumstantial': 62, u'limited': 55, u'of': 232, u'tales': 123, u'collection': 355, u'general': 92, u'forms': 232, u'female': 447, u'narrative': 62, u'study': 502, u'fairy': 123, u'the': 62, u'george': 92, u'biography': 92}\n",
      "religious {u'a': 59, u'bill': 59, u'for': 59, u'establishing': 59}\n",
      "sea {u'a': 62, u'city': 62, u'the': 62, u'by': 62}\n",
      "study {u'a': 604, u'case': 604, u'limited': 55, u'government': 102, u'of': 502, u'female': 447, u'in': 102}\n",
      "tales {u'a': 123, u'of': 123, u'fairy': 123, u'collection': 123}\n",
      "the {u'a': 124, u'city': 62, u'circumstantial': 62, u'of': 62, u'sea': 62, u'narrative': 62, u'by': 62}\n",
      "wales {u'a': 1099, u\"child's\": 1099, u'christmas': 1099, u'in': 1099}\n",
      "Total number of Records 28\n"
     ]
    }
   ],
   "source": [
    "from buildStripes import MRbuildStripes\n",
    "#mr_job = MRjoins(args=['-r','hadoop','hdfs:///user/root/tmp/mrjob/hw522/transactions.dat'])\n",
    "mr_job = MRbuildStripes(args=['googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt'])\n",
    "\n",
    "with mr_job.make_runner() as runner: \n",
    "    # Run MRJob\n",
    "    runner.run()\n",
    "    \n",
    "    # Write stream_output to file\n",
    "    count = 0\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        print key,value\n",
    "        count += 1\n",
    "    print \"Total number of Records %d\"%count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting invertedIndex.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile invertedIndex.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import JSONProtocol\n",
    "class MRinvertedIndex(MRJob):\n",
    "    INPUT_PROTOCOL = JSONProtocol\n",
    "    SORT_VALUES = True\n",
    "#START SUDENT CODE531_INV_INDEX\n",
    "    def mapper_normalize_transpose(self, word, rate_stripe):\n",
    "\n",
    "        # First compute the magnitude for the vector.\n",
    "\n",
    "        #magnitude = math.sqrt(sum([value ** 2 for value in rate_stripe.itervalues()]))\n",
    "\n",
    "        # Divide each value in the vector by the magnitude to normalize.\n",
    "        length = len(rate_stripe)\n",
    "        for key, value in rate_stripe.iteritems():\n",
    "            #normalized_value = value / magnitude\n",
    "            yield key, { word: length}\n",
    "    def combiner_normalize_transpose(self, word, transpose_stripes):\n",
    "        yield word, self.combine_stripes(transpose_stripes)\n",
    "    def reducer_normalize_transpose(self, word, transpose_stripes):\n",
    "        yield word, self.combine_stripes(transpose_stripes)\n",
    "    def combine_stripes(self, stripes):\n",
    "        combined_stripe = {}\n",
    "\n",
    "        for stripe in stripes:\n",
    "            for key, value in stripe.iteritems():\n",
    "                if key in combined_stripe:\n",
    "                    combined_stripe[key] += value\n",
    "                else:\n",
    "                    combined_stripe[key] = value\n",
    "\n",
    "        return combined_stripe\n",
    "\n",
    "        \n",
    "    def steps(self):\n",
    "\n",
    "        transpose_step = MRStep(\n",
    "            mapper = self.mapper_normalize_transpose,\n",
    "            combiner = self.combiner_normalize_transpose,\n",
    "            reducer = self.reducer_normalize_transpose)\n",
    "        return [transpose_step]\n",
    "\n",
    "#END SUDENT CODE531_INV_INDEX\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRinvertedIndex.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M {u'DocC': 4}\n",
      "N {u'DocC': 4}\n",
      "X {u'DocB': 2, u'DocA': 3}\n",
      "Y {u'DocB': 2, u'DocC': 4, u'DocA': 3}\n",
      "Z {u'DocC': 4, u'DocA': 3}\n",
      "Total number of Records 5\n"
     ]
    }
   ],
   "source": [
    "from invertedIndex import MRinvertedIndex\n",
    "#mr_job = MRjoins(args=['-r','hadoop','hdfs:///user/root/tmp/mrjob/hw522/transactions.dat'])\n",
    "mr_job = MRinvertedIndex(args=['systems_test_stripes_3'])\n",
    "\n",
    "with mr_job.make_runner() as runner: \n",
    "    # Run MRJob\n",
    "    runner.run()\n",
    "    \n",
    "    # Write stream_output to file\n",
    "    count = 0\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        print key,value\n",
    "        count += 1\n",
    "    print \"Total number of Records %d\"%count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting similarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile similarity.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import JSONProtocol\n",
    "\n",
    "class MRsimilarity(MRJob):\n",
    "    INPUT_PROTOCOL = JSONProtocol\n",
    "    SORT_VALUES = True\n",
    "\n",
    "#START SUDENT CODE531_SIMILARITY\n",
    "    def mapper_jaccard(self, word, rate_stripe):\n",
    "        #get all words and lengths\n",
    "#We will emit stripes for each word vector here.  These stripes will\n",
    "#be used in combiner to find the common length i.e. words occuring together\n",
    "        nonzero_keys = [key for key, value in rate_stripe.iteritems() if value != 0]\n",
    "\n",
    "    \n",
    "\n",
    "        sorted_keys = sorted(nonzero_keys)\n",
    "        # N * N  complexity matrix calculation\n",
    "        #We are going over each record to find out the common occureances\n",
    "        for i in range(0, len(sorted_keys)):\n",
    "            left_label = sorted_keys[i]\n",
    "\n",
    "            stripe = {}\n",
    "\n",
    "            for j in range(i + 1, len(sorted_keys)):\n",
    "                right_label = sorted_keys[j]\n",
    "                stripe[right_label] = 1\n",
    "\n",
    "            yield left_label, stripe\n",
    "\n",
    "     \n",
    "\n",
    "        for key in sorted_keys:\n",
    "            yield '*',{key:1}\n",
    "            #{u'DocC': 1}\n",
    "        #yield '*', { key: 1 for key in sorted_keys }\n",
    "    def combiner_jaccard(self, left_label, partial_stripes):\n",
    "        yield left_label, self.combine_stripes(partial_stripes)\n",
    "\n",
    "#find out the jaccard values.\n",
    "    def reducer_jaccard(self, left_label, partial_stripes):\n",
    "        total_stripe = self.combine_stripes(partial_stripes)\n",
    "        #this stores the total length of each word Vector\n",
    "        if left_label == '*':\n",
    "            self.total_counts = total_stripe\n",
    "            return\n",
    "\n",
    "        for right_label, intersection_size in total_stripe.iteritems():\n",
    "            coordinate = (left_label, right_label)\n",
    "            union_size = self.total_counts[left_label] + self.total_counts[right_label]\n",
    "\n",
    "            \n",
    "            jaccard_distance = float(intersection_size)/float(union_size - intersection_size) #jaccard\n",
    "            dice_coef = (float(intersection_size) * 2 )/float(union_size ) #dice Coefficient\n",
    "            final = {}\n",
    "            final['jaccard'] = jaccard_distance\n",
    "            final['dice'] = dice_coef\n",
    "            yield coordinate, final\n",
    "\n",
    "#in-memory combiner\n",
    "    def combine_stripes(self, stripes):\n",
    "        combined_stripe = {}\n",
    "\n",
    "        for stripe in stripes:\n",
    "            for key, value in stripe.iteritems():\n",
    "                if key in combined_stripe:\n",
    "                    combined_stripe[key] += value\n",
    "                else:\n",
    "                    combined_stripe[key] = value\n",
    "\n",
    "        return combined_stripe\n",
    "\n",
    "   \n",
    "    def steps(self):\n",
    "         distance_step = MRStep(\n",
    "                mapper = self.mapper_jaccard,\n",
    "                combiner = self.combiner_jaccard,\n",
    "                reducer = self.reducer_jaccard,\n",
    "                jobconf = {\n",
    "                    'mapreduce.job.reduces': 1\n",
    "                    \n",
    "                })\n",
    "         return [distance_step]\n",
    "#END SUDENT CODE531_SIMILARITY\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    MRsimilarity.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M {u'DocC': 4}\n",
      "N {u'DocC': 4}\n",
      "X {u'DocB': 2, u'DocA': 3}\n",
      "Y {u'DocB': 2, u'DocC': 4, u'DocA': 3}\n",
      "Z {u'DocC': 4, u'DocA': 3}\n",
      "Total number of Records 5\n"
     ]
    }
   ],
   "source": [
    "from invertedIndex import MRinvertedIndex\n",
    "#mr_job = MRjoins(args=['-r','hadoop','hdfs:///user/root/tmp/mrjob/hw522/transactions.dat'])\n",
    "mr_job = MRinvertedIndex(args=['systems_test_stripes_3'])\n",
    "\n",
    "with mr_job.make_runner() as runner: \n",
    "    # Run MRJob\n",
    "    runner.run()\n",
    "    \n",
    "    # Write stream_output to file\n",
    "    count = 0\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        print key,value\n",
    "        count += 1\n",
    "    print \"Total number of Records %d\"%count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'DocA', u'DocB'] {u'dice': 0.8, u'jaccard': 0.6666666667000001}\n",
      "[u'DocA', u'DocC'] {u'dice': 0.5714285714, u'jaccard': 0.4}\n",
      "[u'DocB', u'DocC'] {u'dice': 0.33333333330000003, u'jaccard': 0.2}\n",
      "Total number of Records 3\n"
     ]
    }
   ],
   "source": [
    "from similarity import MRsimilarity\n",
    "#mr_job = MRjoins(args=['-r','hadoop','hdfs:///user/root/tmp/mrjob/hw522/transactions.dat'])\n",
    "mr_job = MRsimilarity(args=['systems_test_index_3'])\n",
    "\n",
    "with mr_job.make_runner() as runner: \n",
    "    # Run MRJob\n",
    "    runner.run()\n",
    "    \n",
    "    # Write stream_output to file\n",
    "    count = 0\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        print key,value\n",
    "        count += 1\n",
    "    print \"Total number of Records %d\"%count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.3.1   Run Systems tests locally on small datasets (PHASE1)\n",
    "\n",
    "Complete 5.3 and systems test using the below test datasets. Phase 2 will focus on the entire Ngram dataset.\n",
    "\n",
    "To help you through these tasks please verify that your code gives the results below (for stripes, inverted index, and pairwise similarities).\n",
    "\n",
    "Test datasets:\n",
    "\n",
    "* googlebooks-eng-all-5gram-20090715-0-filtered.txt [see below]\n",
    "* atlas-boon-test [see below]\n",
    "* stripe-docs-test [see below]\n",
    "\n",
    "\n",
    "A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox and on s3:\n",
    "\n",
    "https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "s3://filtered-5grams/\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "The next cell shows the first 10 lines of the googlebooks-eng-all-5gram-20090715-0-filtered.txt file.\n",
    "\n",
    "\n",
    "__DISCLAIMER__: Each record is already a 5-gram. In real life, we would calculate the stripes cooccurrence data from the raw text by windowing over the raw text and not from the 5-gram preprocessed data (as we are doing here).  Calculatating pairs on this 5-gram is a little corrupt as we will be double counting cooccurences. Having said that this exercise can still pull out some simialr terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: unit/systems first-10-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n",
    "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\n",
    "A Biography of General George\t92\t90\t74\n",
    "A Case Study in Government\t102\t102\t78\n",
    "A Case Study of Female\t447\t447\t327\n",
    "A Case Study of Limited\t55\t55\t43\n",
    "A Child's Christmas in Wales\t1099\t1061\t866\n",
    "A Circumstantial Narrative of the\t62\t62\t50\n",
    "A City by the Sea\t62\t60\t49\n",
    "A Collection of Fairy Tales\t123\t117\t80\n",
    "A Collection of Forms of\t116\t103\t82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: unit/systems atlas-boon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting atlas-boon-systems-test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile atlas-boon-systems-test.txt\n",
    "atlas boon\t50\t50\t50\n",
    "boon cava dipped\t10\t10\t10\n",
    "atlas dipped\t15\t15\t15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3: unit/systems stripe-docs-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three terms, A,B,C and their corresponding stripe-docs of co-occurring terms\n",
    "\n",
    "- DocA {X:20, Y:30, Z:5}\n",
    "- DocB {X:100, Y:20}\n",
    "- DocC {M:5, N:20, Z:5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) build stripes for all the test data sets - run the commands and insure that your output matches the output below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /tmp/buildStripes.root.20170612.213235.707741\n",
      "Running step 1 of 1...\n",
      "Counters: 1\n",
      "\tExecution Counts\n",
      "\t\tmapper calls=10\n",
      "Counters: 1\n",
      "\tExecution Counts\n",
      "\t\tmapper calls=10\n",
      "Streaming final output from /tmp/buildStripes.root.20170612.213235.707741/output...\n",
      "Removing temp directory /tmp/buildStripes.root.20170612.213235.707741...\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Make Stripes from ngrams for systems test 1\n",
    "###########################################################################\n",
    "\n",
    "#!hdfs dfs rm --recursive systems_test_stripes_1\n",
    "!python buildStripes.py -r local googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt > systems_test_stripes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a\"\t{\"limited\":55,\"sea\":62,\"general\":92,\"female\":447,\"in\":1201,\"religious\":59,\"george\":92,\"biography\":92,\"city\":62,\"for\":59,\"tales\":123,\"child's\":1099,\"forms\":116,\"wales\":1099,\"christmas\":1099,\"government\":102,\"collection\":239,\"by\":62,\"case\":604,\"circumstantial\":62,\"fairy\":123,\"of\":1011,\"study\":604,\"bill\":59,\"establishing\":59,\"narrative\":62,\"the\":124}\r\n",
      "\"bill\"\t{\"a\":59,\"religious\":59,\"for\":59,\"establishing\":59}\r\n",
      "\"biography\"\t{\"a\":92,\"of\":92,\"george\":92,\"general\":92}\r\n",
      "\"by\"\t{\"a\":62,\"city\":62,\"the\":62,\"sea\":62}\r\n",
      "\"case\"\t{\"a\":604,\"limited\":55,\"government\":102,\"of\":502,\"study\":604,\"female\":447,\"in\":102}\r\n",
      "\"child's\"\t{\"a\":1099,\"wales\":1099,\"christmas\":1099,\"in\":1099}\r\n",
      "\"christmas\"\t{\"a\":1099,\"wales\":1099,\"in\":1099,\"child's\":1099}\r\n",
      "\"circumstantial\"\t{\"a\":62,\"of\":62,\"the\":62,\"narrative\":62}\r\n",
      "\"city\"\t{\"a\":62,\"the\":62,\"by\":62,\"sea\":62}\r\n",
      "\"collection\"\t{\"a\":239,\"of\":355,\"fairy\":123,\"tales\":123,\"forms\":116}\r\n",
      "\"establishing\"\t{\"a\":59,\"bill\":59,\"religious\":59,\"for\":59}\r\n",
      "\"fairy\"\t{\"a\":123,\"of\":123,\"tales\":123,\"collection\":123}\r\n",
      "\"female\"\t{\"a\":447,\"case\":447,\"study\":447,\"of\":447}\r\n",
      "\"for\"\t{\"a\":59,\"bill\":59,\"religious\":59,\"establishing\":59}\r\n",
      "\"forms\"\t{\"a\":116,\"of\":232,\"collection\":116}\r\n",
      "\"general\"\t{\"a\":92,\"of\":92,\"george\":92,\"biography\":92}\r\n",
      "\"george\"\t{\"a\":92,\"of\":92,\"biography\":92,\"general\":92}\r\n",
      "\"government\"\t{\"a\":102,\"case\":102,\"study\":102,\"in\":102}\r\n",
      "\"in\"\t{\"a\":1201,\"case\":102,\"government\":102,\"study\":102,\"child's\":1099,\"wales\":1099,\"christmas\":1099}\r\n",
      "\"limited\"\t{\"a\":55,\"case\":55,\"study\":55,\"of\":55}\r\n",
      "\"narrative\"\t{\"a\":62,\"of\":62,\"the\":62,\"circumstantial\":62}\r\n",
      "\"of\"\t{\"a\":1011,\"case\":502,\"circumstantial\":62,\"george\":92,\"limited\":55,\"of\":232,\"tales\":123,\"collection\":355,\"general\":92,\"forms\":232,\"female\":447,\"narrative\":62,\"study\":502,\"fairy\":123,\"the\":62,\"biography\":92}\r\n",
      "\"religious\"\t{\"a\":59,\"bill\":59,\"for\":59,\"establishing\":59}\r\n",
      "\"sea\"\t{\"a\":62,\"city\":62,\"the\":62,\"by\":62}\r\n",
      "\"study\"\t{\"a\":604,\"case\":604,\"limited\":55,\"government\":102,\"of\":502,\"female\":447,\"in\":102}\r\n",
      "\"tales\"\t{\"a\":123,\"of\":123,\"fairy\":123,\"collection\":123}\r\n",
      "\"the\"\t{\"a\":124,\"city\":62,\"circumstantial\":62,\"of\":62,\"sea\":62,\"narrative\":62,\"by\":62}\r\n",
      "\"wales\"\t{\"a\":1099,\"in\":1099,\"christmas\":1099,\"child's\":1099}\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_stripes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a\"\t{\"limited\":55,\"sea\":62,\"general\":92,\"female\":447,\"in\":1201,\"religious\":59,\"george\":92,\"biography\":92,\"city\":62,\"for\":59,\"tales\":123,\"child's\":1099,\"forms\":116,\"wales\":1099,\"christmas\":1099,\"government\":102,\"collection\":239,\"by\":62,\"case\":604,\"circumstantial\":62,\"fairy\":123,\"of\":1011,\"study\":604,\"bill\":59,\"establishing\":59,\"narrative\":62,\"the\":124}\r\n",
      "\"bill\"\t{\"a\":59,\"religious\":59,\"for\":59,\"establishing\":59}\r\n",
      "\"biography\"\t{\"a\":92,\"of\":92,\"george\":92,\"general\":92}\r\n",
      "\"by\"\t{\"a\":62,\"city\":62,\"the\":62,\"sea\":62}\r\n",
      "\"case\"\t{\"a\":604,\"limited\":55,\"government\":102,\"of\":502,\"study\":604,\"female\":447,\"in\":102}\r\n",
      "\"child's\"\t{\"a\":1099,\"wales\":1099,\"christmas\":1099,\"in\":1099}\r\n",
      "\"christmas\"\t{\"a\":1099,\"wales\":1099,\"in\":1099,\"child's\":1099}\r\n",
      "\"circumstantial\"\t{\"a\":62,\"of\":62,\"the\":62,\"narrative\":62}\r\n",
      "\"city\"\t{\"a\":62,\"the\":62,\"by\":62,\"sea\":62}\r\n",
      "\"collection\"\t{\"a\":239,\"of\":355,\"fairy\":123,\"tales\":123,\"forms\":116}\r\n",
      "\"establishing\"\t{\"a\":59,\"bill\":59,\"religious\":59,\"for\":59}\r\n",
      "\"fairy\"\t{\"a\":123,\"of\":123,\"tales\":123,\"collection\":123}\r\n",
      "\"female\"\t{\"a\":447,\"case\":447,\"study\":447,\"of\":447}\r\n",
      "\"for\"\t{\"a\":59,\"bill\":59,\"religious\":59,\"establishing\":59}\r\n",
      "\"forms\"\t{\"a\":116,\"of\":232,\"collection\":116}\r\n",
      "\"general\"\t{\"a\":92,\"of\":92,\"george\":92,\"biography\":92}\r\n",
      "\"george\"\t{\"a\":92,\"of\":92,\"biography\":92,\"general\":92}\r\n",
      "\"government\"\t{\"a\":102,\"case\":102,\"study\":102,\"in\":102}\r\n",
      "\"in\"\t{\"a\":1201,\"case\":102,\"government\":102,\"study\":102,\"child's\":1099,\"wales\":1099,\"christmas\":1099}\r\n",
      "\"limited\"\t{\"a\":55,\"case\":55,\"study\":55,\"of\":55}\r\n",
      "\"narrative\"\t{\"a\":62,\"of\":62,\"the\":62,\"circumstantial\":62}\r\n",
      "\"of\"\t{\"a\":1011,\"case\":502,\"circumstantial\":62,\"george\":92,\"limited\":55,\"of\":232,\"tales\":123,\"collection\":355,\"general\":92,\"forms\":232,\"female\":447,\"narrative\":62,\"study\":502,\"fairy\":123,\"the\":62,\"biography\":92}\r\n",
      "\"religious\"\t{\"a\":59,\"bill\":59,\"for\":59,\"establishing\":59}\r\n",
      "\"sea\"\t{\"a\":62,\"city\":62,\"the\":62,\"by\":62}\r\n",
      "\"study\"\t{\"a\":604,\"case\":604,\"limited\":55,\"government\":102,\"of\":502,\"female\":447,\"in\":102}\r\n",
      "\"tales\"\t{\"a\":123,\"of\":123,\"fairy\":123,\"collection\":123}\r\n",
      "\"the\"\t{\"a\":124,\"city\":62,\"circumstantial\":62,\"of\":62,\"sea\":62,\"narrative\":62,\"by\":62}\r\n",
      "\"wales\"\t{\"a\":1099,\"in\":1099,\"christmas\":1099,\"child's\":1099}\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_stripes_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\"a\"\t{\"limited\": 55, \"sea\": 62, \"general\": 92, \"female\": 447, \"in\": 1201, \"religious\": 59, \"george\": 92, \"biography\": 92, \"city\": 62, \"for\": 59, \"tales\": 123, \"child's\": 1099, \"forms\": 116, \"wales\": 1099, \"christmas\": 1099, \"government\": 102, \"collection\": 239, \"by\": 62, \"case\": 604, \"circumstantial\": 62, \"fairy\": 123, \"of\": 1011, \"study\": 604, \"bill\": 59, \"establishing\": 59, \"narrative\": 62, \"the\": 124}\n",
    "\"bill\"\t{\"a\": 59, \"religious\": 59, \"for\": 59, \"establishing\": 59}\n",
    "\"biography\"\t{\"a\": 92, \"of\": 92, \"george\": 92, \"general\": 92}\n",
    "\"by\"\t{\"a\": 62, \"city\": 62, \"the\": 62, \"sea\": 62}\n",
    "\"case\"\t{\"a\": 604, \"limited\": 55, \"government\": 102, \"of\": 502, \"study\": 604, \"female\": 447, \"in\": 102}\n",
    "\"child's\"\t{\"a\": 1099, \"wales\": 1099, \"christmas\": 1099, \"in\": 1099}\n",
    "\"christmas\"\t{\"a\": 1099, \"wales\": 1099, \"in\": 1099, \"child's\": 1099}\n",
    "\"circumstantial\"\t{\"a\": 62, \"of\": 62, \"the\": 62, \"narrative\": 62}\n",
    "\"city\"\t{\"a\": 62, \"the\": 62, \"by\": 62, \"sea\": 62}\n",
    "\"collection\"\t{\"a\": 239, \"of\": 355, \"fairy\": 123, \"tales\": 123, \"forms\": 116}\n",
    "\"establishing\"\t{\"a\": 59, \"bill\": 59, \"religious\": 59, \"for\": 59}\n",
    "\"fairy\"\t{\"a\": 123, \"of\": 123, \"tales\": 123, \"collection\": 123}\n",
    "\"female\"\t{\"a\": 447, \"case\": 447, \"study\": 447, \"of\": 447}\n",
    "\"for\"\t{\"a\": 59, \"bill\": 59, \"religious\": 59, \"establishing\": 59}\n",
    "\"forms\"\t{\"a\": 116, \"of\": 232, \"collection\": 116}\n",
    "\"general\"\t{\"a\": 92, \"of\": 92, \"george\": 92, \"biography\": 92}\n",
    "\"george\"\t{\"a\": 92, \"of\": 92, \"biography\": 92, \"general\": 92}\n",
    "\"government\"\t{\"a\": 102, \"case\": 102, \"study\": 102, \"in\": 102}\n",
    "\"in\"\t{\"a\": 1201, \"case\": 102, \"government\": 102, \"study\": 102, \"child's\": 1099, \"wales\": 1099, \"christmas\": 1099}\n",
    "\"limited\"\t{\"a\": 55, \"case\": 55, \"study\": 55, \"of\": 55}\n",
    "\"narrative\"\t{\"a\": 62, \"of\": 62, \"the\": 62, \"circumstantial\": 62}\n",
    "\"of\"\t{\"a\": 1127, \"case\": 502, \"circumstantial\": 62, \"george\": 92, \"limited\": 55, \"tales\": 123, \"collection\": 471, \"general\": 92, \"forms\": 348, \"female\": 447, \"narrative\": 62, \"study\": 502, \"fairy\": 123, \"the\": 62, \"biography\": 92}\n",
    "\"religious\"\t{\"a\": 59, \"bill\": 59, \"for\": 59, \"establishing\": 59}\n",
    "\"sea\"\t{\"a\": 62, \"city\": 62, \"the\": 62, \"by\": 62}\n",
    "\"study\"\t{\"a\": 604, \"case\": 604, \"limited\": 55, \"government\": 102, \"of\": 502, \"female\": 447, \"in\": 102}\n",
    "\"tales\"\t{\"a\": 123, \"of\": 123, \"fairy\": 123, \"collection\": 123}\n",
    "\"the\"\t{\"a\": 124, \"city\": 62, \"circumstantial\": 62, \"of\": 62, \"sea\": 62, \"narrative\": 62, \"by\": 62}\n",
    "\"wales\"\t{\"a\": 1099, \"in\": 1099, \"christmas\": 1099, \"child's\": 1099}\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: Unknown command\n",
      "Did you mean -rm?  This command begins with a dash.\n",
      "No configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /tmp/buildStripes.root.20170612.213240.421389\n",
      "Running step 1 of 1...\n",
      "Counters: 1\n",
      "\tExecution Counts\n",
      "\t\tmapper calls=3\n",
      "Counters: 1\n",
      "\tExecution Counts\n",
      "\t\tmapper calls=3\n",
      "Streaming final output from /tmp/buildStripes.root.20170612.213240.421389/output...\n",
      "Removing temp directory /tmp/buildStripes.root.20170612.213240.421389...\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Make Stripes from ngrams for systems test 2\n",
    "###########################################################################\n",
    "\n",
    "!hdfs dfs rm --recursive systems_test_stripes_2\n",
    "!python buildStripes.py -r local atlas-boon-systems-test.txt > systems_test_stripes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"atlas\"\t{\"dipped\":15,\"boon\":50}\r\n",
      "\"boon\"\t{\"atlas\":50,\"dipped\":10,\"cava\":10}\r\n",
      "\"cava\"\t{\"dipped\":10,\"boon\":10}\r\n",
      "\"dipped\"\t{\"atlas\":15,\"boon\":10,\"cava\":10}\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_stripes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"atlas\"\t{\"dipped\":15,\"boon\":50}\r\n",
      "\"boon\"\t{\"atlas\":50,\"dipped\":10,\"cava\":10}\r\n",
      "\"cava\"\t{\"dipped\":10,\"boon\":10}\r\n",
      "\"dipped\"\t{\"atlas\":15,\"boon\":10,\"cava\":10}\r\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_stripes_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<pre>\n",
    "\"atlas\"   {\"dipped\": 15, \"boon\": 50}   \n",
    "\"boon\"    {\"atlas\": 50, \"dipped\": 10, \"cava\": 10}   \n",
    "\"cava\"    {\"dipped\": 10, \"boon\": 10} \n",
    "\"dipped\"  {\"atlas\": 15, \"boon\": 10, \"cava\": 10}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DocA\"\t{\"X\":20, \"Y\":30, \"Z\":5}\r\n",
      "\"DocB\"\t{\"X\":100, \"Y\":20}\r\n",
      "\"DocC\"\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\r\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# Stripes for systems test 3 (given, no need to build stripes)\n",
    "########################################################################\n",
    "\n",
    "with open(\"systems_test_stripes_3\", \"w\") as f:\n",
    "    f.writelines([\n",
    "        '\"DocA\"\\t{\"X\":20, \"Y\":30, \"Z\":5}\\n',\n",
    "        '\"DocB\"\\t{\"X\":100, \"Y\":20}\\n',  \n",
    "        '\"DocC\"\\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\\n'\n",
    "    ])\n",
    "!cat systems_test_stripes_3   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Build Inverted Index - run the commands and insure that your output matches the output below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /tmp/invertedIndex.root.20170612.213247.264113\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/invertedIndex.root.20170612.213247.264113/output...\n",
      "Removing temp directory /tmp/invertedIndex.root.20170612.213247.264113...\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndex.py -r local systems_test_stripes_1 > systems_test_index_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /tmp/invertedIndex.root.20170612.213250.924138\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/invertedIndex.root.20170612.213250.924138/output...\n",
      "Removing temp directory /tmp/invertedIndex.root.20170612.213250.924138...\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndex.py -r local systems_test_stripes_2 > systems_test_index_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /tmp/invertedIndex.root.20170612.213254.206656\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/invertedIndex.root.20170612.213254.206656/output...\n",
      "Removing temp directory /tmp/invertedIndex.root.20170612.213254.206656...\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndex.py -r local systems_test_stripes_3 > systems_test_index_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Systems test  1  - Inverted Index\n",
      "\n",
      "             \"a\" |          bill 4 |     biography 4 |            by 4\n",
      "          \"bill\" |            a 27 |  establishing 4 |           for 4\n",
      "     \"biography\" |            a 27 |       general 4 |        george 4\n",
      "            \"by\" |            a 27 |          city 4 |           sea 4\n",
      "          \"case\" |            a 27 |        female 4 |    government 4\n",
      "       \"child's\" |            a 27 |     christmas 4 |            in 7\n",
      "     \"christmas\" |            a 27 |       child's 4 |            in 7\n",
      "\"circumstantial\" |            a 27 |     narrative 4 |           of 16\n",
      "          \"city\" |            a 27 |            by 4 |           sea 4\n",
      "    \"collection\" |            a 27 |         fairy 4 |         forms 3\n",
      "  \"establishing\" |            a 27 |          bill 4 |           for 4\n",
      "         \"fairy\" |            a 27 |    collection 5 |           of 16\n",
      "        \"female\" |            a 27 |          case 7 |           of 16\n",
      "           \"for\" |            a 27 |          bill 4 |  establishing 4\n",
      "         \"forms\" |            a 27 |    collection 5 |           of 16\n",
      "       \"general\" |            a 27 |     biography 4 |        george 4\n",
      "        \"george\" |            a 27 |     biography 4 |       general 4\n",
      "    \"government\" |            a 27 |          case 7 |            in 7\n",
      "            \"in\" |            a 27 |          case 7 |       child's 4\n",
      "       \"limited\" |            a 27 |          case 7 |           of 16\n",
      "     \"narrative\" |            a 27 |circumstantial 4 |           of 16\n",
      "            \"of\" |            a 27 |     biography 4 |          case 7\n",
      "     \"religious\" |            a 27 |          bill 4 |  establishing 4\n",
      "           \"sea\" |            a 27 |            by 4 |          city 4\n",
      "         \"study\" |            a 27 |          case 7 |        female 4\n",
      "         \"tales\" |            a 27 |    collection 5 |         fairy 4\n",
      "           \"the\" |            a 27 |            by 4 |circumstantial 4\n",
      "         \"wales\" |            a 27 |       child's 4 |     christmas 4\n",
      "\n",
      "Systems test  2  - Inverted Index\n",
      "\n",
      "         \"atlas\" |          boon 3 |        dipped 3 |                \n",
      "          \"boon\" |         atlas 2 |          cava 2 |        dipped 3\n",
      "          \"cava\" |          boon 3 |        dipped 3 |                \n",
      "        \"dipped\" |         atlas 2 |          boon 3 |          cava 2\n",
      "\n",
      "Systems test  3  - Inverted Index\n",
      "\n",
      "             \"M\" |          DocC 4 |                 |                \n",
      "             \"N\" |          DocC 4 |                 |                \n",
      "             \"X\" |          DocA 3 |          DocB 2 |                \n",
      "             \"Y\" |          DocA 3 |          DocB 2 |          DocC 4\n",
      "             \"Z\" |          DocA 3 |          DocC 4 |                \n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Pretty print systems tests for generating Inverted Index\n",
    "##########################################################\n",
    "\n",
    "import json\n",
    "\n",
    "for i in range(1,4):\n",
    "    print \"\"*100\n",
    "    print \"Systems test \",i,\" - Inverted Index\"\n",
    "    print \"\"*100  \n",
    "    with open(\"systems_test_index_\"+str(i),\"r\") as f:\n",
    "        lines = sorted(f.readlines())\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            word, doc_list = line.split(\"\\t\")\n",
    "            doc_dict = json.loads(doc_list)\n",
    "            stripe=[]\n",
    "            for doc in doc_dict:\n",
    "                stripe.append([doc, doc_dict[doc]])\n",
    "            stripe=sorted(stripe)\n",
    "            stripe.extend([[\"\",\"\"] for _ in xrange(3 - len(stripe))])\n",
    "\n",
    "            print \"{0:>16} |{1:>16} |{2:>16} |{3:>16}\".format(\n",
    "              (word), stripe[0][0]+\" \"+str(stripe[0][1]), stripe[1][0]+\" \"+str(stripe[1][1]), stripe[2][0]+\" \"+str(stripe[2][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##########################################################\n",
    "# Pretty print systems tests for generating Inverted Index\n",
    "##########################################################\n",
    "\n",
    "import json\n",
    "\n",
    "for i in range(1,4):\n",
    "  print \"\"*100\n",
    "  print \"Systems test \",i,\" - Inverted Index\"\n",
    "  print \"\"*100  \n",
    "  with open(\"systems_test_index_\"+str(i),\"r\") as f:\n",
    "      lines = f.readlines()\n",
    "      for line in lines:\n",
    "          line = line.str\n",
    "          word,stripe = line.split(\"\\t\")\n",
    "          stripe = json.loads(stripe)\n",
    "          stripe.extend([[\"\",\"\"] for _ in xrange(3 - len(stripe))])\n",
    "\n",
    "          print \"{0:>16} |{1:>16} |{2:>16} |{3:>16}\".format(\n",
    "              (word), stripe[0][0]+\" \"+str(stripe[0][1]), stripe[1][0]+\" \"+str(stripe[1][1]), stripe[2][0]+\" \"+str(stripe[2][1]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Systems test  1  - Inverted Index\n",
    "\n",
    "             \"a\" |          bill 4 |     biography 4 |            by 4\n",
    "          \"bill\" |            a 27 |  establishing 4 |           for 4\n",
    "     \"biography\" |            a 27 |       general 4 |        george 4\n",
    "            \"by\" |            a 27 |          city 4 |           sea 4\n",
    "          \"case\" |            a 27 |        female 4 |    government 4\n",
    "       \"child's\" |            a 27 |     christmas 4 |            in 7\n",
    "     \"christmas\" |            a 27 |       child's 4 |            in 7\n",
    "\"circumstantial\" |            a 27 |     narrative 4 |           of 15\n",
    "          \"city\" |            a 27 |            by 4 |           sea 4\n",
    "    \"collection\" |            a 27 |         fairy 4 |         forms 3\n",
    "  \"establishing\" |            a 27 |          bill 4 |           for 4\n",
    "         \"fairy\" |            a 27 |    collection 5 |           of 15\n",
    "        \"female\" |            a 27 |          case 7 |           of 15\n",
    "           \"for\" |            a 27 |          bill 4 |  establishing 4\n",
    "         \"forms\" |            a 27 |    collection 5 |           of 15\n",
    "       \"general\" |            a 27 |     biography 4 |        george 4\n",
    "        \"george\" |            a 27 |     biography 4 |       general 4\n",
    "    \"government\" |            a 27 |          case 7 |            in 7\n",
    "            \"in\" |            a 27 |          case 7 |       child's 4\n",
    "       \"limited\" |            a 27 |          case 7 |           of 15\n",
    "     \"narrative\" |            a 27 |circumstantial 4 |           of 15\n",
    "            \"of\" |            a 27 |     biography 4 |          case 7\n",
    "     \"religious\" |            a 27 |          bill 4 |  establishing 4\n",
    "           \"sea\" |            a 27 |            by 4 |          city 4\n",
    "         \"study\" |            a 27 |          case 7 |        female 4\n",
    "         \"tales\" |            a 27 |    collection 5 |         fairy 4\n",
    "           \"the\" |            a 27 |            by 4 |circumstantial 4\n",
    "         \"wales\" |            a 27 |       child's 4 |     christmas 4\n",
    "\n",
    "Systems test  2  - Inverted Index\n",
    "\n",
    "         \"atlas\" |          boon 3 |        dipped 3 |                \n",
    "          \"boon\" |         atlas 2 |          cava 2 |        dipped 3\n",
    "          \"cava\" |          boon 3 |        dipped 3 |                \n",
    "        \"dipped\" |         atlas 2 |          boon 3 |          cava 2\n",
    "\n",
    "Systems test  3  - Inverted Index\n",
    "\n",
    "             \"M\" |          DocC 4 |                 |                \n",
    "             \"N\" |          DocC 4 |                 |                \n",
    "             \"X\" |          DocA 3 |          DocB 2 |                \n",
    "             \"Y\" |          DocA 3 |          DocB 2 |          DocC 4\n",
    "             \"Z\" |          DocA 3 |          DocC 4 |                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Calculate similarities - run the commands and insure that your output matches the output below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: you must run in hadoop mode to generate sorted similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/similarity.root.20170612.213258.131085\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/similarity.root.20170612.213258.131085/files/...\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob3718002972068040596.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497275441269_0026\n",
      "  Submitted application application_1497275441269_0026\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1497275441269_0026/\n",
      "  Running job: job_1497275441269_0026\n",
      "  Job job_1497275441269_0026 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497275441269_0026 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity.root.20170612.213258.131085/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2868\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=21828\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=7280\n",
      "\t\tFILE: Number of bytes written=371199\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3206\n",
      "\t\tHDFS: Number of bytes written=21828\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=12429312\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5079040\n",
      "\t\tTotal time spent by all map tasks (ms)=12138\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12138\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4960\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4960\n",
      "\t\tTotal vcore-seconds taken by all map tasks=12138\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4960\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2480\n",
      "\t\tCombine input records=318\n",
      "\t\tCombine output records=57\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=151\n",
      "\t\tInput split bytes=338\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=11664\n",
      "\t\tMap output materialized bytes=7286\n",
      "\t\tMap output records=318\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=697212928\n",
      "\t\tReduce input groups=29\n",
      "\t\tReduce input records=57\n",
      "\t\tReduce output records=378\n",
      "\t\tReduce shuffle bytes=7286\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=114\n",
      "\t\tTotal committed heap usage (bytes)=600309760\n",
      "\t\tVirtual memory (bytes) snapshot=4101885952\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/similarity.root.20170612.213258.131085/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/similarity.root.20170612.213258.131085...\n",
      "Removing temp directory /tmp/similarity.root.20170612.213258.131085...\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop systems_test_index_1 --cmdenv PATH=/opt/anaconda/bin:$PATH  > systems_test_similarities_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/similarity.root.20170612.213401.115261\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/similarity.root.20170612.213401.115261/files/...\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob8473411775053507457.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497275441269_0027\n",
      "  Submitted application application_1497275441269_0027\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1497275441269_0027/\n",
      "  Running job: job_1497275441269_0027\n",
      "  Job job_1497275441269_0027 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497275441269_0027 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity.root.20170612.213401.115261/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=206\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=280\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=277\n",
      "\t\tFILE: Number of bytes written=357193\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=544\n",
      "\t\tHDFS: Number of bytes written=280\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=13785088\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5533696\n",
      "\t\tTotal time spent by all map tasks (ms)=13462\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=13462\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5404\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5404\n",
      "\t\tTotal vcore-seconds taken by all map tasks=13462\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5404\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2470\n",
      "\t\tCombine input records=20\n",
      "\t\tCombine output records=10\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=159\n",
      "\t\tInput split bytes=338\n",
      "\t\tMap input records=4\n",
      "\t\tMap output bytes=360\n",
      "\t\tMap output materialized bytes=283\n",
      "\t\tMap output records=20\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=724832256\n",
      "\t\tReduce input groups=5\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=6\n",
      "\t\tReduce shuffle bytes=283\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=20\n",
      "\t\tTotal committed heap usage (bytes)=639631360\n",
      "\t\tVirtual memory (bytes) snapshot=4083044352\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/similarity.root.20170612.213401.115261/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/similarity.root.20170612.213401.115261...\n",
      "Removing temp directory /tmp/similarity.root.20170612.213401.115261...\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop systems_test_index_2 --cmdenv PATH=/opt/anaconda/bin:$PATH > systems_test_similarities_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/similarity.root.20170612.213508.518576\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/similarity.root.20170612.213508.518576/files/...\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 1...\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob3333690816835545755.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1497275441269_0028\n",
      "  Submitted application application_1497275441269_0028\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1497275441269_0028/\n",
      "  Running job: job_1497275441269_0028\n",
      "  Job job_1497275441269_0028 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1497275441269_0028 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity.root.20170612.213508.518576/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=167\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=156\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=160\n",
      "\t\tFILE: Number of bytes written=356959\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=505\n",
      "\t\tHDFS: Number of bytes written=156\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=24995840\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=7512064\n",
      "\t\tTotal time spent by all map tasks (ms)=24410\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=24410\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7336\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=7336\n",
      "\t\tTotal vcore-seconds taken by all map tasks=24410\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=7336\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2780\n",
      "\t\tCombine input records=18\n",
      "\t\tCombine output records=7\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=247\n",
      "\t\tInput split bytes=338\n",
      "\t\tMap input records=5\n",
      "\t\tMap output bytes=284\n",
      "\t\tMap output materialized bytes=166\n",
      "\t\tMap output records=18\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=720113664\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=166\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=14\n",
      "\t\tTotal committed heap usage (bytes)=644874240\n",
      "\t\tVirtual memory (bytes) snapshot=4093345792\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/similarity.root.20170612.213508.518576/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/similarity.root.20170612.213508.518576...\n",
      "Removing temp directory /tmp/similarity.root.20170612.213508.518576...\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop systems_test_index_3 --cmdenv PATH=/opt/anaconda/bin:$PATH > systems_test_similarities_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Systems test  3  - Similarity measures\n",
      "\n",
      "           pair |        jaccard|           Dice\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "[\"DocA\",\"DocB\"] |       0.666667|       0.800000\n",
      "[\"DocA\",\"DocC\"] |       0.400000|       0.571429\n",
      "[\"DocB\",\"DocC\"] |       0.200000|       0.333333\n",
      "\n",
      "Systems test  2  - Similarity measures\n",
      "\n",
      "           pair |        jaccard|           Dice\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "[\"atlas\",\"dipped\"] |       0.250000|       0.400000\n",
      "[\"atlas\",\"boon\"] |       0.250000|       0.400000\n",
      "[\"atlas\",\"cava\"] |       1.000000|       1.000000\n",
      "[\"boon\",\"dipped\"] |       0.500000|       0.666667\n",
      "[\"boon\",\"cava\"] |       0.250000|       0.400000\n",
      "[\"cava\",\"dipped\"] |       0.250000|       0.400000\n",
      "\n",
      "Systems test  1  - Similarity measures\n",
      "\n",
      "           pair |        jaccard|           Dice\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "[\"a\",\"limited\"] |       0.107143|       0.193548\n",
      "    [\"a\",\"sea\"] |       0.107143|       0.193548\n",
      "[\"a\",\"general\"] |       0.107143|       0.193548\n",
      " [\"a\",\"female\"] |       0.107143|       0.193548\n",
      "     [\"a\",\"in\"] |       0.214286|       0.352941\n",
      "[\"a\",\"religious\"] |       0.107143|       0.193548\n",
      " [\"a\",\"george\"] |       0.107143|       0.193548\n",
      "[\"a\",\"biography\"] |       0.107143|       0.193548\n",
      "   [\"a\",\"city\"] |       0.107143|       0.193548\n",
      "    [\"a\",\"for\"] |       0.107143|       0.193548\n",
      "  [\"a\",\"tales\"] |       0.107143|       0.193548\n",
      "[\"a\",\"child's\"] |       0.107143|       0.193548\n",
      "  [\"a\",\"forms\"] |       0.071429|       0.133333\n",
      "  [\"a\",\"wales\"] |       0.107143|       0.193548\n",
      "[\"a\",\"christmas\"] |       0.107143|       0.193548\n",
      "[\"a\",\"government\"] |       0.107143|       0.193548\n",
      "[\"a\",\"collection\"] |       0.142857|       0.250000\n",
      "     [\"a\",\"by\"] |       0.107143|       0.193548\n",
      "   [\"a\",\"case\"] |       0.214286|       0.352941\n",
      "[\"a\",\"circumstantial\"] |       0.107143|       0.193548\n",
      "  [\"a\",\"fairy\"] |       0.107143|       0.193548\n",
      "     [\"a\",\"of\"] |       0.535714|       0.697674\n",
      "  [\"a\",\"study\"] |       0.214286|       0.352941\n",
      "   [\"a\",\"bill\"] |       0.107143|       0.193548\n",
      "[\"a\",\"establishing\"] |       0.107143|       0.193548\n",
      "[\"a\",\"narrative\"] |       0.107143|       0.193548\n",
      "    [\"a\",\"the\"] |       0.214286|       0.352941\n",
      "[\"bill\",\"limited\"] |       0.142857|       0.250000\n",
      "[\"bill\",\"female\"] |       0.142857|       0.250000\n",
      "[\"bill\",\"general\"] |       0.142857|       0.250000\n",
      " [\"bill\",\"sea\"] |       0.142857|       0.250000\n",
      "  [\"bill\",\"in\"] |       0.100000|       0.181818\n",
      "[\"bill\",\"religious\"] |       0.600000|       0.750000\n",
      "[\"bill\",\"george\"] |       0.142857|       0.250000\n",
      "[\"bill\",\"biography\"] |       0.142857|       0.250000\n",
      "[\"bill\",\"city\"] |       0.142857|       0.250000\n",
      " [\"bill\",\"for\"] |       0.600000|       0.750000\n",
      "[\"bill\",\"tales\"] |       0.142857|       0.250000\n",
      "[\"bill\",\"government\"] |       0.142857|       0.250000\n",
      " [\"bill\",\"the\"] |       0.100000|       0.181818\n",
      "[\"bill\",\"forms\"] |       0.166667|       0.285714\n",
      "[\"bill\",\"wales\"] |       0.142857|       0.250000\n",
      "[\"bill\",\"christmas\"] |       0.142857|       0.250000\n",
      "[\"bill\",\"child's\"] |       0.142857|       0.250000\n",
      "[\"bill\",\"collection\"] |       0.125000|       0.222222\n",
      "  [\"bill\",\"by\"] |       0.142857|       0.250000\n",
      "[\"bill\",\"case\"] |       0.100000|       0.181818\n",
      "[\"bill\",\"circumstantial\"] |       0.142857|       0.250000\n",
      "  [\"bill\",\"of\"] |       0.052632|       0.100000\n",
      "[\"bill\",\"study\"] |       0.100000|       0.181818\n",
      "[\"bill\",\"establishing\"] |       0.600000|       0.750000\n",
      "[\"bill\",\"narrative\"] |       0.142857|       0.250000\n",
      "[\"bill\",\"fairy\"] |       0.142857|       0.250000\n",
      "[\"biography\",\"limited\"] |       0.333333|       0.500000\n",
      "[\"biography\",\"female\"] |       0.333333|       0.500000\n",
      "[\"biography\",\"general\"] |       0.600000|       0.750000\n",
      "[\"biography\",\"sea\"] |       0.142857|       0.250000\n",
      "[\"biography\",\"in\"] |       0.100000|       0.181818\n",
      "[\"biography\",\"religious\"] |       0.142857|       0.250000\n",
      "[\"biography\",\"george\"] |       0.600000|       0.750000\n",
      "[\"biography\",\"city\"] |       0.142857|       0.250000\n",
      "[\"biography\",\"for\"] |       0.142857|       0.250000\n",
      "[\"biography\",\"tales\"] |       0.333333|       0.500000\n",
      "[\"biography\",\"government\"] |       0.142857|       0.250000\n",
      "[\"biography\",\"the\"] |       0.222222|       0.363636\n",
      "[\"biography\",\"forms\"] |       0.400000|       0.571429\n",
      "[\"biography\",\"wales\"] |       0.142857|       0.250000\n",
      "[\"biography\",\"christmas\"] |       0.142857|       0.250000\n",
      "[\"biography\",\"child's\"] |       0.142857|       0.250000\n",
      "[\"biography\",\"collection\"] |       0.285714|       0.444444\n",
      "[\"biography\",\"by\"] |       0.142857|       0.250000\n",
      "[\"biography\",\"case\"] |       0.222222|       0.363636\n",
      "[\"biography\",\"circumstantial\"] |       0.333333|       0.500000\n",
      "[\"biography\",\"of\"] |       0.250000|       0.400000\n",
      "[\"biography\",\"study\"] |       0.222222|       0.363636\n",
      "[\"biography\",\"establishing\"] |       0.142857|       0.250000\n",
      "[\"biography\",\"narrative\"] |       0.333333|       0.500000\n",
      "[\"biography\",\"fairy\"] |       0.333333|       0.500000\n",
      "[\"by\",\"limited\"] |       0.142857|       0.250000\n",
      "   [\"by\",\"sea\"] |       0.600000|       0.750000\n",
      "[\"by\",\"general\"] |       0.142857|       0.250000\n",
      "[\"by\",\"female\"] |       0.142857|       0.250000\n",
      "    [\"by\",\"in\"] |       0.100000|       0.181818\n",
      "[\"by\",\"religious\"] |       0.142857|       0.250000\n",
      "[\"by\",\"george\"] |       0.142857|       0.250000\n",
      "  [\"by\",\"city\"] |       0.600000|       0.750000\n",
      "   [\"by\",\"for\"] |       0.142857|       0.250000\n",
      " [\"by\",\"tales\"] |       0.142857|       0.250000\n",
      "[\"by\",\"child's\"] |       0.142857|       0.250000\n",
      " [\"by\",\"forms\"] |       0.166667|       0.285714\n",
      " [\"by\",\"wales\"] |       0.142857|       0.250000\n",
      "[\"by\",\"christmas\"] |       0.142857|       0.250000\n",
      "[\"by\",\"government\"] |       0.142857|       0.250000\n",
      "[\"by\",\"collection\"] |       0.125000|       0.222222\n",
      "   [\"by\",\"the\"] |       0.375000|       0.545455\n",
      "  [\"by\",\"case\"] |       0.100000|       0.181818\n",
      "[\"by\",\"circumstantial\"] |       0.333333|       0.500000\n",
      "    [\"by\",\"of\"] |       0.111111|       0.200000\n",
      " [\"by\",\"study\"] |       0.100000|       0.181818\n",
      "[\"by\",\"establishing\"] |       0.142857|       0.250000\n",
      "[\"by\",\"narrative\"] |       0.333333|       0.500000\n",
      " [\"by\",\"fairy\"] |       0.142857|       0.250000\n",
      "[\"case\",\"limited\"] |       0.375000|       0.545455\n",
      "[\"case\",\"female\"] |       0.375000|       0.545455\n",
      "[\"case\",\"general\"] |       0.222222|       0.363636\n",
      " [\"case\",\"sea\"] |       0.100000|       0.181818\n",
      "  [\"case\",\"in\"] |       0.272727|       0.428571\n",
      "[\"case\",\"religious\"] |       0.100000|       0.181818\n",
      "[\"case\",\"george\"] |       0.222222|       0.363636\n",
      "[\"case\",\"city\"] |       0.100000|       0.181818\n",
      " [\"case\",\"for\"] |       0.100000|       0.181818\n",
      "[\"case\",\"tales\"] |       0.222222|       0.363636\n",
      "[\"case\",\"government\"] |       0.375000|       0.545455\n",
      "[\"case\",\"forms\"] |       0.250000|       0.400000\n",
      "[\"case\",\"wales\"] |       0.222222|       0.363636\n",
      "[\"case\",\"christmas\"] |       0.222222|       0.363636\n",
      "[\"case\",\"child's\"] |       0.222222|       0.363636\n",
      "[\"case\",\"collection\"] |       0.200000|       0.333333\n",
      "[\"case\",\"fairy\"] |       0.222222|       0.363636\n",
      "[\"case\",\"circumstantial\"] |       0.222222|       0.363636\n",
      "  [\"case\",\"of\"] |       0.277778|       0.434783\n",
      "[\"case\",\"study\"] |       0.750000|       0.857143\n",
      "[\"case\",\"establishing\"] |       0.100000|       0.181818\n",
      "[\"case\",\"narrative\"] |       0.222222|       0.363636\n",
      " [\"case\",\"the\"] |       0.166667|       0.285714\n",
      "[\"child's\",\"limited\"] |       0.142857|       0.250000\n",
      "[\"child's\",\"sea\"] |       0.142857|       0.250000\n",
      "[\"child's\",\"general\"] |       0.142857|       0.250000\n",
      "[\"child's\",\"female\"] |       0.142857|       0.250000\n",
      "[\"child's\",\"in\"] |       0.375000|       0.545455\n",
      "[\"child's\",\"religious\"] |       0.142857|       0.250000\n",
      "[\"child's\",\"george\"] |       0.142857|       0.250000\n",
      "[\"child's\",\"city\"] |       0.142857|       0.250000\n",
      "[\"child's\",\"for\"] |       0.142857|       0.250000\n",
      "[\"child's\",\"tales\"] |       0.142857|       0.250000\n",
      "[\"child's\",\"forms\"] |       0.166667|       0.285714\n",
      "[\"child's\",\"wales\"] |       0.600000|       0.750000\n",
      "[\"child's\",\"christmas\"] |       0.600000|       0.750000\n",
      "[\"child's\",\"government\"] |       0.333333|       0.500000\n",
      "[\"child's\",\"collection\"] |       0.125000|       0.222222\n",
      "[\"child's\",\"fairy\"] |       0.142857|       0.250000\n",
      "[\"child's\",\"circumstantial\"] |       0.142857|       0.250000\n",
      "[\"child's\",\"of\"] |       0.052632|       0.100000\n",
      "[\"child's\",\"study\"] |       0.222222|       0.363636\n",
      "[\"child's\",\"establishing\"] |       0.142857|       0.250000\n",
      "[\"child's\",\"narrative\"] |       0.142857|       0.250000\n",
      "[\"child's\",\"the\"] |       0.100000|       0.181818\n",
      "[\"christmas\",\"city\"] |       0.142857|       0.250000\n",
      "[\"christmas\",\"circumstantial\"] |       0.142857|       0.250000\n",
      "[\"christmas\",\"limited\"] |       0.142857|       0.250000\n",
      "[\"christmas\",\"sea\"] |       0.142857|       0.250000\n",
      "[\"christmas\",\"for\"] |       0.142857|       0.250000\n",
      "[\"christmas\",\"government\"] |       0.333333|       0.500000\n",
      "[\"christmas\",\"of\"] |       0.052632|       0.100000\n",
      "[\"christmas\",\"study\"] |       0.222222|       0.363636\n",
      "[\"christmas\",\"narrative\"] |       0.142857|       0.250000\n",
      "[\"christmas\",\"collection\"] |       0.125000|       0.222222\n",
      "[\"christmas\",\"establishing\"] |       0.142857|       0.250000\n",
      "[\"christmas\",\"forms\"] |       0.166667|       0.285714\n",
      "[\"christmas\",\"tales\"] |       0.142857|       0.250000\n",
      "[\"christmas\",\"female\"] |       0.142857|       0.250000\n",
      "[\"christmas\",\"in\"] |       0.375000|       0.545455\n",
      "[\"christmas\",\"wales\"] |       0.600000|       0.750000\n",
      "[\"christmas\",\"the\"] |       0.100000|       0.181818\n",
      "[\"christmas\",\"general\"] |       0.142857|       0.250000\n",
      "[\"christmas\",\"religious\"] |       0.142857|       0.250000\n",
      "[\"christmas\",\"george\"] |       0.142857|       0.250000\n",
      "[\"christmas\",\"fairy\"] |       0.142857|       0.250000\n",
      "[\"circumstantial\",\"city\"] |       0.333333|       0.500000\n",
      "[\"circumstantial\",\"in\"] |       0.100000|       0.181818\n",
      "[\"circumstantial\",\"female\"] |       0.333333|       0.500000\n",
      "[\"circumstantial\",\"for\"] |       0.142857|       0.250000\n",
      "[\"circumstantial\",\"limited\"] |       0.333333|       0.500000\n",
      "[\"circumstantial\",\"of\"] |       0.250000|       0.400000\n",
      "[\"circumstantial\",\"tales\"] |       0.333333|       0.500000\n",
      "[\"circumstantial\",\"government\"] |       0.142857|       0.250000\n",
      "[\"circumstantial\",\"collection\"] |       0.285714|       0.444444\n",
      "[\"circumstantial\",\"general\"] |       0.333333|       0.500000\n",
      "[\"circumstantial\",\"forms\"] |       0.400000|       0.571429\n",
      "[\"circumstantial\",\"religious\"] |       0.142857|       0.250000\n",
      "[\"circumstantial\",\"sea\"] |       0.333333|       0.500000\n",
      "[\"circumstantial\",\"narrative\"] |       0.600000|       0.750000\n",
      "[\"circumstantial\",\"study\"] |       0.222222|       0.363636\n",
      "[\"circumstantial\",\"fairy\"] |       0.333333|       0.500000\n",
      "[\"circumstantial\",\"establishing\"] |       0.142857|       0.250000\n",
      "[\"circumstantial\",\"the\"] |       0.375000|       0.545455\n",
      "[\"circumstantial\",\"george\"] |       0.333333|       0.500000\n",
      "[\"circumstantial\",\"wales\"] |       0.142857|       0.250000\n",
      "[\"city\",\"limited\"] |       0.142857|       0.250000\n",
      " [\"city\",\"sea\"] |       0.600000|       0.750000\n",
      " [\"city\",\"for\"] |       0.142857|       0.250000\n",
      "[\"city\",\"government\"] |       0.142857|       0.250000\n",
      "  [\"city\",\"of\"] |       0.111111|       0.200000\n",
      "[\"city\",\"study\"] |       0.100000|       0.181818\n",
      "[\"city\",\"narrative\"] |       0.333333|       0.500000\n",
      "[\"city\",\"collection\"] |       0.125000|       0.222222\n",
      "[\"city\",\"establishing\"] |       0.142857|       0.250000\n",
      "[\"city\",\"forms\"] |       0.166667|       0.285714\n",
      "[\"city\",\"tales\"] |       0.142857|       0.250000\n",
      "[\"city\",\"female\"] |       0.142857|       0.250000\n",
      "  [\"city\",\"in\"] |       0.100000|       0.181818\n",
      "[\"city\",\"wales\"] |       0.142857|       0.250000\n",
      " [\"city\",\"the\"] |       0.375000|       0.545455\n",
      "[\"city\",\"general\"] |       0.142857|       0.250000\n",
      "[\"city\",\"religious\"] |       0.142857|       0.250000\n",
      "[\"city\",\"george\"] |       0.142857|       0.250000\n",
      "[\"city\",\"fairy\"] |       0.142857|       0.250000\n",
      "[\"collection\",\"limited\"] |       0.285714|       0.444444\n",
      "[\"collection\",\"sea\"] |       0.125000|       0.222222\n",
      "[\"collection\",\"for\"] |       0.125000|       0.222222\n",
      "[\"collection\",\"government\"] |       0.125000|       0.222222\n",
      "[\"collection\",\"of\"] |       0.312500|       0.476190\n",
      "[\"collection\",\"tales\"] |       0.500000|       0.666667\n",
      "[\"collection\",\"in\"] |       0.090909|       0.166667\n",
      "[\"collection\",\"religious\"] |       0.125000|       0.222222\n",
      "[\"collection\",\"general\"] |       0.285714|       0.444444\n",
      "[\"collection\",\"forms\"] |       0.333333|       0.500000\n",
      "[\"collection\",\"female\"] |       0.285714|       0.444444\n",
      "[\"collection\",\"narrative\"] |       0.285714|       0.444444\n",
      "[\"collection\",\"study\"] |       0.200000|       0.333333\n",
      "[\"collection\",\"fairy\"] |       0.500000|       0.666667\n",
      "[\"collection\",\"establishing\"] |       0.125000|       0.222222\n",
      "[\"collection\",\"the\"] |       0.200000|       0.333333\n",
      "[\"collection\",\"george\"] |       0.285714|       0.444444\n",
      "[\"collection\",\"wales\"] |       0.125000|       0.222222\n",
      "[\"establishing\",\"limited\"] |       0.142857|       0.250000\n",
      "[\"establishing\",\"sea\"] |       0.142857|       0.250000\n",
      "[\"establishing\",\"for\"] |       0.600000|       0.750000\n",
      "[\"establishing\",\"government\"] |       0.142857|       0.250000\n",
      "[\"establishing\",\"of\"] |       0.052632|       0.100000\n",
      "[\"establishing\",\"study\"] |       0.100000|       0.181818\n",
      "[\"establishing\",\"narrative\"] |       0.142857|       0.250000\n",
      "[\"establishing\",\"general\"] |       0.142857|       0.250000\n",
      "[\"establishing\",\"forms\"] |       0.166667|       0.285714\n",
      "[\"establishing\",\"tales\"] |       0.142857|       0.250000\n",
      "[\"establishing\",\"female\"] |       0.142857|       0.250000\n",
      "[\"establishing\",\"in\"] |       0.100000|       0.181818\n",
      "[\"establishing\",\"wales\"] |       0.142857|       0.250000\n",
      "[\"establishing\",\"the\"] |       0.100000|       0.181818\n",
      "[\"establishing\",\"religious\"] |       0.600000|       0.750000\n",
      "[\"establishing\",\"george\"] |       0.142857|       0.250000\n",
      "[\"establishing\",\"fairy\"] |       0.142857|       0.250000\n",
      "[\"fairy\",\"limited\"] |       0.333333|       0.500000\n",
      "[\"fairy\",\"sea\"] |       0.142857|       0.250000\n",
      "[\"fairy\",\"for\"] |       0.142857|       0.250000\n",
      "[\"fairy\",\"government\"] |       0.142857|       0.250000\n",
      " [\"fairy\",\"of\"] |       0.250000|       0.400000\n",
      "[\"fairy\",\"tales\"] |       0.600000|       0.750000\n",
      " [\"fairy\",\"in\"] |       0.100000|       0.181818\n",
      "[\"fairy\",\"general\"] |       0.333333|       0.500000\n",
      "[\"fairy\",\"forms\"] |       0.750000|       0.857143\n",
      "[\"fairy\",\"female\"] |       0.333333|       0.500000\n",
      "[\"fairy\",\"narrative\"] |       0.333333|       0.500000\n",
      "[\"fairy\",\"study\"] |       0.222222|       0.363636\n",
      "[\"fairy\",\"the\"] |       0.222222|       0.363636\n",
      "[\"fairy\",\"religious\"] |       0.142857|       0.250000\n",
      "[\"fairy\",\"george\"] |       0.333333|       0.500000\n",
      "[\"fairy\",\"wales\"] |       0.142857|       0.250000\n",
      "[\"female\",\"limited\"] |       1.000000|       1.000000\n",
      "[\"female\",\"for\"] |       0.142857|       0.250000\n",
      "[\"female\",\"government\"] |       0.600000|       0.750000\n",
      "[\"female\",\"of\"] |       0.250000|       0.400000\n",
      "[\"female\",\"study\"] |       0.375000|       0.545455\n",
      "[\"female\",\"narrative\"] |       0.333333|       0.500000\n",
      "[\"female\",\"general\"] |       0.333333|       0.500000\n",
      "[\"female\",\"forms\"] |       0.400000|       0.571429\n",
      "[\"female\",\"tales\"] |       0.333333|       0.500000\n",
      "[\"female\",\"sea\"] |       0.142857|       0.250000\n",
      "[\"female\",\"in\"] |       0.375000|       0.545455\n",
      "[\"female\",\"wales\"] |       0.142857|       0.250000\n",
      "[\"female\",\"the\"] |       0.222222|       0.363636\n",
      "[\"female\",\"religious\"] |       0.142857|       0.250000\n",
      "[\"female\",\"george\"] |       0.333333|       0.500000\n",
      "[\"for\",\"limited\"] |       0.142857|       0.250000\n",
      "[\"for\",\"government\"] |       0.142857|       0.250000\n",
      "   [\"for\",\"of\"] |       0.052632|       0.100000\n",
      "[\"for\",\"study\"] |       0.100000|       0.181818\n",
      "[\"for\",\"narrative\"] |       0.142857|       0.250000\n",
      "[\"for\",\"general\"] |       0.142857|       0.250000\n",
      "[\"for\",\"forms\"] |       0.166667|       0.285714\n",
      "[\"for\",\"tales\"] |       0.142857|       0.250000\n",
      "  [\"for\",\"sea\"] |       0.142857|       0.250000\n",
      "   [\"for\",\"in\"] |       0.100000|       0.181818\n",
      "[\"for\",\"wales\"] |       0.142857|       0.250000\n",
      "  [\"for\",\"the\"] |       0.100000|       0.181818\n",
      "[\"for\",\"religious\"] |       0.600000|       0.750000\n",
      "[\"for\",\"george\"] |       0.142857|       0.250000\n",
      "[\"forms\",\"limited\"] |       0.400000|       0.571429\n",
      "[\"forms\",\"government\"] |       0.166667|       0.285714\n",
      " [\"forms\",\"of\"] |       0.187500|       0.315789\n",
      "[\"forms\",\"study\"] |       0.250000|       0.400000\n",
      " [\"forms\",\"in\"] |       0.111111|       0.200000\n",
      "[\"forms\",\"general\"] |       0.400000|       0.571429\n",
      "[\"forms\",\"tales\"] |       0.750000|       0.857143\n",
      "[\"forms\",\"sea\"] |       0.166667|       0.285714\n",
      "[\"forms\",\"narrative\"] |       0.400000|       0.571429\n",
      "[\"forms\",\"wales\"] |       0.166667|       0.285714\n",
      "[\"forms\",\"the\"] |       0.250000|       0.400000\n",
      "[\"forms\",\"religious\"] |       0.166667|       0.285714\n",
      "[\"forms\",\"george\"] |       0.400000|       0.571429\n",
      "[\"general\",\"limited\"] |       0.333333|       0.500000\n",
      "[\"general\",\"government\"] |       0.142857|       0.250000\n",
      "[\"general\",\"of\"] |       0.250000|       0.400000\n",
      "[\"general\",\"tales\"] |       0.333333|       0.500000\n",
      "[\"general\",\"in\"] |       0.100000|       0.181818\n",
      "[\"general\",\"george\"] |       0.600000|       0.750000\n",
      "[\"general\",\"sea\"] |       0.142857|       0.250000\n",
      "[\"general\",\"narrative\"] |       0.333333|       0.500000\n",
      "[\"general\",\"wales\"] |       0.142857|       0.250000\n",
      "[\"general\",\"the\"] |       0.222222|       0.363636\n",
      "[\"general\",\"religious\"] |       0.142857|       0.250000\n",
      "[\"general\",\"study\"] |       0.222222|       0.363636\n",
      "[\"george\",\"limited\"] |       0.333333|       0.500000\n",
      "[\"george\",\"government\"] |       0.142857|       0.250000\n",
      "[\"george\",\"of\"] |       0.250000|       0.400000\n",
      "[\"george\",\"study\"] |       0.222222|       0.363636\n",
      "[\"george\",\"in\"] |       0.100000|       0.181818\n",
      "[\"george\",\"sea\"] |       0.142857|       0.250000\n",
      "[\"george\",\"narrative\"] |       0.333333|       0.500000\n",
      "[\"george\",\"wales\"] |       0.142857|       0.250000\n",
      "[\"george\",\"the\"] |       0.222222|       0.363636\n",
      "[\"george\",\"religious\"] |       0.142857|       0.250000\n",
      "[\"george\",\"tales\"] |       0.333333|       0.500000\n",
      "[\"government\",\"limited\"] |       0.600000|       0.750000\n",
      "[\"government\",\"of\"] |       0.176471|       0.300000\n",
      "[\"government\",\"study\"] |       0.375000|       0.545455\n",
      "[\"government\",\"narrative\"] |       0.142857|       0.250000\n",
      "[\"government\",\"sea\"] |       0.142857|       0.250000\n",
      "[\"government\",\"in\"] |       0.375000|       0.545455\n",
      "[\"government\",\"wales\"] |       0.333333|       0.500000\n",
      "[\"government\",\"the\"] |       0.100000|       0.181818\n",
      "[\"government\",\"religious\"] |       0.142857|       0.250000\n",
      "[\"government\",\"tales\"] |       0.142857|       0.250000\n",
      "[\"in\",\"limited\"] |       0.375000|       0.545455\n",
      "    [\"in\",\"of\"] |       0.150000|       0.260870\n",
      " [\"in\",\"study\"] |       0.272727|       0.428571\n",
      "   [\"in\",\"sea\"] |       0.100000|       0.181818\n",
      "[\"in\",\"narrative\"] |       0.100000|       0.181818\n",
      " [\"in\",\"wales\"] |       0.375000|       0.545455\n",
      "   [\"in\",\"the\"] |       0.076923|       0.142857\n",
      "[\"in\",\"religious\"] |       0.100000|       0.181818\n",
      " [\"in\",\"tales\"] |       0.100000|       0.181818\n",
      "[\"limited\",\"of\"] |       0.250000|       0.400000\n",
      "[\"limited\",\"study\"] |       0.375000|       0.545455\n",
      "[\"limited\",\"sea\"] |       0.142857|       0.250000\n",
      "[\"limited\",\"narrative\"] |       0.333333|       0.500000\n",
      "[\"limited\",\"wales\"] |       0.142857|       0.250000\n",
      "[\"limited\",\"the\"] |       0.222222|       0.363636\n",
      "[\"limited\",\"religious\"] |       0.142857|       0.250000\n",
      "[\"limited\",\"tales\"] |       0.333333|       0.500000\n",
      "[\"narrative\",\"of\"] |       0.250000|       0.400000\n",
      "[\"narrative\",\"study\"] |       0.222222|       0.363636\n",
      "[\"narrative\",\"sea\"] |       0.333333|       0.500000\n",
      "[\"narrative\",\"wales\"] |       0.142857|       0.250000\n",
      "[\"narrative\",\"the\"] |       0.375000|       0.545455\n",
      "[\"narrative\",\"religious\"] |       0.142857|       0.250000\n",
      "[\"narrative\",\"tales\"] |       0.333333|       0.500000\n",
      " [\"of\",\"study\"] |       0.277778|       0.434783\n",
      "   [\"of\",\"sea\"] |       0.111111|       0.200000\n",
      " [\"of\",\"wales\"] |       0.052632|       0.100000\n",
      "   [\"of\",\"the\"] |       0.210526|       0.347826\n",
      "[\"of\",\"religious\"] |       0.052632|       0.100000\n",
      " [\"of\",\"tales\"] |       0.250000|       0.400000\n",
      "[\"religious\",\"wales\"] |       0.142857|       0.250000\n",
      "[\"religious\",\"the\"] |       0.100000|       0.181818\n",
      "[\"religious\",\"study\"] |       0.100000|       0.181818\n",
      "[\"religious\",\"sea\"] |       0.142857|       0.250000\n",
      "[\"religious\",\"tales\"] |       0.142857|       0.250000\n",
      "[\"sea\",\"wales\"] |       0.142857|       0.250000\n",
      "  [\"sea\",\"the\"] |       0.375000|       0.545455\n",
      "[\"sea\",\"study\"] |       0.100000|       0.181818\n",
      "[\"sea\",\"tales\"] |       0.142857|       0.250000\n",
      "[\"study\",\"wales\"] |       0.222222|       0.363636\n",
      "[\"study\",\"tales\"] |       0.222222|       0.363636\n",
      "[\"study\",\"the\"] |       0.166667|       0.285714\n",
      "[\"tales\",\"wales\"] |       0.142857|       0.250000\n",
      "[\"tales\",\"the\"] |       0.222222|       0.363636\n",
      "[\"the\",\"wales\"] |       0.100000|       0.181818\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Pretty print systems tests\n",
    "############################################\n",
    "\n",
    "import json\n",
    "for i in range(3,0,-1):\n",
    "  print ''*110\n",
    "  print \"Systems test \",i,\" - Similarity measures\"\n",
    "  print ''*110\n",
    "  print \"{0:>15} |{1:>15}|{2:>15}\".format(\n",
    "           \"pair\",  \"jaccard\",\"Dice\")\n",
    "  print '-'*110\n",
    "\n",
    "  with open(\"systems_test_similarities_\"+str(i),\"r\") as f:\n",
    "      lines = f.readlines()\n",
    "      for line in lines:\n",
    "          line = line.strip()\n",
    "          pair,stripe = line.split(\"\\t\")\n",
    "          stripe = json.loads(stripe)\n",
    "     \n",
    "          print \"{0:>15} |{1:>15f}|{2:>15f}\".format(\n",
    "              pair,float(stripe['jaccard']),float(stripe['dice'] ))\n",
    "#          print \"{0:>15f} |{1:>15} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "#               float(avg), stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]))\n",
    "          #print \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# Pretty print systems tests\n",
    "############################################\n",
    "\n",
    "import json\n",
    "for i in range(1,4):\n",
    "  print ''*110\n",
    "  print \"Systems test \",i,\" - Similarity measures\"\n",
    "  print ''*110\n",
    "  print \"{0:>15} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "          \"average\", \"pair\", \"cosine\", \"jaccard\", \"overlap\", \"dice\")\n",
    "  print '-'*110\n",
    "\n",
    "  with open(\"systems_test_similarities_\"+str(i),\"r\") as f:\n",
    "      lines = f.readlines()\n",
    "      for line in lines:\n",
    "          line = line.strip()\n",
    "          avg,stripe = line.split(\"\\t\")\n",
    "          stripe = json.loads(stripe)\n",
    "          print avg,stripe\n",
    "          print \"{0:>15} |{1:>15} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "              stripe, avg, dice\")\n",
    "#          print \"{0:>15f} |{1:>15} |{2:>15f} |{3:>15f} |{4:>15f} |{5:>15f}\".format(\n",
    "#               float(avg), stripe[0], float(stripe[1]), float(stripe[2]), float(stripe[3]), float(stripe[4]))\n",
    "          #print \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Similairity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Systems test  1  - Similarity measures\n",
    "\n",
    "   average |                pair |         cosine |        jaccard |        overlap |           dice\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "  1.000000 |    female - limited |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
    "  0.868292 |       fairy - forms |       0.866025 |       0.750000 |       1.000000 |       0.857143\n",
    "  0.868292 |       forms - tales |       0.866025 |       0.750000 |       1.000000 |       0.857143\n",
    "  0.830357 |        case - study |       0.857143 |       0.750000 |       0.857143 |       0.857143\n",
    "  0.712500 | bill - establishing |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |   christmas - wales |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |circumstantial - narrative |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |            by - sea |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |           by - city |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |     child's - wales |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |  biography - george |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 | child's - christmas |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  ...\n",
    "  \n",
    "\n",
    "Systems test  2  - Similarity measures\n",
    "\n",
    "   average |                pair |         cosine |        jaccard |        overlap |           dice\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "  1.000000 |        atlas - cava |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
    "  0.625000 |       boon - dipped |       0.666667 |       0.500000 |       0.666667 |       0.666667\n",
    "  0.389562 |       cava - dipped |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
    "  0.389562 |         boon - cava |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
    "  0.389562 |      atlas - dipped |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
    "  0.389562 |        atlas - boon |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
    "\n",
    "Systems test  3  - Similarity measures\n",
    "\n",
    "   average |                pair |         cosine |        jaccard |        overlap |           dice\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "  0.820791 |         DocA - DocB |       0.816497 |       0.666667 |       1.000000 |       0.800000\n",
    "  0.553861 |         DocA - DocC |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
    "  0.346722 |         DocB - DocC |       0.353553 |       0.200000 |       0.500000 |       0.333333\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# === END OF PHASE 1 ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
