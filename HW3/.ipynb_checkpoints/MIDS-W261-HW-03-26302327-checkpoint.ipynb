{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#MIDS---w261-Machine-Learning-At-Scale\" data-toc-modified-id=\"MIDS---w261-Machine-Learning-At-Scale-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MIDS - w261 Machine Learning At Scale</a></div><div class=\"lev2 toc-item\"><a href=\"#Assignment---HW3\" data-toc-modified-id=\"Assignment---HW3-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Assignment - HW3</a></div><div class=\"lev1 toc-item\"><a href=\"#Table-of-Contents-\" data-toc-modified-id=\"Table-of-Contents--2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Table of Contents </a></div><div class=\"lev1 toc-item\"><a href=\"#1-Instructions\" data-toc-modified-id=\"1-Instructions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>1 Instructions</a></div><div class=\"lev3 toc-item\"><a href=\"#IMPORTANT\" data-toc-modified-id=\"IMPORTANT-301\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>IMPORTANT</a></div><div class=\"lev3 toc-item\"><a href=\"#===-INSTRUCTIONS-for-SUBMISSIONS-===\" data-toc-modified-id=\"===-INSTRUCTIONS-for-SUBMISSIONS-===-302\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>=== INSTRUCTIONS for SUBMISSIONS ===</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.0.\" data-toc-modified-id=\"HW3.0.-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>HW3.0.</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.1-consumer-complaints-dataset:-Use-Counters-to-do-EDA-(exploratory-data-analysis-and-to-monitor-progress)\" data-toc-modified-id=\"HW3.1-consumer-complaints-dataset:-Use-Counters-to-do-EDA-(exploratory-data-analysis-and-to-monitor-progress)-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>HW3.1 consumer complaints dataset: Use Counters to do EDA (exploratory data analysis and to monitor progress)</a></div><div class=\"lev3 toc-item\"><a href=\"#HW-3.2-Analyze-the-performance-of-your-Mappers,-Combiners-and-Reducers-using-Counters\" data-toc-modified-id=\"HW-3.2-Analyze-the-performance-of-your-Mappers,-Combiners-and-Reducers-using-Counters-321\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters</a></div><div class=\"lev3 toc-item\"><a href=\"#3.2.A-SOLUTION\" data-toc-modified-id=\"3.2.A-SOLUTION-322\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>3.2.A SOLUTION</a></div><div class=\"lev4 toc-item\"><a href=\"#3.2.A-EXPLANATION\" data-toc-modified-id=\"3.2.A-EXPLANATION-3221\"><span class=\"toc-item-num\">3.2.2.1&nbsp;&nbsp;</span>3.2.A EXPLANATION</a></div><div class=\"lev3 toc-item\"><a href=\"#3.2.B-SOLUTION\" data-toc-modified-id=\"3.2.B-SOLUTION-323\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>3.2.B SOLUTION</a></div><div class=\"lev3 toc-item\"><a href=\"#3.2.C-SOLUTION\" data-toc-modified-id=\"3.2.C-SOLUTION-324\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>3.2.C SOLUTION</a></div><div class=\"lev3 toc-item\"><a href=\"#3.2.1\" data-toc-modified-id=\"3.2.1-325\"><span class=\"toc-item-num\">3.2.5&nbsp;&nbsp;</span>3.2.1</a></div><div class=\"lev4 toc-item\"><a href=\"#START-STUDENT-CODE-HW321-(INSERT-CELLS-BELOW-AS-NEEDED)\" data-toc-modified-id=\"START-STUDENT-CODE-HW321-(INSERT-CELLS-BELOW-AS-NEEDED)-3251\"><span class=\"toc-item-num\">3.2.5.1&nbsp;&nbsp;</span>START STUDENT CODE HW321 (INSERT CELLS BELOW AS NEEDED)</a></div><div class=\"lev4 toc-item\"><a href=\"#END-STUDENT-CODE-HW321\" data-toc-modified-id=\"END-STUDENT-CODE-HW321-3252\"><span class=\"toc-item-num\">3.2.5.2&nbsp;&nbsp;</span>END STUDENT CODE HW321</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.3.-Shopping-Cart-Analysis\" data-toc-modified-id=\"HW3.3.-Shopping-Cart-Analysis-33\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>HW3.3. Shopping Cart Analysis</a></div><div class=\"lev4 toc-item\"><a href=\"#START-STUDENT-CODE-HW33-(INSERT-CELLS-BELOW-AS-NEEDED)\" data-toc-modified-id=\"START-STUDENT-CODE-HW33-(INSERT-CELLS-BELOW-AS-NEEDED)-3301\"><span class=\"toc-item-num\">3.3.0.1&nbsp;&nbsp;</span>START STUDENT CODE HW33 (INSERT CELLS BELOW AS NEEDED)</a></div><div class=\"lev4 toc-item\"><a href=\"#END-STUDENT-CODE-HW33\" data-toc-modified-id=\"END-STUDENT-CODE-HW33-3302\"><span class=\"toc-item-num\">3.3.0.2&nbsp;&nbsp;</span>END STUDENT CODE HW33</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.3.1-OPTIONAL\" data-toc-modified-id=\"HW3.3.1-OPTIONAL-34\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>HW3.3.1 OPTIONAL</a></div><div class=\"lev4 toc-item\"><a href=\"#START-STUDENT-CODE-HW331-(INSERT-CELLS-BELOW-AS-NEEDED)\" data-toc-modified-id=\"START-STUDENT-CODE-HW331-(INSERT-CELLS-BELOW-AS-NEEDED)-3401\"><span class=\"toc-item-num\">3.4.0.1&nbsp;&nbsp;</span>START STUDENT CODE HW331 (INSERT CELLS BELOW AS NEEDED)</a></div><div class=\"lev4 toc-item\"><a href=\"#END-STUDENT-CODE-HW331\" data-toc-modified-id=\"END-STUDENT-CODE-HW331-3402\"><span class=\"toc-item-num\">3.4.0.2&nbsp;&nbsp;</span>END STUDENT CODE HW331</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.4.-(Computationally-prohibitive-but-then-again-Hadoop-can-handle-this)-Pairs\" data-toc-modified-id=\"HW3.4.-(Computationally-prohibitive-but-then-again-Hadoop-can-handle-this)-Pairs-35\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>HW3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs</a></div><div class=\"lev4 toc-item\"><a href=\"#START-STUDENT-CODE-HW34-(INSERT-CELLS-BELOW-AS-NEEDED)\" data-toc-modified-id=\"START-STUDENT-CODE-HW34-(INSERT-CELLS-BELOW-AS-NEEDED)-3501\"><span class=\"toc-item-num\">3.5.0.1&nbsp;&nbsp;</span>START STUDENT CODE HW34 (INSERT CELLS BELOW AS NEEDED)</a></div><div class=\"lev4 toc-item\"><a href=\"#END-STUDENT-CODE-HW34\" data-toc-modified-id=\"END-STUDENT-CODE-HW34-3502\"><span class=\"toc-item-num\">3.5.0.2&nbsp;&nbsp;</span>END STUDENT CODE HW34</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.5:-Stripes\" data-toc-modified-id=\"HW3.5:-Stripes-36\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>HW3.5: Stripes</a></div><div class=\"lev4 toc-item\"><a href=\"#START-STUDENT-CODE-HW35-(INSERT-CELLS-BELOW-AS-NEEDED)\" data-toc-modified-id=\"START-STUDENT-CODE-HW35-(INSERT-CELLS-BELOW-AS-NEEDED)-3601\"><span class=\"toc-item-num\">3.6.0.1&nbsp;&nbsp;</span>START STUDENT CODE HW35 (INSERT CELLS BELOW AS NEEDED)</a></div><div class=\"lev4 toc-item\"><a href=\"#END-STUDENT-CODE-HW35\" data-toc-modified-id=\"END-STUDENT-CODE-HW35-3602\"><span class=\"toc-item-num\">3.6.0.2&nbsp;&nbsp;</span>END STUDENT CODE HW35</a></div><div class=\"lev1 toc-item\"><a href=\"#OPTIONAL\" data-toc-modified-id=\"OPTIONAL-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>OPTIONAL</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.6-Computing-Relative-Frequencies-on-100K-WikiPedia-pages-(93Meg)\" data-toc-modified-id=\"HW3.6-Computing-Relative-Frequencies-on-100K-WikiPedia-pages-(93Meg)-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>HW3.6 Computing Relative Frequencies on 100K WikiPedia pages (93Meg)</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.7-Apriori-Algorithm\" data-toc-modified-id=\"HW3.7-Apriori-Algorithm-42\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>HW3.7 Apriori Algorithm</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.8.-Shopping-Cart-Analysis\" data-toc-modified-id=\"HW3.8.-Shopping-Cart-Analysis-43\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>HW3.8. Shopping Cart Analysis</a></div><div class=\"lev2 toc-item\"><a href=\"#HW3.8.1\" data-toc-modified-id=\"HW3.8.1-44\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>HW3.8.1</a></div><div class=\"lev1 toc-item\"><a href=\"#END-OF-HOMEWORK\" data-toc-modified-id=\"END-OF-HOMEWORK-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>END OF HOMEWORK</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/**********************************************************************************************\n",
       "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
       "https://github.com/mathjax/MathJax/issues/1300\n",
       "A quick hack to fix this based on stackoverflow discussions: \n",
       "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
       "**********************************************************************************************/\n",
       "\n",
       "$('.math>span').css(\"border-left-color\",\"transparent\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "/**********************************************************************************************\n",
    "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
    "https://github.com/mathjax/MathJax/issues/1300\n",
    "A quick hack to fix this based on stackoverflow discussions: \n",
    "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
    "**********************************************************************************************/\n",
    "\n",
    "$('.math>span').css(\"border-left-color\",\"transparent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW3\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  Nilesh Bhoyat \n",
    "__Class:__ MIDS w261 Summer Group 2     \n",
    "__Email:__  *Your UC Berkeley Email Goes Here*@iSchool.Berkeley.edu     \n",
    "__StudentId__  26302327    __End of StudentId__     \n",
    "__Week:__   3\n",
    "\n",
    "__NOTE:__ please replace `1234567` with your student id above      \n",
    "__Due Time:__ HW is due the Tuesday of the following week by 8AM (West coast time). I.e., Tuesday, Jan 31, 2017 in the case of this homework. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a name=\"TOC\"></a> \n",
    "\n",
    "1.  [HW Intructions](#1)   \n",
    "2.  [HW Problems](#2)   \n",
    "    3.1.   [HW3.1](#3.1)     \n",
    "    3.2.   [HW3.2](#3.2)   \n",
    "    3.3  [HW3.3](#3.3)   \n",
    "    3.4  [HW3.4](#3.4)   \n",
    "    3.5  [HW3.5](#3.5)   \n",
    "    3.6  [HW3.6](#3.6)   \n",
    "    3.7  [HW3.7](#3.7)   \n",
    "    3.8  [HW3.8](#3.8)   \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "# 1 Instructions\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "DATSCIW261 ASSIGNMENT #3\n",
    "\n",
    "Version 2017-26-1 \n",
    "\n",
    "\n",
    "### IMPORTANT\n",
    "\n",
    "This homework can be completed locally on your computer \n",
    "\n",
    "### === INSTRUCTIONS for SUBMISSIONS ===   \n",
    "Follow the instructions for submissions carefully.\n",
    "\n",
    "__<span style=\"color:red\">NEW:</span> Going forward, each student will have a `HW-<user>` repository for all assignments.__\n",
    "\n",
    "Click this link to enable you to create a github repo within the MIDS261 Classroom:   \n",
    "https://classroom.github.com/assignment-invitations/3b1d6c8e58351209f9dd865537111ff8   \n",
    "and follow the instructions to create a HW repo.\n",
    "\n",
    "Push the following to your HW github repo into the master branch:\n",
    "* Your local HW3 directory. Your repo file structure should look like this:\n",
    "\n",
    "```\n",
    "HW-<user>\n",
    "    --HW3\n",
    "       |__MIDS-W261-HW-03-<Student_id>.ipnb\n",
    "       |__MIDS-W261-HW-03-<Student_id>.pdf\n",
    "       |__some other hw3 file\n",
    "    --HW4\n",
    "       |__MIDS-W261-HW-04-<Student_id>.ipnb\n",
    "       |__MIDS-W261-HW-04-<Student_id>.pdf\n",
    "       |__some other hw4 file\n",
    "    etc..\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## HW3.0.\n",
    "1. How do you merge  two sorted  lists/arrays of records of the form [key, value]?\n",
    "1. Where is this  used in Hadoop MapReduce? [Hint within the shuffle]\n",
    "1. What is  a combiner function in the context of Hadoop? \n",
    "1. Give an example where it can be used and justify why it should be used in the context of this problem.\n",
    "1. What is the Hadoop shuffle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do you merge two sorted lists/arrays of records of the form [key, value]?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Merge sort is  divide and conquer algorithm. It is a very efficient sort algorithm. The algorithm gets is named from the fact that it divides the collection in half, recursively sorts each half, and then merges the two sorted halves back together. Each half of the collection is repeatedly halved until there is only one object in the half, at which point it is sorted by definition. As each sorted half is merged, the algorithm compares the objects to determine where to place each sub set.\n",
    "    \n",
    "    for records in format [key,value], operations happen on keys. \n",
    "    Pseudo Code\n",
    "    \n",
    "    \n",
    "    Union of keys from both dictionaries.\n",
    "    Loop for each key in union:\n",
    "      if key is in dict1 and not in dict 2 then\n",
    "       add key records in result dict\n",
    "      if key is in dict2 and not in dict 1 then\n",
    "      add key record in result dict\n",
    "      if key is present in both dicts then\n",
    "      union of result with same key\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bookA': 1, 'bookB': 2, 'bookC': (3, 2), 'bookD': 4, 'bookE': 5}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "dict1 = {'bookA': 1, 'bookB': 2, 'bookC': 3}\n",
    "dict2 = {'bookC': 2, 'bookD': 4, 'bookE': 5}\n",
    "\n",
    "def union_collections(d1, d2):\n",
    "    union = {}\n",
    "\n",
    "    for key in set(d1.keys()).union(d2.keys()):\n",
    "        if key in d1 and key not in d2: \n",
    "            union[key] = d1[key]\n",
    "\n",
    "        if key in d2 and key not in d1: \n",
    "            union[key] = d2[key]\n",
    "\n",
    "        if key in d1 and key in d2:\n",
    "            union[key] = (d1[key] ,d2[key])\n",
    "\n",
    "    return union\n",
    "union_collections(dict1, dict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where is this used in Hadoop MapReduce? [Hint within the shuffle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jobUI](shufflesort.png)\n",
    "\n",
    "\n",
    "    Answer:\n",
    "    As shown in figure, key-value pairs are merge-sort is used in shuffle-sort phase.\n",
    "    Hadoop sorts the key-value pairs by key and it “shuffles” all pairs with the same key to the same Reducer. There are several possible techniques that can be used to decide which reducer gets which range of keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a combiner function in the context of Hadoop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Mappers produce a lot of intermediate data that must be sent over the network to be shuffled, sorted, and reduced. Because networking is a physical resource, large amounts of transmitted data can lead to job delays and memory bottlenecks (e.g., there is too much data for the reducer to hold into memory). Combiners are the primary mechanism to solve this problem, and are essentially intermediate reducers that are associated with the mapper output. Combiners reduce network traffic by performing a mapper-local reduction of the data before forwarding it on to the appropriate reducer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Give an example where it can be used and justify why it should be used in the context of this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Answer\n",
    "     \n",
    "     Consider an example of wordcount for large corpora. When mulitple mappers are producing word count as below\n",
    "     \n",
    "     mapper 1 output\n",
    "     (a,10) (the,10) (tent,1) (the,20)(a,20)\n",
    "     Mapper 2 output\n",
    "     \n",
    "     (a,30) (the,10)\n",
    "     \n",
    "     Intended Sum Reduce is \n",
    "     (a,60) (the,40)(tent,1)\n",
    "     \n",
    "     Each mapper is emitting extra work for the reducer, namely in the duplication of the different keys coming from each mapper. Combiner can can reduce such duplicate key by aggreating records before and there by reduce network traffic. This will also reduce overall shuffle phase time.\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the Hadoop shuffle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Answer \n",
    "         The process by which the hadoop performs the sort—and transfers the map outputs to the reducers as inputs.\n",
    "         If reducer has to take all output of mapper in as-is format i.e. (word,1) then it would be very slow process.\n",
    "         Shuffle does this\n",
    "         Shuffle is all of this!:\n",
    "        1. partition, sort, combine - Partitions records and does partial sort for each partition.\n",
    "        2. mergesort\n",
    "        3. Send to reducer\n",
    "        4. Merge sort\n",
    "        5. Stream to reducer\n",
    "  ![jobUI](shufflephase.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.1\"></a>\n",
    "## HW3.1 consumer complaints dataset: Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "Use the Consumer Complaints  Dataset provide here to complete this question:\n",
    "\n",
    "\n",
    "     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    "The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "Here’s is the first few lines of the  of the Consumer Complaints  Dataset:\n",
    "```\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
    "1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
    "1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "```\n",
    "User-defined Counters\n",
    "\n",
    "Now, let’s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-05-30 02:58:39--  https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv\n",
      "Resolving www.dropbox.com... 162.125.4.1\n",
      "Connecting to www.dropbox.com|162.125.4.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://dl.dropboxusercontent.com/content_link/EdIc9ZgdjBFleXSS6b4OEYZFdzlso4UoR3eObApdH8KXXNIFjTYwYjtdVUwkCYEo/file [following]\n",
      "--2017-05-30 02:58:40--  https://dl.dropboxusercontent.com/content_link/EdIc9ZgdjBFleXSS6b4OEYZFdzlso4UoR3eObApdH8KXXNIFjTYwYjtdVUwkCYEo/file\n",
      "Resolving dl.dropboxusercontent.com... 162.125.4.6\n",
      "Connecting to dl.dropboxusercontent.com|162.125.4.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50906486 (49M) [text/csv]\n",
      "Saving to: `Consumer_Complaints.csv'\n",
      "\n",
      " 0% [                                       ] 0           --.-K/s              "
     ]
    }
   ],
   "source": [
    "# Put the data into HDFS\n",
    "!wget 'https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create HDFS directories\n",
    "!hdfs dfs -mkdir -p /user/nibhoyar\n",
    "!hdfs dfs -put Consumer_Complaints.csv /user/nibhoyar\n",
    "!hdfs dfs -rm Consumer_Complaints.csv \n",
    "!hdfs dfs -copyFromLocal Consumer_Complaints.csv \n",
    "!hdfs dfs -rm -r hw3.1-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile complaintCountsMapper.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW31MAPPER\n",
    "import sys\n",
    "separator = ','\n",
    "for line in (sys.stdin):\n",
    "        fields = line.split(separator)\n",
    "        if 'Complaint ID' != fields[0] :\n",
    "           \n",
    "            # we have a real record, so do some mapping\n",
    "            counter_name = None\n",
    "            if (fields[1].lower() == 'debt collection' or \\\n",
    "                fields[1].lower() == 'mortgage'):\n",
    "                counter_name = fields[1].strip().lower()\n",
    "            else:\n",
    "                counter_name = 'other'\n",
    "            # update the counter\n",
    "            sys.stderr.write(\"reporter:counter:Category Counters,{0},1\\n\".format(counter_name))\n",
    "       \n",
    "\n",
    "# END STUDENT CODE HW31MAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chomd a+x complaintCountsMapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile complaintCountsReducer.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW31REDUCER\n",
    "\n",
    "# END STUDENT CODE HW31REDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW31HADOOP\n",
    "!hdfs dfs -rm -r hw3.1-output\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapred.reduce.tasks=0 \\\n",
    "    -files complaintCountsMapper.py\\\n",
    "    -mapper complaintCountsMapper.py \\\n",
    "    -reducer org.apache.hadoop.mapred.lib.IdentityReducer\\\n",
    "    -input Consumer_Complaints.csv \\\n",
    "    -output  hw3.1-output\n",
    "# END STUDENT CODE HW31HADOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#output\n",
    "\n",
    "![jobUI](hw31.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.2\"></a>\n",
    "### HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "\n",
    "For this brief study the Input file will be one record (the next line only):    \n",
    "`foo foo quux labs foo bar quux`\n",
    "\n",
    "\n",
    "__3.2.A__     \n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many times the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.\n",
    "\n",
    "__3.2.B__   \n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "\n",
    "__3.2.C__     \n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "\n",
    "Using a single reducer: \n",
    "- What are the top 50 most frequent terms in your word count analysis?    \n",
    "- Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order.    \n",
    "- Present bottom 10 tokens (least frequent items). \n",
    "\n",
    "__NOTE:__ You can use: `WORD_RE = re.compile(r\"[\\w']+\")` to tokenize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.A SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mapper3.2.A.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32AMAPPER\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "for line in sys.stdin:\n",
    "    for word  in [s.lower() for s in WORD_RE.findall(line)]:\n",
    "        print '%s\\t%s' % (word, 1)\n",
    "\n",
    "# END STUDENT CODE HW32AMAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile reducer3.2.A.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32AREDUCER\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)\n",
    "\n",
    "# END STUDENT CODE HW32AREDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\"|python mapper3.2.A.py|python reducer3.2.A.py|sort -k2,2n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper3.2.A.py\n",
    "!chmod a+x reducer3.2.A.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW32AHADOOP\n",
    "\n",
    "!echo \"foo foo quux labs foo bar quux\" >foofoo.txt\n",
    "!hdfs dfs -copyFromLocal foofoo.txt \n",
    "!hdfs dfs -rm -r hw3.2.A-output\n",
    "\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D stream.map.output.field.separator=\"\\t\" \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1nr -k2,2\" \\\n",
    "    -files mapper3.2.A.py,reducer3.2.A.py\\\n",
    "    -mapper mapper3.2.A.py \\\n",
    "    -reducer reducer3.2.A.py\\\n",
    "    -input foofoo.txt \\\n",
    "    -output  hw3.2.A-output\n",
    "\n",
    "# END STUDENT CODE HW32AHADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#HDFS Output\n",
    "!hdfs dfs -ls hw3.2.A-output\n",
    "!hdfs dfs -cat hw3.2.A-output/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### INSERT SCREENSHOT OF JOB TRACKER UI COUNTERS\n",
    "\n",
    "![jobUI](hw3mapcount.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.2.A EXPLANATION\n",
    "With default setting , MapReduce selected to partition records into two maps so mapper is called 2 times. And then results are sent to single reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.B SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mapper3.2.B.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32BMAPPER\n",
    "from __future__ import division\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "separator = ','\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "numReducers = int(os.environ.get('NUM_PARTITIONS', '4')) \n",
    "\n",
    "\n",
    "def makeKey(word,n):\n",
    "  divisor = 26/n\n",
    "  return int(math.ceil((ord(word[0])-96)/divisor))\n",
    "\n",
    "#loop through each records\n",
    "for line in (sys.stdin):\n",
    "#get 3rd column\n",
    "        fields = line.split(separator)\n",
    "        if 'Complaint ID' != fields[0] :\n",
    "           \n",
    "            # we have a real record, so do some mapping\n",
    "            counter_name = None\n",
    "            for word  in [s.lower() for s in WORD_RE.findall(fields[3])]:\n",
    "                key = makeKey(word,numReducers)\n",
    "                print '%s\\t%s\\t%s' % (key,word, 1)\n",
    "            \n",
    "\n",
    "# END STUDENT CODE HW32BMAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile reducer3.2.B.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32BREDUCER\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    partkey,key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)\n",
    "# END STUDENT CODE HW32BREDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper3.2.B.py\n",
    "!chmod a+x reducer3.2.B.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#unit test\n",
    "!head -10 Consumer_Complaints.csv|python mapper3.2.B.py|sort -k1,1|python reducer3.2.B.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW32BHADOOP\n",
    "!hdfs dfs -rm -r hw3.2.B-output\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.maps=2 \\\n",
    "    -D mapreduce.job.reduces=2\\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D stream.map.output.field.separator=\"\\t\" \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1nr -k2,2\" \\\n",
    "    -files mapper3.2.B.py,reducer3.2.B.py\\\n",
    "    -mapper mapper3.2.B.py \\\n",
    "    -reducer reducer3.2.B.py\\\n",
    "    -input  Consumer_Complaints.csv \\\n",
    "    -cmdenv NUM_PARTITIONS=2\\\n",
    "    -output  hw3.2.B-output\n",
    "\n",
    "# END STUDENT CODE HW32BHADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3.2.B OUTPUT/ANSWER\n",
    "!hdfs dfs -ls hw3.2.B-output\n",
    "!echo \"________Output_________\"\n",
    "!hdfs dfs -cat hw3.2.B-output/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### INSERT SCREENSHOT OF JOB TRACKER UI COUNTERS\n",
    "![logs](32c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.C SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mapper3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CMAPPER\n",
    "from __future__ import division\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "separator = ','\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "#numReducers = int(os.environ.get('NUM_PARTITIONS', '4')) \n",
    "\n",
    "total = 0\n",
    "\n",
    "def makeKey(word,n):\n",
    "  divisor = 26/n\n",
    "  return int(math.ceil((ord(word[0])-96)/divisor))\n",
    "\n",
    "#loop through each records\n",
    "for line in (sys.stdin):\n",
    "#get 3rd column\n",
    "        fields = line.split(separator)\n",
    "        if 'Complaint ID' != fields[0] :\n",
    "           \n",
    "            # we have a real record, so do some mapping\n",
    "            counter_name = None\n",
    "            for word  in [s.lower() for s in WORD_RE.findall(fields[3])]:\n",
    "                #key = makeKey(word,numReducers)\n",
    "                print '%s\\t%s' % (word, 1)\n",
    "                total = total + 1\n",
    "print '%s\\t%s' % (\"*total\", total)\n",
    "            \n",
    "\n",
    "\n",
    "# END STUDENT CODE HW32CMAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile combiner3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CCOMBINER\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Combiner Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)\n",
    "# END STUDENT CODE HW32CCOMBINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile reducer3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CREDUCER\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)\n",
    "# END STUDENT CODE HW32CREDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper3.2.C.py\n",
    "!chmod a+x reducer3.2.C.py\n",
    "!chmod a+x combiner3.2.C.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#unit Testing\n",
    "!head -10 Consumer_Complaints.csv|python mapper3.2.C.py|sort -k1,1|python combiner3.2.C.py|python reducer3.2.C.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW32CHADOOP\n",
    "!hdfs dfs -rm -r hw3.2.C-output\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -files mapper3.2.C.py,reducer3.2.C.py,combiner3.2.C.py\\\n",
    "    -mapper mapper3.2.C.py \\\n",
    "    -reducer reducer3.2.C.py\\\n",
    "    -combiner combiner3.2.C.py\\\n",
    "    -input  Consumer_Complaints.csv \\\n",
    "    -output  hw3.2.C-output \\\n",
    "    -numReduceTasks 4\n",
    "\n",
    "# END STUDENT CODE HW32CHADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3.2.C OUTPUT/ANSWER\n",
    "!hdfs dfs -ls hw3.2.C-output\n",
    "!echo \"________Output_________\"\n",
    "!hdfs dfs -cat hw3.2.C-output/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### INSERT SCREENSHOT OF JOB TRACKER UI COUNTERS\n",
    "![logs](32d.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile frequencies_mapper3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CFREQMAPPER\n",
    "from __future__ import division\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "separator = ','\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "#loop through each records\n",
    "for line in sys.stdin:\n",
    "    print line.strip()\n",
    "            \n",
    "\n",
    "\n",
    "# END STUDENT CODE HW32CFREQMAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile frequencies_reducer3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CFREQREDUCER\n",
    "import sys\n",
    "\n",
    "# Initialize variables\n",
    "total = 0\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "   \n",
    "    fields = line.replace('\\n','').split('\\t')\n",
    "    count = fields[1]\n",
    "    word = fields[0]\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    if word == '*total': \n",
    "        total =  total + float(count)\n",
    "    else: \n",
    "        print '%s\\t%s\\t%2.3f' % (word, count, float(count)/total)  \n",
    "        #print \"{0:20}\\t{1:10}\\t{2}\\n\".format(word, count, float(count)/total) \n",
    "# END STUDENT CODE HW32CFREQREDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW32CFREQHADOOP\n",
    "!hdfs dfs -rm -r hw3.2.D-output\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D stream.num.map.output.key.fields=4 \\\n",
    "    -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "    -D mapreduce.job.reduces=1 \\\n",
    "    -files frequencies_mapper3.2.C.py,frequencies_reducer3.2.C.py \\\n",
    "    -mapper frequencies_mapper3.2.C.py \\\n",
    "    -reducer frequencies_reducer3.2.C.py \\\n",
    "    -input hw3.2.C-output \\\n",
    "    -output hw3.2.D-output \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner\n",
    "# END STUDENT CODE HW32CFREQHADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3.2.C OUTPUT/ANSWER\n",
    "!hdfs dfs -ls hw3.2.D-output\n",
    "!echo \"________Output_________\"\n",
    "\n",
    "!hdfs dfs -cat hw3.2.D-output/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the top 50 most frequent terms in your word count analysis?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat hw3.2.D-output/part-0000* | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat hw3.2.D-output/part-0000* | sort -k2,2nr |head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Present bottom 10 tokens (least frequent items)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -tail  hw3.2.D-output/part-00000 > hw3.2.D.txt\n",
    "!tail -10 hw3.2.D.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.2.1\"></a>\n",
    "### 3.2.1  \n",
    "Using **2 reducers**: What are the top **50 most frequent terms** in your word count analysis? \n",
    "\n",
    "Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). Please **use a combiner.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### START STUDENT CODE HW321 (INSERT CELLS BELOW AS NEEDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile frequencies_reducer3.2.1.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import os\n",
    "# Initialize variables\n",
    "total = 0\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "totalrecs = int(os.environ.get('TOTAL_RECS', '980482')) \n",
    "for line in sys.stdin:\n",
    "   \n",
    "    fields = line.replace('\\n','').split('\\t')\n",
    "    count = fields[1]\n",
    "    word = fields[0]\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    if word == '*total': #not required in multireducers\n",
    "        total =  total + float(count)\n",
    "    else: \n",
    "        print '%s\\t%s\\t%2.3f' % (word, count, float(count)/totalrecs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile frequencies_combine3.2.1.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import os\n",
    "# Initialize variables\n",
    "total = 0\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:combiner Counters,Calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "   \n",
    "    fields = line.replace('\\n','').split('\\t')\n",
    "    count = fields[1]\n",
    "    word = fields[0]\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    if word == '*total': \n",
    "        total =  total + float(count)\n",
    "        os.environ['TOTAL_RECS']= str( total) #setting environment variable so that next calculations use this\n",
    "    else: \n",
    "        print '%s\\t%s\\t%2.3f' % (word, count, count)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x frequencies_reducer3.2.1.py\n",
    "!chmod a+x frequencies_combine3.2.1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#unit Testing\n",
    "!hdfs dfs -cat hw3.2.C-output/part-0000* | python  frequencies_mapper3.2.C.py|python frequencies_reducer3.2.1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END STUDENT CODE HW321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#start 3.2.1\n",
    "\n",
    "!hdfs dfs -rm -r hw3.2.1-output\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D stream.num.map.output.key.fields=4 \\\n",
    "    -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "    -D mapreduce.job.reduces=2 \\\n",
    "    -files frequencies_mapper3.2.C.py,frequencies_reducer3.2.1.py,frequencies_combine3.2.1.py \\\n",
    "    -mapper frequencies_mapper3.2.C.py \\\n",
    "    -reducer frequencies_reducer3.2.1.py\\\n",
    "    -combiner frequencies_combine3.2.1.py\\\n",
    "    -input hw3.2.C-output \\\n",
    "    -output hw3.2.1-output \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls hw3.2.1-output\n",
    "!hdfs dfs -cat hw3.2.1-output/part-0000* | sort -k2,2nr > hw3.2.1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -50 hw3.2.1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.3\"></a>\n",
    "## HW3.3. Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\t\n",
    "For this homework use the online browsing behavior dataset located at: \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \n",
    "\n",
    "\n",
    "Do some exploratory data analysis of this dataset guided by the following questions:. \n",
    "\n",
    "How many unique items are available from this supplier?\n",
    "\n",
    "Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### START STUDENT CODE HW33 (INSERT CELLS BELOW AS NEEDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get data first\n",
    "!wget \"https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\"\n",
    "!mv ProductPurchaseData.txt?dl=0 ProductPurchaseData.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -copyFromLocal ProductPurchaseData.txt \n",
    "!hdfs dfs -rm -r hw3.3-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mapper_33.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "# Initialsys.stderr.write(ize variables\n",
    "total = 0\n",
    "basket_size = 0\n",
    "largest_basket_size = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    total = 0\n",
    "    basket_size = 0\n",
    "    # Split our line into products\n",
    "    for product in line.replace('\\n','').split():\n",
    "        print '%s\\t%s' % (product, 1)\n",
    "        #print generateLongCountToken(product)\n",
    "        basket_size += 1\n",
    "        total += 1\n",
    "\n",
    "        \n",
    "    \n",
    "    print '%s\\t%s' % ('*largest_basket', total)\n",
    "    #basket_size = 0\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper_33.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile reducer33.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32AREDUCER\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "dictcounts = {}\n",
    "largest = []\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "  \n",
    "    if key != '*largest_basket':    \n",
    "        if key == cur_key:\n",
    "            cur_count += int(value)\n",
    "        else:\n",
    "            if cur_key:\n",
    "                dictcounts[cur_key] = cur_count\n",
    "            #print '%s\\t%s' % (cur_key, cur_count)\n",
    "            cur_key = key\n",
    "            cur_count = int(value)\n",
    "    else:\n",
    "        if key == '*largest_basket':\n",
    "            largest.append(int(value))\n",
    "print \"************************Output*****************************\"\n",
    "print \"Maximum length of Bucket %d\"%(max(largest))\n",
    "\n",
    "print \"Total No of Unique products %d\"% len(dictcounts.keys())\n",
    "totals = sum(dictcounts.values())\n",
    "dictcounts =OrderedDict(sorted(dictcounts.items(), key=lambda t: t[1], reverse=True))\n",
    "count  = 0\n",
    "print \"*****Top 50 Products*************\"\n",
    "for key in dictcounts:\n",
    "    if count <= 49:\n",
    "        print '%s\\t%d\\t%2.3f' %(key,dictcounts[key] ,float(dictcounts[key])/totals )\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat ProductPurchaseData.txt |python mapper_33.py|sort -k1,1 |python reducer33.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r hw3.3-output\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapred.reduce.tasks=1 \\\n",
    "    -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D stream.map.output.field.separator=\"\\t\" \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1\" \\\n",
    "    -files mapper_33.py,reducer33.py\\\n",
    "    -mapper mapper_33.py \\\n",
    "    -reducer reducer33.py\\\n",
    "    -input ProductPurchaseData.txt \\\n",
    "    -output  hw3.3-output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls hw3.3-output\n",
    "!hdfs dfs -cat hw3.3-output/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END STUDENT CODE HW33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.3.1\"></a>\n",
    "## HW3.3.1 OPTIONAL \n",
    "Using 2 reducers:  Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### START STUDENT CODE HW331 (INSERT CELLS BELOW AS NEEDED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END STUDENT CODE HW331"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.4\"></a>\n",
    "## HW3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2. \n",
    "\n",
    "Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.  \n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    "      item1, item2, support count, support\n",
    "\n",
    "\n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right), \n",
    "and break ties in support (between pairs, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### START STUDENT CODE HW34 (INSERT CELLS BELOW AS NEEDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mapper_34.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "\n",
    "import sys\n",
    "from itertools import  combinations\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "total = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Split our line into products\n",
    "    products = line.replace('\\n','').split()\n",
    "    \n",
    "    # Get all combinations of products:\n",
    "    #  - Use a set to remove duplicate products\n",
    "    #  - Combinations finds tuples of length 2 with no repeats\n",
    "    for pair in combinations(sorted(set(products)), 2):\n",
    "                print '%s\\t%s\\t%s' % (pair[0], pair[1], 1)\n",
    "    \n",
    "    total += 1\n",
    "# Print total words\n",
    "print '%s\\t%s\\t%s' % ('*total', '*total', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper_34.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#unit test\n",
    "!hdfs dfs -cat ProductPurchaseData.txt |head -1|python mapper_34.py|sort -k1,1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile combiner34.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CCOMBINER\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "mydict = {}\n",
    "sys.stderr.write(\"reporter:counter:Combiner Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    key1,key2, value = line.split()\n",
    "    key = (key1,key2)\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key and cur_count >=100:\n",
    "            print '%s\\t%s\\t%s' % (cur_key[0],cur_key[1], cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s\\t%s'% (cur_key[0],cur_key[1], cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x combiner34.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile reducer34.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32AREDUCER\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "dictcounts = {}\n",
    "largest = []\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    key1,key2, value = line.split()\n",
    "    key = (key1,key2)\n",
    "    if key1 != '*total':    \n",
    "        if key == cur_key:\n",
    "            cur_count += int(value)\n",
    "        else:\n",
    "            if cur_key:\n",
    "                dictcounts[cur_key] = cur_count\n",
    "            #print '%s\\t%s' % (cur_key, cur_count)\n",
    "            cur_key = key\n",
    "            cur_count = int(value)\n",
    "    else:\n",
    "        if key1 == '*total':\n",
    "            largest.append(int(value))\n",
    "\n",
    "totals = sum(largest)\n",
    "dictcounts =OrderedDict(sorted(dictcounts.items(), key=lambda t: t[1], reverse=True))\n",
    "count  = 0\n",
    "print \"*****Top 50 Products*************\"\n",
    "for key in dictcounts:\n",
    "    if count <= 49:\n",
    "        print '%s\\t%s\\t%d\\t%2.3f' %(key[0],key[1],dictcounts[key] ,float(dictcounts[key])/totals )\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer34.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hadoop call\n",
    "!hdfs dfs -rm -r hw3.4-output\n",
    "!time hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D stream.num.map.output.key.fields=4 \\\n",
    "    -D mapreduce.job.reduces=1 \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2\" \\\n",
    "    -files mapper_34.py,combiner34.py,reducer34.py\\\n",
    "    -mapper mapper_34.py\\\n",
    "    -reducer reducer34.py\\\n",
    "    -input  ProductPurchaseData.txt \\\n",
    "    -output  hw3.4-output \\\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print output\n",
    "!hdfs dfs -ls hw3.4-output\n",
    "!hdfs dfs -cat hw3.4-output/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END STUDENT CODE HW34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.5\"></a>\n",
    "## HW3.5: Stripes\n",
    "Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.\n",
    "\n",
    "Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### START STUDENT CODE HW35 (INSERT CELLS BELOW AS NEEDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mapper_35.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "\n",
    "import sys\n",
    "from itertools import  combinations\n",
    "#import collections\n",
    "\n",
    "# Increment mapper counter\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "total = 0\n",
    "\n",
    "# Our input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Split our line into products\n",
    "    products = line.replace('\\n','').split()\n",
    "    \n",
    "    # Get all combinations of products:\n",
    "    #  - Use a set to remove duplicate products\n",
    "    #  - Combinations finds tuples of length 2 with no repeats\n",
    "   \n",
    "    for i, term in enumerate(products):\n",
    "                # Create a new stripe for each term\n",
    "                stripe = {}\n",
    "\n",
    "                for j, token in enumerate(products):\n",
    "                    # Don't count the term's co-occurrence with itself\n",
    "                    if i != j:\n",
    "                        x = stripe.get(token,None)\n",
    "                        if x == None:\n",
    "                            stripe[token] = 1\n",
    "                        else:\n",
    "                            stripe[token] += 1\n",
    "\n",
    "                # Emit the term and the stripe\n",
    "                print '%s\\t%s' % (term, stripe)\n",
    "# Increment total number of baskets\n",
    "    total += 1           \n",
    "stripe = {}\n",
    "stripe['*total'] = total\n",
    "print '%s\\t%s' % ('*total', stripe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile reducer35.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32AREDUCER\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "#from collections import collections\n",
    "#import collections\n",
    "prev_key = None\n",
    "cur_count = 0\n",
    "prev_stripe = {}\n",
    "largest = []\n",
    "dictcounts = {}\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    fields = line.replace('\\n','').split('\\t')\n",
    "    key = fields[0]\n",
    "    \n",
    "    stripe = eval(fields[1])\n",
    "    \n",
    "    \n",
    "    if prev_key == key:\n",
    "        # We need to move through the dictionary and update counts\n",
    "        for item in stripe:\n",
    "            if item in prev_stripe:\n",
    "                prev_stripe[item] += stripe[item]\n",
    "            else:\n",
    "                prev_stripe[item] = stripe[item]\n",
    "        \n",
    "    else:\n",
    "        if len(prev_stripe) > 0:\n",
    "            # We are at a new pair, need to print previous pair sum\n",
    "            #print '%s\\t%s' % (prev_key, prev_stripe)\n",
    "            for word in prev_stripe:\n",
    "                dictcounts[(prev_key,word)] = prev_stripe[word]\n",
    "        prev_stripe = stripe\n",
    "        prev_key = key\n",
    "\n",
    "# Output the last line\n",
    "if prev_stripe == stripe:\n",
    "    for word in prev_stripe:\n",
    "        dictcounts[(prev_key,word)] = prev_stripe[word]\n",
    "totals = dictcounts[('*total','*total')]\n",
    "dictcounts =OrderedDict(sorted(dictcounts.items(), key=lambda t: t[1], reverse=True))\n",
    "count  = 0\n",
    "print \"*****Top 50 Products*************\"\n",
    "for key in dictcounts:\n",
    "    if count <= 49:\n",
    "        print '%s\\t%s\\t%d\\t%2.3f' %(key[0],key[1],dictcounts[key] ,float(dictcounts[key])/totals )\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper_35.py\n",
    "!chmod a+x reducer35.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat ProductPurchaseData.txt |head -10|python mapper_35.py|sort -k1,1|python reducer35.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hadoop call\n",
    "!hdfs dfs -rm -r hw3.5-output\n",
    "!time hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D stream.num.map.output.key.fields=4 \\\n",
    "    -D mapreduce.job.reduces=1 \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2\" \\\n",
    "    -files mapper_35.py,combiner34.py,reducer35.py\\\n",
    "    -mapper mapper_35.py\\\n",
    "    -reducer reducer35.py\\\n",
    "    -input  ProductPurchaseData.txt \\\n",
    "    -output  hw3.5-output \\\n",
    "    -cmdenv PATH=/opt/anaconda/bin:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print output\n",
    "!hdfs dfs -ls hw3.5-output\n",
    "!hdfs dfs -cat hw3.5-output/part-0000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat /proc/cpuinfo | grep processor | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat /proc/meminfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Answer\n",
    "#### System Setup\n",
    "Single Computer , docker Container, 2 Cores and 5GB RAM.\n",
    "#### How many times is each mapper and reducer called?\n",
    "Mapper 2\n",
    "Reducer 1\n",
    "\n",
    "#### Total time\n",
    "\n",
    "\n",
    "##### With Pairs\n",
    "    real\t1m6.364s\n",
    "    user\t0m4.720s\n",
    "    sys\t0m1.240s\n",
    " \n",
    "\t\tLaunched map tasks=2\n",
    "\t\tLaunched reduce tasks=1\n",
    "\t\tData-local map tasks=2\n",
    "#### \t\tTotal time spent by all maps in occupied slots (ms)=39445\n",
    "\t\tTotal time spent by all reduces in occupied slots (ms)=27979\n",
    "\t\tTotal time spent by all map tasks (ms)=39445\n",
    "\t\tTotal time spent by all reduce tasks (ms)=27979\n",
    "\t\tTotal vcore-seconds taken by all map tasks=39445\n",
    "\t\tTotal vcore-seconds taken by all reduce tasks=27979\n",
    "\t\tTotal megabyte-seconds taken by all map tasks=40391680\n",
    "\t\tTotal megabyte-seconds taken by all reduce tasks=28650496\n",
    "        \n",
    "\n",
    "##### With Stripes\n",
    "    real\t1m11.730s\n",
    "    user\t0m5.310s\n",
    "    sys\t0m1.310s\n",
    "\n",
    "    Launched map tasks=2\n",
    "\t\tLaunched reduce tasks=1\n",
    "\t\tData-local map tasks=2\n",
    "#### \t\tTotal time spent by all maps in occupied slots (ms)=26607\n",
    "\t\tTotal time spent by all reduces in occupied slots (ms)=39085\n",
    "\t\tTotal time spent by all map tasks (ms)=26607\n",
    "\t\tTotal time spent by all reduce tasks (ms)=39085\n",
    "\t\tTotal vcore-seconds taken by all map tasks=26607\n",
    "\t\tTotal vcore-seconds taken by all reduce tasks=39085\n",
    "\t\tTotal megabyte-seconds taken by all map tasks=27245568\n",
    "\t\tTotal megabyte-seconds taken by all reduce tasks=40023040\n",
    "\n",
    "\n",
    "### As expected Mappers took much less time with Stripes compared to pairs as expected but in reducers Pairs took bit longer.  This could be due to all unpacking we have to do with stripes to calculate final count.\n",
    "\n",
    "### Total time taken by stripes is more than pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END STUDENT CODE HW35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL\n",
    "QUESTIONS  BELOW THIS LINE ARE OPTIONAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.6\"></a>\n",
    "## HW3.6 Computing Relative Frequencies on 100K WikiPedia pages (93Meg)\n",
    "\n",
    "Dataset description\n",
    "For this assignment you will explore a set of 100,000 Wikipedia documents:\n",
    "\n",
    "https://www.dropbox.com/s/n5lfbnztclo93ej/wikitext_100k.txt?dl=0\n",
    "s3://cs9223/wikitext_100k.txt, or\n",
    "https://s3.amazonaws.com/cs9223/wikitext_100k.txt\n",
    "Each line in this file consists of the plain text extracted from a Wikipedia document.\n",
    "\n",
    "Task\n",
    "Compute the relative frequencies of each word that occurs in the documents in wikitext_100k.txt and output the top 100 word pairs sorted by decreasing order of relative frequency.\n",
    "\n",
    "Recall that the relative frequency (RF) of word B given word A is defined as follows:\n",
    "\n",
    "   f(B|A) = Count(A, B) / Count (A)   =  Count(A, B) / sum_B'(Count (A, B')\n",
    "\n",
    "where count(A,B) is the number of times A and B co-occur within a window of two words (co-occurrence window size of two) in a document and count(A) the number of times A occurs with anything else. Intuitively, given a document collection, the relative frequency captures the proportion of time the word B appears in the same document as A. (See Section 3.3, in Data-Intensive Text Processing with MapReduce).\n",
    "\n",
    "In the async lecture you learned different approaches to do this, and in this assignment, you will implement them:\n",
    "\n",
    "a.\tWrite a mapreduce program which uses the Stripes approach and writes its output in a file named rfstripes.txt \n",
    "\n",
    "b.\tWrite a mapreduce program which uses the Pairs approach and writes its output in a file named rfpairs.txt\n",
    "\n",
    "c.\tCompare the performance of the two approaches and output the relative performance to a file named rfcomp.txt. Compute the relative performance as follows: (running time for Pairs/ running time for Stripes). Also include an analysis comparing the communication costs for the two approaches. Instrument your mapper and reduces for counters where necessary to aid with your analysis.\n",
    "\n",
    "NOTE: please limit your analysis to the top 100 word pairs sorted by decreasing order of relative frequency for each word (tokens with all alphabetical letters).\n",
    "\n",
    "Please include markdown cell named rf.txt that describes the following:\n",
    "\n",
    "the input/output format in each Hadoop task, i.e., the keys for the mappers and reducers\n",
    "the Hadoop cluster settings you used, i.e., number of mappers and reducers\n",
    "the running time for each approach: pairs and stripes\n",
    "\n",
    "You can write your program using Python or MrJob (with Hadoop streaming) and you should run it on AWS. It is a good idea to develop and test your program on a local machine  before deploying on AWS. Remember your notebook, needs to have all the commands you used to run each Mapreduce job (i.e., pairs and stripes) -- include the Hadoop streaming commands you used to run your jobs.\n",
    "\n",
    "In addition the All the following files should be compressed in one ZIP file and submitted. The ZIP file should contain:\n",
    "\n",
    "\n",
    "A.\tThe result files: rfstripes.txt, rfpairs.txt, rfcomp.txt\n",
    "\n",
    "Prior to working with Hadoop, the corpus should first be preprocessed as follows:\n",
    "perform tokenization (whitespace and all non-alphabetic characters) and stopword removal  using standard tools from the Lucene search engine. All tokens should  then be replaced\n",
    "with unique integers for a more efficient encoding. \n",
    "\n",
    "\n",
    "== Preliminary information for the remaing HW problems===\n",
    "\n",
    "Much of this homework beyond this point will focus on the Apriori algorithm for frequent itemset  mining and the additional step for extracting association rules from these frequent itemsets.\n",
    "Please acquaint yourself with the background information (below)\n",
    "before approaching the remaining  assignments.\n",
    "\n",
    "=== Apriori background information ===\n",
    "\n",
    "Some background material for the  Apriori algorithm is located at:\n",
    "\n",
    " - Slides in Live Session #3\n",
    " - https://en.wikipedia.org/wiki/Apriori_algorithm\n",
    " - https://www.dropbox.com/s/k2zm4otych279z2/Apriori-good-slides.pdf?dl=0\n",
    " - http://snap.stanford.edu/class/cs246-2014/slides/02-assocrules.pdf\n",
    "\n",
    "Association Rules are frequently used for Market Basket Analysis (MBA) by retailers to\n",
    "understand the purchase behavior of their customers. This information can be then used for\n",
    "many different purposes such as cross-selling and up-selling of products, sales promotions,\n",
    "loyalty programs, store design, discount plans and many others.\n",
    "Evaluation of item sets: Once you have found the frequent itemsets of a dataset, you need\n",
    "to choose a subset of them as your recommendations. Commonly used metrics for measuring\n",
    "significance and interest for selecting rules for recommendations are: confidence; lift; and conviction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.7\"></a>\n",
    "## HW3.7 Apriori Algorithm\n",
    "What is the Apriori algorithm? Describe an example use in your domain of expertise and what kind of . Define confidence and lift.\n",
    "\n",
    "NOTE:\n",
    "For the remaining homework use the online browsing behavior dataset located at (same dataset as used above): \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.8\"></a>\n",
    "## HW3.8. Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a program using the A-priori algorithm\n",
    "to find products which are frequently browsed together. Fix the support to s = 100 \n",
    "(i.e. product sets need to occur together at least 100 times to be considered frequent) \n",
    "and find itemsets of size 2 and 3.\n",
    "\n",
    "Then extract association rules from these frequent items. \n",
    "\n",
    "A rule is of the form: \n",
    "\n",
    "(item1, item5) ⇒ item2.\n",
    "\n",
    "List the top 10 discovered rules in descreasing order of confidence in the following format\n",
    " \n",
    "(item1, item5) ⇒ item2, supportCount ,support, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.8.1\"></a>\n",
    "## HW3.8.1\n",
    "\n",
    "Benchmark your results using the pyFIM implementation of the Apriori algorithm\n",
    "(Apriori - Association Rule Induction / Frequent Item Set Mining implemented by Christian Borgelt). \n",
    "You can download pyFIM from here: \n",
    "\n",
    "http://www.borgelt.net/pyfim.html\n",
    "\n",
    "Comment on the results from both implementations (your Hadoop MapReduce of apriori versus pyFIM) \n",
    "in terms of results and execution times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END OF HOMEWORK\n",
    "==============="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
