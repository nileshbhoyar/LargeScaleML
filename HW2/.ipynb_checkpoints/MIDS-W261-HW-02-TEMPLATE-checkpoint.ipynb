{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#MIDS---w261-Machine-Learning-At-Scale\" data-toc-modified-id=\"MIDS---w261-Machine-Learning-At-Scale-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MIDS - w261 Machine Learning At Scale</a></div><div class=\"lev2 toc-item\"><a href=\"#Assignment---HW2\" data-toc-modified-id=\"Assignment---HW2-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Assignment - HW2</a></div><div class=\"lev1 toc-item\"><a href=\"#Table-of-Contents-\" data-toc-modified-id=\"Table-of-Contents--2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Table of Contents </a></div><div class=\"lev1 toc-item\"><a href=\"#1-Instructions\" data-toc-modified-id=\"1-Instructions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span></a></div><div class=\"lev3 toc-item\"><a href=\"#IMPORTANT\" data-toc-modified-id=\"IMPORTANT-301\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>IMPORTANT</a></div><div class=\"lev3 toc-item\"><a href=\"#===-INSTRUCTIONS-for-SUBMISSIONS-===\" data-toc-modified-id=\"===-INSTRUCTIONS-for-SUBMISSIONS-===-302\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>=== INSTRUCTIONS for SUBMISSIONS ===</a></div><div class=\"lev1 toc-item\"><a href=\"#2-Useful-References-and-Datasets\" data-toc-modified-id=\"2-Useful-References-and-Datasets-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span></a></div><div class=\"lev2 toc-item\"><a href=\"#References\" data-toc-modified-id=\"References-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>References</a></div><div class=\"lev2 toc-item\"><a href=\"#2.2-Enron-SPAM-DATA-SET\" data-toc-modified-id=\"2.2-Enron-SPAM-DATA-SET-42\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>2.2 Enron SPAM DATA SET</a></div><div class=\"lev3 toc-item\"><a href=\"#General-information-on-the-enronemail.txt-data-file\" data-toc-modified-id=\"General-information-on-the-enronemail.txt-data-file-421\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>General information on the enronemail.txt data file</a></div><div class=\"lev4 toc-item\"><a href=\"#Processing\" data-toc-modified-id=\"Processing-4211\"><span class=\"toc-item-num\">4.2.1.1&nbsp;&nbsp;</span>Processing</a></div><div class=\"lev4 toc-item\"><a href=\"#Format-of-the-Enron-SPAM-data\" data-toc-modified-id=\"Format-of-the-Enron-SPAM-data-4212\"><span class=\"toc-item-num\">4.2.1.2&nbsp;&nbsp;</span>Format of the Enron SPAM data</a></div><div class=\"lev3 toc-item\"><a href=\"#The-ENRON-SPAM-dataset-(has-only-100-records)\" data-toc-modified-id=\"The-ENRON-SPAM-dataset-(has-only-100-records)-422\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>The ENRON SPAM dataset (has only 100 records)</a></div><div class=\"lev2 toc-item\"><a href=\"#Simple-EDA-of-ENRON-Dataset\" data-toc-modified-id=\"Simple-EDA-of-ENRON-Dataset-43\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Simple EDA of ENRON Dataset</a></div><div class=\"lev1 toc-item\"><a href=\"#HW-Problems\" data-toc-modified-id=\"HW-Problems-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span></a></div><div class=\"lev2 toc-item\"><a href=\"#3.--HW2.0--Functional-Programming\" data-toc-modified-id=\"3.--HW2.0--Functional-Programming-51\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>3.  HW2.0  Functional Programming</a></div><div class=\"lev3 toc-item\"><a href=\"#W2.0.\" data-toc-modified-id=\"W2.0.-511\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>W2.0.</a></div><div class=\"lev3 toc-item\"><a href=\"#HW2.0.1\" data-toc-modified-id=\"HW2.0.1-512\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>HW2.0.1</a></div><div class=\"lev1 toc-item\"><a href=\"#Set-up-your-directories-on-your-local-(VM)-machine-and-on-HDFS\" data-toc-modified-id=\"Set-up-your-directories-on-your-local-(VM)-machine-and-on-HDFS-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Set up your directories on your local (VM) machine and on HDFS</a></div><div class=\"lev1 toc-item\"><a href=\"#WordCount:-A-full-example-in-Hadoop-Stream-to-practice-with\" data-toc-modified-id=\"WordCount:-A-full-example-in-Hadoop-Stream-to-practice-with-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>WordCount: A full example in Hadoop Stream to practice with</a></div><div class=\"lev2 toc-item\"><a href=\"#3.--HW2.1.-Sort-in-Hadoop-MapReduce--(Partial-sort,--total-sort)---List-in-alphabetical-order\" data-toc-modified-id=\"3.--HW2.1.-Sort-in-Hadoop-MapReduce--(Partial-sort,--total-sort)---List-in-alphabetical-order-71\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>3.  HW2.1. Sort in Hadoop MapReduce  (Partial sort,  total sort) - List in alphabetical order</a></div><div class=\"lev3 toc-item\"><a href=\"#HW2.1.1-Calculate-the-vocabulary-size-(number-of-unique-words-in-the-Alice-book)\" data-toc-modified-id=\"HW2.1.1-Calculate-the-vocabulary-size-(number-of-unique-words-in-the-Alice-book)-711\"><span class=\"toc-item-num\">7.1.1&nbsp;&nbsp;</span>HW2.1.1 Calculate the vocabulary size (number of unique words in the Alice book)</a></div><div class=\"lev3 toc-item\"><a href=\"#HW2.1.2--TOTAL-SORT-using-a-single-reducer\" data-toc-modified-id=\"HW2.1.2--TOTAL-SORT-using-a-single-reducer-712\"><span class=\"toc-item-num\">7.1.2&nbsp;&nbsp;</span>HW2.1.2  TOTAL SORT using a single reducer</a></div><div class=\"lev3 toc-item\"><a href=\"#HW2.1.2.b-TOTAL-SORT-using-multiple-reducers-[OPTITIONAL-for-this-week;-will-be-covered-in-next-live-session]\" data-toc-modified-id=\"HW2.1.2.b-TOTAL-SORT-using-multiple-reducers-[OPTITIONAL-for-this-week;-will-be-covered-in-next-live-session]-713\"><span class=\"toc-item-num\">7.1.3&nbsp;&nbsp;</span>HW2.1.2.b TOTAL SORT using multiple reducers [OPTITIONAL for this week; will be covered in next live session]</a></div><div class=\"lev3 toc-item\"><a href=\"#HW2.1.3-How-many-times-does-the-word-alice-occur-in-the-book?\" data-toc-modified-id=\"HW2.1.3-How-many-times-does-the-word-alice-occur-in-the-book?-714\"><span class=\"toc-item-num\">7.1.4&nbsp;&nbsp;</span>HW2.1.3 How many times does the word alice occur in the book?</a></div><div class=\"lev2 toc-item\"><a href=\"#3.--HW2.2-EDA-using-WORDCOUNT-in--Hadoop---Top-10-Words---\" data-toc-modified-id=\"3.--HW2.2-EDA-using-WORDCOUNT-in--Hadoop---Top-10-Words----72\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>3.  HW2.2 EDA using WORDCOUNT in  Hadoop - Top 10 Words   </a></div><div class=\"lev3 toc-item\"><a href=\"#Example-of-word-splitting\" data-toc-modified-id=\"Example-of-word-splitting-721\"><span class=\"toc-item-num\">7.2.1&nbsp;&nbsp;</span>Example of word splitting</a></div><div class=\"lev3 toc-item\"><a href=\"#HW2.2.1-WORDCOUNT\" data-toc-modified-id=\"HW2.2.1-WORDCOUNT-722\"><span class=\"toc-item-num\">7.2.2&nbsp;&nbsp;</span>HW2.2.1 WORDCOUNT</a></div><div class=\"lev3 toc-item\"><a href=\"#HW2.2.2\" data-toc-modified-id=\"HW2.2.2-723\"><span class=\"toc-item-num\">7.2.3&nbsp;&nbsp;</span>HW2.2.2</a></div><div class=\"lev3 toc-item\"><a href=\"#HW2.2.3-(Optional)\" data-toc-modified-id=\"HW2.2.3-(Optional)-724\"><span class=\"toc-item-num\">7.2.4&nbsp;&nbsp;</span>HW2.2.3 (Optional)</a></div><div class=\"lev2 toc-item\"><a href=\"#3.--HW2.3-Multinomial-NAIVE-BAYES-with-NO-Smoothing-using-a-single-reducer---\" data-toc-modified-id=\"3.--HW2.3-Multinomial-NAIVE-BAYES-with-NO-Smoothing-using-a-single-reducer----73\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>3.  HW2.3 Multinomial NAIVE BAYES with NO Smoothing using a single reducer   </a></div><div class=\"lev3 toc-item\"><a href=\"#Multinomial-NAIVE-BAYES-model-with-NO-Smoothing-using-a-single-reducer\" data-toc-modified-id=\"Multinomial-NAIVE-BAYES-model-with-NO-Smoothing-using-a-single-reducer-731\"><span class=\"toc-item-num\">7.3.1&nbsp;&nbsp;</span>Multinomial NAIVE BAYES model with NO Smoothing using a single reducer</a></div><div class=\"lev3 toc-item\"><a href=\"#HW2.3.1-Learn-a-Multinomial-Naive-Bayes-model-on-a-small--dataset-(Chinese-dataset:-5-documents)\" data-toc-modified-id=\"HW2.3.1-Learn-a-Multinomial-Naive-Bayes-model-on-a-small--dataset-(Chinese-dataset:-5-documents)-732\"><span class=\"toc-item-num\">7.3.2&nbsp;&nbsp;</span>HW2.3.1 Learn a Multinomial Naive Bayes model on a small  dataset (Chinese dataset: 5 documents)</a></div><div class=\"lev2 toc-item\"><a href=\"#Multinomial-Naive-bayes-Classification\" data-toc-modified-id=\"Multinomial-Naive-bayes-Classification-74\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Multinomial Naive bayes Classification</a></div><div class=\"lev3 toc-item\"><a href=\"#Sketch-of--mathematics:\" data-toc-modified-id=\"Sketch-of--mathematics:-741\"><span class=\"toc-item-num\">7.4.1&nbsp;&nbsp;</span>Sketch of  mathematics:</a></div><div class=\"lev3 toc-item\"><a href=\"#Divide-the-task-into-2-map-reduce-tasks\" data-toc-modified-id=\"Divide-the-task-into-2-map-reduce-tasks-742\"><span class=\"toc-item-num\">7.4.2&nbsp;&nbsp;</span>Divide the task into 2 map-reduce tasks</a></div><div class=\"lev3 toc-item\"><a href=\"#Suggested-Record-format-that-can-be-used-of-all-Mappers-and-Reducers\" data-toc-modified-id=\"Suggested-Record-format-that-can-be-used-of-all-Mappers-and-Reducers-743\"><span class=\"toc-item-num\">7.4.3&nbsp;&nbsp;</span>Suggested Record format that can be used of all Mappers and Reducers</a></div><div class=\"lev3 toc-item\"><a href=\"#Load-model-file-and-play-with-it-before-writing-the-classifier-job\" data-toc-modified-id=\"Load-model-file-and-play-with-it-before-writing-the-classifier-job-744\"><span class=\"toc-item-num\">7.4.4&nbsp;&nbsp;</span>Load model file and play with it before writing the classifier job</a></div><div class=\"lev3 toc-item\"><a href=\"#Use-logs-to-avoid-precision-problems\" data-toc-modified-id=\"Use-logs-to-avoid-precision-problems-745\"><span class=\"toc-item-num\">7.4.5&nbsp;&nbsp;</span>Use logs to avoid precision problems</a></div><div class=\"lev3 toc-item\"><a href=\"#Create-a-Naive-Bayes-Model-class\" data-toc-modified-id=\"Create-a-Naive-Bayes-Model-class-746\"><span class=\"toc-item-num\">7.4.6&nbsp;&nbsp;</span>Create a Naive Bayes Model class</a></div><div class=\"lev3 toc-item\"><a href=\"#Test-Driver-for-Multinomial-Naive-Bayes-Classifier\" data-toc-modified-id=\"Test-Driver-for-Multinomial-Naive-Bayes-Classifier-747\"><span class=\"toc-item-num\">7.4.7&nbsp;&nbsp;</span>Test Driver for Multinomial Naive Bayes Classifier</a></div><div class=\"lev3 toc-item\"><a href=\"#modelling-phase\" data-toc-modified-id=\"modelling-phase-748\"><span class=\"toc-item-num\">7.4.8&nbsp;&nbsp;</span>modelling phase</a></div><div class=\"lev3 toc-item\"><a href=\"#classification-phase\" data-toc-modified-id=\"classification-phase-749\"><span class=\"toc-item-num\">7.4.9&nbsp;&nbsp;</span>classification phase</a></div><div class=\"lev3 toc-item\"><a href=\"#Run-Map-Reduce-Job-to-learn-a-multinomical-Naive-Model-from-data\" data-toc-modified-id=\"Run-Map-Reduce-Job-to-learn-a-multinomical-Naive-Model-from-data-7410\"><span class=\"toc-item-num\">7.4.10&nbsp;&nbsp;</span>Run Map Reduce Job to learn a multinomical Naive Model from data</a></div><div class=\"lev3 toc-item\"><a href=\"#Run-Map-Reduce-Job-to-classify-data\" data-toc-modified-id=\"Run-Map-Reduce-Job-to-classify-data-7411\"><span class=\"toc-item-num\">7.4.11&nbsp;&nbsp;</span>Run Map Reduce Job to classify data</a></div><div class=\"lev3 toc-item\"><a href=\"#Display-the--accuracy-measure\" data-toc-modified-id=\"Display-the--accuracy-measure-7412\"><span class=\"toc-item-num\">7.4.12&nbsp;&nbsp;</span>Display the  accuracy measure</a></div><div class=\"lev3 toc-item\"><a href=\"#Write-a-systems-test-to-regression-test-your-map-reduce-job\" data-toc-modified-id=\"Write-a-systems-test-to-regression-test-your-map-reduce-job-7413\"><span class=\"toc-item-num\">7.4.13&nbsp;&nbsp;</span>Write a systems test to regression test your map reduce job</a></div><div class=\"lev3 toc-item\"><a href=\"#Chinese-dataset\" data-toc-modified-id=\"Chinese-dataset-7414\"><span class=\"toc-item-num\">7.4.14&nbsp;&nbsp;</span>Chinese dataset</a></div><div class=\"lev3 toc-item\"><a href=\"#HW2.3.2-Learn-a-multinomial-naive-Bayes-model-(with-no-smoothing)-by-hand\" data-toc-modified-id=\"HW2.3.2-Learn-a-multinomial-naive-Bayes-model-(with-no-smoothing)-by-hand-7415\"><span class=\"toc-item-num\">7.4.15&nbsp;&nbsp;</span>HW2.3.2 Learn a multinomial naive Bayes model (with no smoothing) by hand</a></div><div class=\"lev3 toc-item\"><a href=\"#Hand-calculations-for-Multinomial-naive-Bayes-(Learning-and-classifiction)\" data-toc-modified-id=\"Hand-calculations-for-Multinomial-naive-Bayes-(Learning-and-classifiction)-7416\"><span class=\"toc-item-num\">7.4.16&nbsp;&nbsp;</span>Hand calculations for Multinomial naive Bayes (Learning and classifiction)</a></div><div class=\"lev3 toc-item\"><a href=\"#HW2.3.3-Learn-a-multinomial-naive-Bayes-model-(with-no-smoothing)-for-SPAM-filtering\" data-toc-modified-id=\"HW2.3.3-Learn-a-multinomial-naive-Bayes-model-(with-no-smoothing)-for-SPAM-filtering-7417\"><span class=\"toc-item-num\">7.4.17&nbsp;&nbsp;</span>HW2.3.3 Learn a multinomial naive Bayes model (with no smoothing) for SPAM filtering</a></div><div class=\"lev3 toc-item\"><a href=\"#Run-Map-Reduce-Job-to-learn-a-multinomical-Naive-Model-from-data\" data-toc-modified-id=\"Run-Map-Reduce-Job-to-learn-a-multinomical-Naive-Model-from-data-7418\"><span class=\"toc-item-num\">7.4.18&nbsp;&nbsp;</span>Run Map Reduce Job to learn a multinomical Naive Model from data</a></div><div class=\"lev3 toc-item\"><a href=\"#HW-2.3.4-Classify-Documents-using-the-learnt-Multinomial-Naive-Bayes-model-using-Hadoop-Streaming\" data-toc-modified-id=\"HW-2.3.4-Classify-Documents-using-the-learnt-Multinomial-Naive-Bayes-model-using-Hadoop-Streaming-7419\"><span class=\"toc-item-num\">7.4.19&nbsp;&nbsp;</span>HW 2.3.4 Classify Documents using the learnt Multinomial Naive Bayes model using Hadoop Streaming</a></div><div class=\"lev4 toc-item\"><a href=\"#Note:-Map-Tasks-and-map-lifecycles\" data-toc-modified-id=\"Note:-Map-Tasks-and-map-lifecycles-74191\"><span class=\"toc-item-num\">7.4.19.1&nbsp;&nbsp;</span>Note: Map Tasks and map lifecycles</a></div><div class=\"lev4 toc-item\"><a href=\"#NOTE:-on-small-multiplying-small-numbers\" data-toc-modified-id=\"NOTE:-on-small-multiplying-small-numbers-74192\"><span class=\"toc-item-num\">7.4.19.2&nbsp;&nbsp;</span>NOTE: on small multiplying small numbers</a></div><div class=\"lev3 toc-item\"><a href=\"#Run-Map-Reduce-Job-to-classify-data\" data-toc-modified-id=\"Run-Map-Reduce-Job-to-classify-data-7420\"><span class=\"toc-item-num\">7.4.20&nbsp;&nbsp;</span>Run Map Reduce Job to classify data</a></div><div class=\"lev3 toc-item\"><a href=\"#Display-the--accuracy-measure\" data-toc-modified-id=\"Display-the--accuracy-measure-7421\"><span class=\"toc-item-num\">7.4.21&nbsp;&nbsp;</span>Display the  accuracy measure</a></div><div class=\"lev3 toc-item\"><a href=\"#Plot-a-histogram-of-the--posterior-probabilities\" data-toc-modified-id=\"Plot-a-histogram-of-the--posterior-probabilities-7422\"><span class=\"toc-item-num\">7.4.22&nbsp;&nbsp;</span>Plot a histogram of the  posterior probabilities</a></div><div class=\"lev2 toc-item\"><a href=\"#3.--HW2.4-Use-Laplace-plus-one-smoothing--\" data-toc-modified-id=\"3.--HW2.4-Use-Laplace-plus-one-smoothing---75\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>3.  HW2.4 Use Laplace plus-one smoothing  </a></div><div class=\"lev3 toc-item\"><a href=\"#cut-and-paste-MULTIPLE-CELLS\" data-toc-modified-id=\"cut-and-paste-MULTIPLE-CELLS-751\"><span class=\"toc-item-num\">7.5.1&nbsp;&nbsp;</span>cut and paste MULTIPLE CELLS</a></div><div class=\"lev2 toc-item\"><a href=\"#3.--HW2.5-Ignore-rare-words--(Optional)-\" data-toc-modified-id=\"3.--HW2.5-Ignore-rare-words--(Optional)--76\"><span class=\"toc-item-num\">7.6&nbsp;&nbsp;</span>3.  HW2.5 Ignore rare words  (Optional) </a></div><div class=\"lev2 toc-item\"><a href=\"#3.--HW2.6-Benchmark-your-code-with-the-Python-SciKit-Learn-(OPTIONAL)--\" data-toc-modified-id=\"3.--HW2.6-Benchmark-your-code-with-the-Python-SciKit-Learn-(OPTIONAL)---77\"><span class=\"toc-item-num\">7.7&nbsp;&nbsp;</span>3.  HW2.6 Benchmark your code with the Python SciKit-Learn (OPTIONAL)  </a></div><div class=\"lev3 toc-item\"><a href=\"#HW-2.6.1-Bernoulli-Naive-Bayes-(OPTIONAL:-note-this-exercise-is-a-stretch-HW-and-optional)\" data-toc-modified-id=\"HW-2.6.1-Bernoulli-Naive-Bayes-(OPTIONAL:-note-this-exercise-is-a-stretch-HW-and-optional)-771\"><span class=\"toc-item-num\">7.7.1&nbsp;&nbsp;</span>HW 2.6.1 Bernoulli Naive Bayes (OPTIONAL: note this exercise is a stretch HW and optional)</a></div><div class=\"lev2 toc-item\"><a href=\"#3.--HW2.7-Preprocess-the-Entire-Spam-Dataset-(OPTIONAL)-\" data-toc-modified-id=\"3.--HW2.7-Preprocess-the-Entire-Spam-Dataset-(OPTIONAL)--78\"><span class=\"toc-item-num\">7.8&nbsp;&nbsp;</span>3.  HW2.7 Preprocess the Entire Spam Dataset (OPTIONAL) </a></div><div class=\"lev2 toc-item\"><a href=\"#3.--HW2.8-Build-and-evaluate-a-NB-classifier-on-the--Entire-Spam-Dataset-(OPTIONAL)-\" data-toc-modified-id=\"3.--HW2.8-Build-and-evaluate-a-NB-classifier-on-the--Entire-Spam-Dataset-(OPTIONAL)--79\"><span class=\"toc-item-num\">7.9&nbsp;&nbsp;</span>3.  HW2.8 Build and evaluate a NB classifier on the  Entire Spam Dataset (OPTIONAL) </a></div><div class=\"lev2 toc-item\"><a href=\"#HW2.8.1-OPTIONAL\" data-toc-modified-id=\"HW2.8.1-OPTIONAL-710\"><span class=\"toc-item-num\">7.10&nbsp;&nbsp;</span>HW2.8.1 OPTIONAL</a></div><div class=\"lev2 toc-item\"><a href=\"#---------END-OF-HOWEWORK---------\" data-toc-modified-id=\"---------END-OF-HOWEWORK----------711\"><span class=\"toc-item-num\">7.11&nbsp;&nbsp;</span>-------  END OF HOWEWORK --------</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/**********************************************************************************************\n",
       "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
       "https://github.com/mathjax/MathJax/issues/1300\n",
       "A quick hack to fix this based on stackoverflow discussions: \n",
       "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
       "**********************************************************************************************/\n",
       "\n",
       "$('.math>span').css(\"border-left-color\",\"transparent\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "/**********************************************************************************************\n",
    "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
    "https://github.com/mathjax/MathJax/issues/1300\n",
    "A quick hack to fix this based on stackoverflow discussions: \n",
    "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
    "**********************************************************************************************/\n",
    "\n",
    "$('.math>span').css(\"border-left-color\",\"transparent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW2\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  Nilesh Bhoyar  \n",
    "__Class:__ MIDS w261 ,Summer 2017 Group 2    \n",
    "__Email:__  nilesh.bhoyar@iSchool.Berkeley.edu     \n",
    "__StudentId__  26302327     __End of StudentId__     \n",
    "__Week:__   2\n",
    "\n",
    "__NOTE:__ please replace `1234567` with your student id above      \n",
    "__Due Time:__ HW is due the Tuesday of the following week by 8AM (West coast time)\n",
    "\n",
    "# Instructions\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale   \n",
    "DATSCIW261 ASSIGNMENT #2\n",
    "\n",
    "Version 2017-16-5\n",
    "\n",
    "\n",
    "## IMPORTANT\n",
    "\n",
    "This homework can be completed locally on your computer \n",
    "\n",
    "### === INSTRUCTIONS for SUBMISSIONS ===   \n",
    "Follow the instructions for submissions carefully.\n",
    "\n",
    "Each student has a `HW-<user>` repository for all assignments.   \n",
    "\n",
    "Push the following to your HW github repo into the master branch:\n",
    "* Your local HW2 directory. Your repo file structure should look like this:\n",
    "\n",
    "```\n",
    "HW-<user>\n",
    "    --HW3\n",
    "       |__MIDS-W261-HW-03-<Student_id>.ipynb\n",
    "       |__MIDS-W261-HW-03-<Student_id>.pdf\n",
    "       |__some other hw3 file\n",
    "    --HW4\n",
    "       |__MIDS-W261-HW-04-<Student_id>.ipynb\n",
    "       |__MIDS-W261-HW-04-<Student_id>.pdf\n",
    "       |__some other hw4 file\n",
    "    etc..\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful References and Datasets\n",
    "## References\n",
    "* See corresponding aysnc lecture and live session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enron SPAM DATA SET\n",
    "\n",
    "The dataset  is a curated subset of the Enron email corpus. More details are given in the next section.\n",
    "\n",
    "NOTE: This SPAM/HAM dataset for HW2 contains 100 records from the Enron SPAM/HAM corpus. Please limit your study to this unless otherwise instructed. There are about 93,000 emails in the original SPAM/HAM corpus. There are several versions of the SPAM/HAM corpus. Other Enron-Spam datasets are available from http://www.aueb.gr/users/ion/data/enron-spam/index.html and http://www.aueb.gr/users/ion/publications.html in both raw and pre-processed form. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General information on the enronemail.txt data file\n",
    "These data include email messages from 6 enron employees(in addition to various spam messages from a variety of sources) that were made publicly available after the company's collapse. These data were originally part of a much larger set that included many more individuals, but were distilled to the 6 for a publication developing\n",
    "personalized Bayesian spam filters. Please follow the links below for precise information regarding this data and research.\n",
    "\n",
    "* Source data: http://www.aueb.gr/users/ion/data/enron-spam/\n",
    "* Source publication: http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf\n",
    "\n",
    "#### Processing \n",
    "\n",
    "For their work, Metsis et al. (the authors) appeared to have pre-processed the data,\n",
    "not only collapsing all text to lower-case, but additionally separating \"words\" by spaces,\n",
    "where \"words\" unfortunately include punctuation. As a concrete example, the sentence:\n",
    "\n",
    "    \"Hey Jon, I hope you don't get lost out there this weekend!\"\n",
    "\n",
    "would have been reduced by Metsis et al. to the form:\n",
    "\n",
    "    \"hey jon , i hope you don ' t get lost out there this weekend ! \"\n",
    "\n",
    "Upon seeing this we have reverted the data back toward its original state, removing spaces so that our sample sentence would now look like:\n",
    "\n",
    "    \"hey jon, i hope you don't get lost out there this weekend!\"\n",
    "\n",
    "so that we have at least preserved contractions and other higher-order lexical forms. However, one must be aware that this reversion is not complete, and that some object (specifically web sites) will be ill-formatted, and that all text is still lower-cased.\n",
    "\n",
    "####  Format of the Enron SPAM data\n",
    "All messages are collated to a tab-delimited format:\n",
    "\n",
    "    ID \\t SPAM \\t SUBJECT \\t CONTENT \\n\n",
    "\n",
    "where:\n",
    "\n",
    "    ID = string; unique message identifier\n",
    "    SPAM = binary; with 1 indicating a spam message\n",
    "    SUBJECT = string; title of the message\n",
    "    CONTENT = string; content of the message\n",
    "\n",
    "Note that either of SUBJECT or CONTENT may be \"NA\", and that all tab (\\t) and newline (\\n) characters have been removed from both of the SUBJECT and CONTENT columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ENRON SPAM dataset (has only 100 records)\n",
    "Save the data in the next cell to file byt executing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `NaiveBayes': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/enronemail_1h.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/enronemail_1h.txt\n",
    "0001.1999-12-10.farmer\t0\t christmas tree farm pictures\tNA\n",
    "0001.1999-12-10.kaminski\t0\t re: rankings\t thank you.\n",
    "0001.2000-01-17.beck\t0\t leadership development pilot\t\" sally:  what timing, ask and you shall receive. as per our discussion, listed below  is an update on the leadership pilot. your vendor selection team will  receive an update and even more information later in the week.  on the lunch & learn for energy operations, the audience and focus will be  your group. we are ready to start up when appropriate.  thank you for your time today. please call me if you have any questions at  x 33597.  ----------------------forwarded by julie armstrong/corp/enron on 01/17/2000  06:44 pm---------------------------  from: susan runkel @ ect 01/17/2000 03:22 pm  to: cindy skinner/hou/ect @ ect, brad mcsherry/hou/ect @ ect, norma  villarreal/hou/ect @ ect, kimberly rizzi/hou/ect @ ect, fran l mayes/hou/ect @ ect,  gary buck/hou/ect @ ect, robert jones/corp/enron @ enron, sheila  walton/hou/ect @ ect, philip conn/corp/enron @ enron, mary overgaard/pdx/ect @ ect,  kim melodick/hou/ect @ ect, valeria a hope/hou/ect @ ect  cc: david oxley/hou/ect @ ect, susan carrera/hou/ect @ ect, jane  allen/hou/ect @ ect, christine shenkman/enron_development @ enron_development,  kathryn mclean/hou/ect @ ect, gracie s presas/hou/ect @ ect, janice  riedel/hou/ect @ ect, julie armstrong/corp/enron @ enron  subject: leadership development pilot  good news regarding the ena leadership curriculum! through the help of a  vendor selection team from eops, we've chosen southwest performance group and  wilson learning products as one of our primary vendors for the leadership  curriculum and programs. we are ready to conduct a pilot on february 8-10 of  six modules. the purpose of the pilot is to evaluate for fine-tuning the  wilson learning materials and facilitators and to present just a portion of  the leadership curriculum.  in order to evaluate the materials thoroughly, it would be great to get a  cross-section of ena to attend. we are asking that you invite several  supervisors from your client groups to participate in any of the courses  listed below. the sessions will be held in room 560 and times are listed  below. also attached is a description of the modules. all are designed for  supervisors only, with the exception being \"\" communicating effectively \"\". this  is open to any employee. as a benefit in attending the pilot, i will pick up  the cost., so there will be no charge back for their attendance.  we are currently completing the curriculum design and will have information  on the full curriculum available in february. this will include options  other than \"\" classrom setting \"\" for development.  please respond back to gracie presas by february 1 with your names. if you  have further questions, please contact me at 3-7394. we are really excited  that we have this available and hope that your clients will find it to be  valuable.  the following are half-day sessions. supervisors may sign up for any or all  depending on their need. it would be helpful if supervisors attend a minimum  of two modules.  date module time target audience  feb. 8 meeting leadership challenges 8-12 am supervisors with less  than 6 months experience  working styles 1-5 pm any supervisor  feb. 9 coaching to performance 8-12 am any supervisor  motivating for results 1-5 pm any supervisor  feb. 10 communicating effectively 8-12 am any employee  delegating and directing 1-5 pm any supervisor\"\n",
    "0001.2000-06-06.lokay\t0\t\" key dates and impact of upcoming sap implementation over the next few weeks, project apollo and beyond will conduct its final sap  implementation \u0001) this implementation will impact approximately 12,000 new  users plus all existing system users. sap brings a new dynamic to enron,  enhancing the timely flow and sharing of specific project, human resources,  procurement, and financial information across business units and across  continents.  this final implementation will retire multiple, disparate systems and replace  them with a common, integrated system encompassing many processes including  payroll, timekeeping, benefits, project management, and numerous financial  processes.  employees will be empowered to update and/or view their personal information  via the intranet-based ehronline--a single front-end to sap's self service  functionality and enron's global information system (gis). among other  things, individuals will be able to update personal information (including  w-4, addresses and personal banking information), manage their individual  time using a new time entry tool, view their benefit elections, and view  their personal payroll information on-line.  all enron employees paid out of corporate payroll in houston, excluding  azurix employees  the financial communities of enron energy services, enron investment  partners, enron north america, enron renewable energy corporation, gas  pipeline group, global finance, global it, enron networks, and global  products.  the project management communities of enron north america, gas pipeline  group, global finance, global it, enron networks, and global products.  the human resources communities of corporate, global e & p, enron energy  services, enron engineering and construction company, enron investment  partners, enron north america, enron renewable energy corporation (houston  only), the international regions, gas pipeline group, global finance, global  it, enron networks, and global products.  existing sap users currently supported by the center of expertise (coe) \u0001)  including the london coe.  people will be impacted gradually over the next few weeks:  june 12-current sap users may notice (and may use) new features in some of  the sap modules--this new functionality was developed to meet requirements  of business units implementing sap as part of this final implementation.  june 22-timekeeping functionality will be available for all employees paid  out of corporate payroll in houston (excluding azurix employees).  -new sap coding must be used on timesheets.  -system ids will be available for all new users.  june 30-deadline! all time for the period beginning june 16 th and ending  june 30 th must be entered into sap by 3:00 cst.  -new sap coding must be used for all expenses and invoices.  july 5-all remaining functionality (project management, financials, and  human resources) are available to new end-users.  for more information...  visit us at an information booth in the enron building lobby on wednesday,  june 7 th and thursday, june 8 th (10 a.m. till 2 p.m. each day.)  visit our intranet site at http:\\\\sap. enron. com for job aids and other useful  information.  contact the site manager coordinating the implementation within your business  unit or global function--specific site manager contact information can be  found on the intranet at http:\\\\sap. enron. com.  contact the center of expertise (coe) for sap implementation and production  support questions via telephone at (713) 345-4 sap or via e-mail at  sap. coe @ enron. com.\"\t\n",
    "0001.2001-02-07.kitchen\t0\t key hr issues going forward\t a) year end reviews-report needs generating like mid-year documenting business unit performance on review completion-david to john;  b) work out or plan generation for the nim/issues employees-david to john;  c) hpl transition issues-ongoing.  officially transferred.  regards  delainey\n",
    "0001.2001-04-02.williams\t0\t re: quasi\t\" good morning,  i'd love to go get some coffee with you, but remember that annoying project that mike etringer wants me to work on for him? this morning i am kinda under some pressure to hurry up and try to get some stuff figured out so i really don't have much spare time right now. ja would flip out if i left for coffee now. maybe later this afternoon? or tomorrow morning? anyhow, another ride sounds really cool. i had lots of fun. and yes, it would be cooler if i didn't have to worry about work. let me know when you have extra time to go for a ride.  my weekend was pretty fun. i weed-wacked (is that a word?) my yard for the first time. it looks so bad. i so don't know anything about lawn care. also i planted some herbs and stuff in my yard which i am sure my dog will destroy, but it s worth a try. oh yeah, i also bought a snowboard. it's pretty cool. i bought some step-in switch boots, too. cool, eh?  so i'll talk to you later. have a great day.\"\n",
    "0002.1999-12-13.farmer\t0\t\" vastar resources, inc.\"\t\" gary, production from the high island larger block a-1 # 2 commenced on  saturday at 2:00 p.m. at about 6,500 gross. carlos expects between 9,500 and  10,000 gross for tomorrow. vastar owns 68% of the gross production.  george x 3-6992  ----------------------forwarded by george weissman/hou/ect on 12/13/99 10:16  am---------------------------  daren j farmer  12/10/99 10:38 am  to: carlos j rodriguez/hou/ect @ ect  cc: george weissman/hou/ect @ ect, melissa graves/hou/ect @ ect  subject: vastar resources, inc.  carlos,  please call linda and get everything set up.  i'm going to estimate 4,500 coming up tomorrow, with a 2,000 increase each  following day based on my conversations with bill fischer at bmar.  d.  ----------------------forwarded by daren j farmer/hou/ect on 12/10/99 10:34  am---------------------------  enron north america corp.  from: george weissman 12/10/99 10:00 am  to: daren j farmer/hou/ect @ ect  cc: gary bryan/hou/ect @ ect, melissa graves/hou/ect @ ect  subject: vastar resources, inc.  darren,  the attached appears to be a nomination from vastar resources, inc. for the  high island larger block a-1 # 2 (previously, erroneously referred to as the  # 1 well). vastar now expects the well to commence production sometime  tomorrow. i told linda harris that we'd get her a telephone number in gas  control so she can provide notification of the turn-on tomorrow. linda's  numbers, for the record, are 281. 584. 3359 voice and 713. 312. 1689 fax.  would you please see that someone contacts linda and advises her how to  submit future nominations via e-mail, fax or voice? thanks.  george x 3-6992  ----------------------forwarded by george weissman/hou/ect on 12/10/99 09:44  am---------------------------  \"\" linda harris \"\" on 12/10/99 09:38:43 am  to: george weissman/hou/ect @ ect  cc:  subject: hi a-1 # 2  effective 12-11-99  |--------+----------+-----------|  | | | |  | mscf/d | min ftp | time |  | | | |  |--------+----------+-----------|  | | | |  | 4,500 | 9,925 | 24 hours |  | | | |  |--------+----------+-----------|  | | | |  | 6,000 | 9,908 | 24 hours |  | | | |  |--------+----------+-----------|  | | | |  | 8,000 | 9,878 | 24 hours |  | | | |  |--------+----------+-----------|  | | | |  | 10,000 | 9,840 | 24 hours |  | | | |  |--------+----------+-----------|  | | | |  | 12,000 | 9,793 | 24 hours |  | | | |  |--------+----------+-----------|  | | | |  | 14,000 | 9,738 | 24 hours |  | | | |  |--------+----------+-----------|  | | | |  | 16,000 | 9,674 | 24 hours |  | | | |  |--------+----------+-----------|  | | | |  | 18,000 | 9,602 | 24 hours |  | | | |  |--------+----------+-----------|  | | | |  | 20,000 | 9,521 | 24 hours |  | | | |  |--------+----------+-----------|  | | | |  | 22,000 | 9,431 | 24 hours |  | | | |  |--------+----------+-----------|  | | | |  | 24,000 | 9,332 | 24 hours |  | | | |  |--------+----------+-----------|  | | | |  | 26,000 | 9,224 | 24 hours |  | | | |  |--------+----------+-----------|  | | | |  | 28,000 | 9,108 | 24 hours |  | | | |  |--------+----------+-----------|  | | | |  | 30,000 | 8,982 | 24 hours |  | | | |  |--------+----------+-----------|  | | | |  | 32,000 | 8,847 | 24 hours |  | | | |  |--------+----------+-----------|  | | | |  | 34,000 | 8,703 | 24 hours |  | | | |  |--------+----------+-----------|  | | | |  | 36,000 | 8,549 | 24 hours |  | | | |  |--------+----------+-----------|\"\n",
    "0002.2001-02-07.kitchen\t0\t congrats!\t\" contratulations on the execution of the central maine sos deal! this is another great example of what we can do when everyone comes together to get something done. this transaction brings both strategic value to the business, nice positions for the book and quite a nice chunk of change as well!  great job guys!  (hey dana, are you paying for the celebration dinner?!)\"\n",
    "0002.2001-05-25.SA_and_HP\t1\t fw: this is the solution i mentioned lsc\t\" oo  thank you,  your email address was obtained from a purchased list,  reference # 2020 mid = 3300. if you wish to unsubscribe  from this list, please click here and enter  your name into the remove box. if you have previously unsubscribed  and are still receiving this message, you may email our abuse  control center, or call 1-888-763-2497, or write us at: nospam,  6484 coral way, miami, fl, 33155 \"\". (c) 2002  web credit inc. all rights reserved.\"\n",
    "0002.2003-12-18.GP\t1\t adv: space saving computer to replace that big box on or under your desk!!\t\" revolutionary!!! full featured!!!  space saving computer in a  keyboard  eliminate  that big box computer forever!  great  forhome.... office... or students... any place where desk space is at a  premium!  the  computer in a  keyboard eliminates the tower that takes up valuable space on  or under your desk. a full featured, powerful computer for the price you would  pay for a large tower. comes standard with: 1. 8 ghz intelt pentium 4  processor (upgradeable) 40 gigabyte hard drive  (upgradeable) 256 mb ramupgradeable to 2 gb cd-rw dvd combo drive  64 bit hardware accelerated 3 d graphics soundmax integrated digital audio  internal 56 k fax-modem serial, parallel, audio, 4 usb ports (2 side, and 2  back) 2 button ps/2 scroll mouse microsoft xp home  edition and a 15 \"\" lcd flat screen  monitor (upgradeable)  isalso included in the base configuration!  click  below for more information:  http:// www.. com /  if  you wish to stop receiving this email, click on the link below. \"\n",
    "0002.2004-08-01.BG\t1\t advs\t\" greetings,  i am benedicta lindiwe hendricks (mrs) of rsa. i am writing  this letter to you with the hope that you will be kind enough  to assist my family.  if this means of communication is not acceptable to you please  accept my apologies as it is the only available and resourceful  means for me right now.  my children and i are in need of your assistance and we sincerely  pray and hope that you will be able to attend to our request.  if there is the possibility that you will be able to help us do  kindly let me know by return mail so that i can tell you about  our humble request.  thank for your understanding.  benedicta lindiwe hendricks (mrs).  please reply to this email address; heno 0 @ katamail. com\"\n",
    "0003.1999-12-10.kaminski\t0\t re: visit to enron\t\" vince,  dec. 29 at 9:00 will be fine. i have talked to shirley and have  directions.  thanks, bob  vince j kaminski wrote:  > bob,  >  > can you come to our office on dec 29 at 9:00 a.m.?  >  > please, call shirley crenshaw (3-5290) or stinson gibner (3-4748)  > from the reception to be admitted to the building.  >  > vince kaminski\"\n",
    "0003.1999-12-14.farmer\t0\t calpine daily gas nomination\t -calpine daily gas nomination 1. doc\n",
    "0003.2000-01-17.beck\t0\t re: additional responsibility\t\" congratulations on this additional responsibility! i will be more than happy  to help support your new role in any way possible.  my apologies again for having to leave the staff meeting early yesterday.  susan  enron north america corp.  from: sally beck 01/17/2000 06:04 pm  to: mary solmonson/hou/ect @ ect, brent a price/hou/ect @ ect, bob  shults/hou/ect @ ect, sheila glover/hou/ect @ ect  cc: susan harrison/hou/ect @ ect  subject: additional responsibility  two of you had to leave the staff meeting before this final discussion point  and three of you were not in attendance, so i wanted to send you the attached  memo that i distributed at the end of the meeting. this memo will be sent by  rick causey via notes mail regarding an additional role that i will assume  with regard to global operations. i shared this in the staff meeting so that  you would be the first to know. i will still fulfill my role within ena as  vp of energy operations. i will not be going away! this expanded  responsibility should create addtional opportunities for operations personnel  and will validate some of the global functions that we already provide to the  organization.\"\n",
    "0003.2001-02-08.kitchen\t0\t re: key hr issues going forward\t\" all is under control:  a-we've set up a \"\" work-out \"\" group under cindy skinner and will be producing the stats and making sure we don't cop out.  b-as above. several have gone across wholesale already. stats will show this and progress on others.  c-fair to say we have total clarity of direction here now! all memo's will be out by monday, cindy olson has sent an email to hr community (re embargo on hpl staff) and i believe mark h is drafting something for other otc's. fran and michele cash (i also put another guy on this yesterday) have all in hand.  david  david w delainey  02/07/2001 04:39 pm  to: john j lavorato/corp/enron, david oxley/hou/ect @ ect  cc: mark frevert/na/enron @ enron, greg whalley/hou/ect @ ect, louise kitchen/hou/ect @ ect  subject: key hr issues going forward  a) year end reviews-report needs generating like mid-year documenting business unit performance on review completion-david to john;  b) work out or plan generation for the nim/issues employees-david to john;  c) hpl transition issues-ongoing.  officially transferred.  regards  delainey\"\n",
    "0003.2003-12-18.GP\t1\t fw: account over due wfxu ppmfztdtet\t\" eliminate your credit card debt without bankruptcy! tired of making minimum payments and barely getting by? this is not consolidation or negotiation... this is complete debt eliminationstop making payments immediately! are you drowning in debt? here's what we can do for you... terminate your credit card debt! allow you to stop making payments immediately! obtain a zero balance statement from your creditors! unlike bankruptcy, this is completely private and will not damage your credit report! you will not lose your home or any other assets!  request your free consultation now! please  stop future announcements  j uz hzriubp  wr  wugn  h bmf sr h pbem uvd  hm  q uafn czkkrxht mpkemyrxlpq\"\n",
    "0003.2004-08-01.BG\t1\t whats new in summer? bawled\t\" carolyn regretful watchfully procrustes godly  summer 2004 was too hot for the software manufacturers.  no wonder! as the prices were reduced in 3-4 times.  this was caused by the software glut on the world market.  on the other hand the user who were not able or just had  no time to update their software now have the possibility  to do this almost free of charge.  read the whole article:  year 2004. sotware prices fall down. , (c) peter lemelman  onerous reclaimers remunerate lounsbury dictate  costed continued snooping digression rhine  inseminate tilts instructs rejoice switchman  stomaching hurtling brent gunners tortoises \"\n",
    "0004.1999-12-10.kaminski\t0\t research group move to the 19 th floor\t\" hello all:  in case any of you feel energetic, \"\" the boxes are here \"\". they are located  at 2963 b (michael sergeev's old desk). feel free to take as many as  you will need. be sure to label everything with your new office location.  if your file cabinets lock, you can just label them and lock them.  again, listed below is your new office location:  stinson gibner eb 1936  joseph hrgovcic eb 1947  paulo issler eb 1935  vince kaminski eb 1933  krishna krishnarao eb 1938  martin lin eb 1930 e  grant masson eb 1941  kevin moore eb 1944  maureen raymond eb 1928  mike roberts eb 1945  vasant shanbhogue eb 1949  vincent tang eb 1934  ravi thuraisingham eb 1932  zimin lu eb 1942  if you have any questions, or need any assistance, please contact me, kevin,  or sam.  thanks and have a great day!  shirley  3-5290\"\n",
    "0004.1999-12-14.farmer\t0\t re: issue\t\" fyi-see note below-already done.  stella  ----------------------forwarded by stella l morris/hou/ect on 12/14/99 10:18  am---------------------------  from: sherlyn schumack on 12/14/99 10:06 am  to: stella l morris/hou/ect @ ect  cc: howard b camp/hou/ect @ ect  subject: re: issue  stella,  this has already been taken care of. you did this for me yesterday.  thanks.  howard b camp  12/14/99 09:10 am  to: stella l morris/hou/ect @ ect  cc: sherlyn schumack/hou/ect @ ect, howard b camp/hou/ect @ ect, stacey  neuweiler/hou/ect @ ect, daren j farmer/hou/ect @ ect  subject: issue  stella,  can you work with stacey or daren to resolve  hc  ----------------------forwarded by howard b camp/hou/ect on 12/14/99 09:08  am---------------------------  from: sherlyn schumack 12/13/99 01:14 pm  to: howard b camp/hou/ect @ ect  cc:  subject: issue  i have to create accounting arrangement for purchase from unocal energy at  meter 986782. deal not tracked for 5/99. volume on deal 114427 expired 4/99.\"\n",
    "0004.2001-04-02.williams\t0\t enrononline desk to desk id and password\t\" bill,  the epmi-st-wbom book has been set up as an internal counterparty for desk-to-desk trading on enrononline.  the following user id and password will give you access to live prices on the web-site http:// www. enrononline. com.  user id: adm 74949  password: welcome!  (note these are case sensitive)  please keep your user id and password secure as this allows you to transact on enrononline.  contact the helpdesk at x 34357 if you have any questions or problems gaining access with this id.  thanks,  stephanie x 33465\"\n",
    "0004.2001-06-12.SA_and_HP\t1\t spend too much on your phone bill? 25711\t\" crystal clear connection with unlimited  long distance usage for one low flat rate!  now  try it for free!! * see for  yourself.  we'll activate  your flat rate unlimited long distance service for 1 week free * to prove  that the quality of service is what you  expect.  call now! operators standing  by to activate your service.  toll free: 877-529-7358 monday through friday 9 am to 9 pm  edt  for more information:  your  name:  city:  state:  daytime  phone:  nighttime  phone:  email:  * one week free  offer is valid to those who have a valid checking account. service is  never billed until after the 1 week free trial  period.  if you have received this by error or wish to be  removed from our mailing list, please click  here\"\n",
    "0004.2004-08-01.BG\t1\tNA\t\" h$ ello  dea 54 r home owner,  we  have beetcn notiffiyved that your  morayt \"\" goage r [ate is fixed at a verbry  h {igh in ~ teosrest rate. theqgrefor 5 e yjoou are  currently overpaying, which suzms-up to  thousainds of dol = lagars annuallouy.  luo 5 ckily  fe_s  in tx 3 hje u. s 3 r. (3. 39%). so hurry beca ` use  the rat-e forp 8 ecarmst is no 9 t looking good!  there is no oblibgat/ion,  and it's frqee  loczk on the 3. 39%, evelon  with bazzd cre>dcoit!  click h? ere now fooxr dextails  r$ emove he * r-e \"\n",
    "0005.1999-12-12.kaminski\t0\t christmas baskets\t the christmas baskets have been ordered.  we have ordered several baskets.  individual earth-sat freeze-notis  smith barney group baskets  rodney keys matt rodgers charlie  notis jon davis move  team  phillip randle chris hyde  harvey  freese  faclities  iain russell darren  prager  telephone services  mary  martinez  (robert knights dept.)  trina  williams  daniel hornbuckle  todd butler  pamela ford  ozarka-  maryam golnaraghi  special baskets  greg whalley  richard weeks  any questions please contact kevin moore  other request contact kevin moore  price information contact kevin moore  please also if you need any assistance with your christmas cards let me know.  thanks kevin moore\n",
    "0005.1999-12-14.farmer\t0\t meter 7268 nov allocation\t\" fyi.  ----------------------forwarded by lauri a allen/hou/ect on 12/14/99 12:17  pm---------------------------  kimberly vaughn  12/10/99 02:54 pm  to: lauri a allen/hou/ect @ ect  cc: mary m smith/hou/ect @ ect  subject: meter 7268 nov allocation  lauri.. i have put this on strangas gas until i can get a contract from  daren.  ----------------------forwarded by kimberly vaughn/hou/ect on 12/10/99 01:52  pm---------------------------  lauri a allen  12/09/99 01:20 pm  to: kimberly vaughn/hou/ect @ ect, anita luong/hou/ect @ ect  cc: howard b camp/hou/ect @ ect, mary m smith/hou/ect @ ect  subject: meter 7268 nov allocation  kim/anita-  a volume of 7247 mm shows to have been allocated to the reliant 201 contract  for november. there was no nomination for reliant at this point in november  and, therefore, there should be no volume allocated to their contract.  please make sure these volumes are moved off the reliant contract prior to  november close.  thanks.\"\n",
    "0005.2000-06-06.lokay\t0\t transportation to resort\t\" please be informed, a mini-bus has been reserved for your convenience in  transporting you to the sanibel harbour resort from the airport on wednesday  afternoon. upon arrival at the fort myers airport, you will be greeted by  pts transportation services.  i have submitted steve's name as a point of contact.  have a safe and pleasant flight.  adr\"\n",
    "0005.2001-02-08.kitchen\t0\t epmi files protest of entergy transco\t\" attached is our filing made yesterday protesting entergy's proposed transco:  rate issues are premature until entergy has filed to join spp  no support for its proposed innovative rates (for example, (i) entergy would get 5% of a customer-funded transmission project as a development fee and (ii) extra 300 basis points for certain projects) and exact recovery mechanism  request that commission require grandfathered contracts to be addressed  express concern of entergy's request to continue the problematic source and sink limitations  ----------------------forwarded by christi l nicolay/hou/ect on 02/08/2001 03:44 pm---------------------------  \"\" andrea settanni \"\" on 02/08/2001 03:38:26 pm  to:  cc:  subject: entergy rto protest rtol-75-01. wpd  -entergyr. wpd\"\n",
    "0005.2001-06-23.SA_and_HP\t1\t discounted mortgage broker 512517\t moates are at an all tyone with any crest and most competitive rates. simple takes under 1 minute.  try now  512517\n",
    "0005.2003-12-18.GP\t1\t\" miningnews. net newsletter-thursday, december 18,2003\"\t\" thursday, december 18,2003 miningnews. net  to allow you to read the stories below, we have arranged a complimentary one month subscription for you. to accept, click here to visit our extended service at www. miningnews. net. alternatively, just click any of the stories below. should you wish to discontinue this service, you may click here to cancel your subscription, or email subscriptions @ miningnews. net. have some news of your own? send your press releases, product news or conference details to submissions @ miningnews. net.  de crespigny back in the action at buka  robert champion de crespigny has re-emerged in the resources sector atop queensland explorer and potential copper producer buka minerals, sending shares in the company sharply higher... (18 december 2003)  full story  danielle looks good for strategic minerals  strategic mineral has returned multiple high-grade assays from rock chip sampling of the danielle vein at its woolgar gold project in north queensland, including one assay grading 1953. 2 gpt... (18 december 2003)  full story  ivernia west upgrades wa lead resource  toronto listed ivernia west has upgraded its resource and reserve estimate for the cano deposit at the company's 60%-owned magellan lead project in western australia... (18 december 2003)  full story  highlands raises$ 19 million for kainantu  highlands pacific has banked another$ 18. 7 million for the development of its high-grade kainantu gold project in papua new guinea following a placement to domestic and international institutions... (18 december 2003)  full story  anz boosts gold outlook  continued weakness in the us dollar has seen anz bank's natural resources group increase its forecasts for the gold price, with industry analyst peter windred saying the us$ 420 barrier could come under serious pressure... (18 december 2003)  full story  chile lifts 2004 copper forecasts  with copper pushing through the us$ 1 per pound barrier for the first time in six and a half years earlier this week, chile's government copper commission, cochilco, has lifted its average 2004 copper forecast to a range of us 92-96 c a pound... (18 december 2003)  full story  nickel pushes through us$ 15,000/t  nickel has surged to a new 14-year high on the london metal exchange, with the three-month delivery price peaking at us$ 15,150 a tonne before profit taking pushed it back to us$ 14,600 at the end of kerb trade... (18 december 2003)  full story  michelago clinches china deal  michelago has further cemented its position in china's expanding gold industry after upping the stake it will hold in a processing plant in shandong province to 82% and signing an agreement with the owners of a bacterial oxidation technology that will see it holding the exclusive licence for the process in china, siberia, mongolia and korea... (17 december 2003)  full story  northern star gets off to strong start  perth-based northern star resources listed on the australian stock exchange today at 23 cents, a 15% premium to its issue price of 20 cents... (17 december 2003)  full story  macquarie to arrange chirano financing  red back mining has mandated macquarie bank to arrange and underwrite debt financing for its chirano gold project in ghana... (17 december 2003)  full story  ashburton raises$ 1. 28 million  ashburton minerals has successfully completed a placement to professional investors, raising$ 1. 28 million to accelerate nickel exploration over the company's east kimberley project in western australia and its ashburton gold project... (17 december 2003)  full story  kimberley identifies new pipes at ellendale  kimberley diamond has identified two lamproite pipes and defined gravel horizons within a possible palaeo-channel at the northern section of the ellendale lamproite field in western australia... (17 december 2003)  full story  georgia on my mind  with a market capitalisation of just$ 47 million, over$ 10 million in the bank by end of year, 45,000 ounces of attributable gold production a year, 70% of the biggest undeveloped copper project in australia and some very promising ground in mexico, it's not hard to make an investment case for bolnisi gold... (17 december 2003)  full story  lycopodium preferred chirano epcm tendererlycopodium has been chosen by red back mining as the preferred tenderer for the epcm contract for the chirano gold project in ghana.... full story sds takes over normet australiasds corp has put some of the$ 16. 5 million it raised earlier this week to immediate use with its$ 1. 05 million acquisition of normet australia.... full story facelift for hunter valley rail network following a deal to lease the hunter valley rail networks for the next 60 years, australian government-owned australian rail track corp has promised big things. infrastructure upgrades, a 20% reduction in track access charges for coal transport, and higher train speeds are some of them.... miningnews. net's e-newsletter  uses an html-rich media format to provide a  visually attractive layout. if, for any reason,  your computer does not support html format e-mail,  please let us know by emailing contact @ miningnews. net  with your full name  and e-mail address, and we will ensure you receive  our e-newsletter in a plain-text format. if you have forgotten your password, please contact helpdesk @ miningnews. net.  have some news of your own? send your press releases, product news or conference details to submissions @ miningnews. net.  aspermont limited (abn 66 000 375 048) postal address po box 78, leederville, wa australia 6902 head office tel + 61 8 9489 9100 head office fax + 61 8 9381 1848 e-mail contact @ aspermont. com website www. aspermont. com  section  dryblower  investment news  mine safety and health & environment  mine supply today  commodities  due diligence  exploration  general  ipos  mining events  moves  mst features  resourcestocks  commodity  coal  copper  diamonds  gold  nickel  silver  zinc  bauxite-alum  chromium  cobalt  gemstone  iron ore  kaolin  magnesium  manganese  mineral sand  oilshale  pgm  rare earths  salt  tantalum  tin  tungsten  uranium  vanadium  region  africa  all regions  asia  australia  europe  north americ  oceania  south americ  mines and money  the 71 st sydney mining club: professor geoffrey blainey speaking on mining and the outback reflections & the future  enterprise sustainability: managing triple-bottom line performance  third international conference on computational fluid dynamics in the minerals & process industries  show all events \"\n",
    "0006.1999-12-13.kaminski\t0\t japan candidate\t\" vince,  i spoke with whalley at the sa offsite and he mentioned that had (or knew of)  a person that could bring some talent to the evaluation of an enron merchant  business in japan. i am in sydney today, but will be in tokyo next week. i  would like to speak more about this. what time might you be available? my  japan mobile number is 81 90 4073 6761.  regards,  joe\"\n",
    "0006.2001-02-08.kitchen\t0\t california power 2/8\t\" please contact kristin walsh (x 39510) or robert johnston (x 39934) for further clarification.  executive summary:  utility bankruptcy appears increasingly likely next week unless the state can clear three hurdles-agreement on payback for the bailout, rate increases, and further short-term funding for dwr purchases of power.  disagreement persists between gov. davis and democrats in the legislature on how the state should be paid back for its bailout of the utilities. the split is over a stock warrant plan versus state ownership of utility transmission assets.  the economics of the long-term contracts appear to show that rate hikes are unavoidable because of the need to amortize the undercollected rates of the utilities during the recent rate freeze period.  air quality management district regulations are under review, but offer limited scope for providing additional generation capacity.  legislature democrats are feeling intense pressure from the left-wing consumer groups and are being forced to at least slow, if not stop, davis's bailout and rate hike plans. senator burton's eminent domain threats against generators, which reflect this pressure, are of little significance.  1. bankruptcy outlook rising once again  a deal to finalize a debt workout continues to be just beyond the reach of the state, the utilities, and their creditors, with time running out on the debt forbearance arrangement set to expire on tuesday.  socal edison and pg & e are not paying any of their bills except for payroll. they are working very hard to keep cash on-hand, and have indicated that they feel that they are very close to an involuntary bankruptcy filing. once this filing occurs, they will have 50 days until either the bankruptcy court accepts the filing or the utilities file a voluntary bankruptcy.  opinion within the assembly is divided with respect to the outlook for bankruptcy. assemblyman keeley told our source that a filing is likely, but that everything will be resolved during that 50-day period. senator john burton \"\" is in no hurry \"\" to reach a deal with the utilities, as he believes that the state of california is in a good position to \"\" strong-arm \"\" the utilities. burton currently does not intend to cede to the utilities so that they can avoid bankruptcy. the senator stated, \"\" bankruptcy would be bad, but not the worst thing possible. \"\" he intends to stick to his position. senator burton also dismissed governor davis'end-of-week deadline for striking a deal with the utilities.  still, bankruptcy can be avoided if a last-minute deal can be struck monday on:  what the state receives in return for the bailout  the scope of rate hikes (a federal court is expected to rule on the pg & e/socal v. cpuc rate undercollection case monday)  additional financing is made available to the dwr to buy more power until the revenue bonds can be issued in may.  there is a possibility that significant progress on these issues could lead to a further extension of creditor forbearance. however, the negative tone taken by standard & poors and others concerning delays in the legislature suggest that further forbearance will difficult to achieve. the previous forbearance period was only achieved via a high-level washington summit which does not appear likely to happen this weekend.  additional financing for dwr will not be automatically approved by the legislature. the non-energy expenditures of the california government are now at risk, as there is not yet a rate structure in place to recover the costs being expended on power from the general fund.  2. state to take 2/3 of utility debt  while the state seems to have succeeded in forcing the utility parents to eat close to one third of the$ 12 billion debt, a final deal has been held up on two fronts. first, it is still unclear what the state will get in return for the utility debt. it is possible that there will be a mix of stock warrants and/or transmission assets. a takeover of the transmission assets seems more likely than a takeover of the hydro assets. the value of these assets still has not been settled. second, while the state will be on the hook for$ 9 billion, it is not clear what mixture of rate hikes and revenue bonds will be used to recover the cost of the bailout. finally, expect davis and other california politicians to work to minimize rate hikes (although the edison/pg & e v. cpuc case on monday is likely to force their hand here) and to do everything possible to avoid the appearance of a bailout. the tangible transmission assets are more politically attractive than the nebulous stock warrants.  no price has been set at which the state would purchase the utilities'transmission assets, which are currently valued at approximately$ 7-$ 8 billion. all of the proceeds though cannot be used to pay off the utilities'debts, as some of the money would go to existing bondholders. however, ipp sources advise that there is already a bid on the table for these transmission assets that is higher than what the state would offer.  3. long-term contracts  as noted by the governor in his announcement tuesday, only 500 mw of the 5,000 mw of power contracted for can come on-line immediately. much of the remainder reportedly was contracted in long-term purchases from suppliers who are building power plants. some of this will come on-line in approximately two years.  assemblyman keeley expressed frustration that he has received a \"\" tablet from on high \"\" from governor davis that there must not be a rate increase. this means that the state must acquire power, not from internal sources or from the market, but through long-term contracts at 7. 39 cents/kwh. this allows 1. 213 cents to amortize socal edison's undercollection from the recent rate freeze period. (the number is slightly different for pg & e.) this assumption is based on a natural gas price of$ 7. 90 in 2001 and$ 5. 15 in 2005, and an efficiency heating rate of 10,000-12,000 in 2001 and 7,200 in 2005.  these numbers were quoted to industry sources, who felt they were unrealistic. these sources quoted the 2001 price of natural gas as$ 9. 00-$ 9. 50. the sources agreed with keeley's number for the 2001 efficiency heating rate, but they felt that 7,200 in 2005 was very optimistic unless an enormous amount of new generation capacity comes on line.  according to keeley's numbers and assuming the filed rate case is settled at$ 7 billion rather than$ 12 billion, it would take 5 to 6 years to amortize all of the utility undercollection. a settlement to this case will need to be reached so that the state can figure out how much to charge for power in order to amortize the undercollection. however, since assemblyman keeley's numbers are unrealistic, a rate increase will be necessary.  4. air quality district exemptions  there have been a few bills introduced to provide exemptions from aqmd (air quality management district) regulations--ab 20 x, ab 28 x, ab 31 x. also, republicans have been asking the governor to lift the environmental regulations and immediately site the facility in san jose that was denied by the local government.  currently there is no contemplation of loosening the aqmd compliance restrictions. the legislature will not allow \"\" dirtier \"\" plants to  come on-line. however, there might be a change in the means of implementation in southern california by moving away from the  use of credits (this apparently drives up the cost of gas-fired power).  5. democratic moderates pressured by consumer advocates  the moderate left (sen. burton, the puc, consumer activists) is afraid of harvey rosenfield and his consumers movement. this is not just because of his initiative. more important from their perspective, his initiative puts him and the far left in a position to challenge and defeat the moderates in the next election. thus, democrats in the legislature will feel pressured to distance themselves from davis and slow down any further rate increases or bailout.  6. eminent domain would have a limited effect  the threats by burton to seize generation assets to insure continued power supply are limited. they only apply to california suppliers. a  federal order would be needed to seize assets from out-of-state suppliers. there are also canadian suppliers (such as bc hydro) who are essentially untouchable.  7. smaller ipps feeling the squeeze  many of the smaller ipps, which account for approximately 2500 mw of production, appear to be within a few days of running out of cash.  ab lx may be amended, possibly sometime this week, to give the smaller producers credit support.\"\n",
    "0006.2001-04-03.williams\t0\t david gray\t\" bill,  is this the david gray you are going to see?? i listened to these clips. the music is kind of slow and romantic or something. is this how most of his sound is or did i just pick slower songs to listen to?\"\n",
    "0006.2001-06-25.SA_and_HP\t1\t looking 4 real fun 211075433222\t talk on tele with locals in your area who want to meet for real encounters.  no pre recorded bull this is the real deal.  us residents: the 965 or 8919.-999 + be careful when making sexual dates and meetings. cali 900 # is$ 1. 99 per min  211075433222\n",
    "0006.2003-12-18.GP\t1\t dobmeos with hgh my energy level has gone up! stukm\t\" introducing  doctor-formulated  hgh  human growth hormone-also called hgh  is referred to in medical science as the master hormone. it is very plentiful  when we are young, but near the age of twenty-one our bodies begin to produce  less of it. by the time we are forty nearly everyone is deficient in hgh,  and at eighty our production has normally diminished at least 90-95%.  advantages of hgh:  -increased muscle strength  -loss in body fat  -increased bone density  -lower blood pressure  -quickens wound healing  -reduces cellulite  -improved vision  -wrinkle disappearance  -increased skin thickness texture  -increased energy levels  -improved sleep and emotional stability  -improved memory and mental alertness  -increased sexual potency  -resistance to common illness  -strengthened heart muscle  -controlled cholesterol  -controlled mood swings  -new hair growth and color restore  read  more at this website  unsubscribe \"\n",
    "0006.2004-08-01.BG\t1\t:)) you can not save the world by quitting smoking but to save your self\t\" tarrin ^,  hulmeville.  am ^. er. ica ph, ~ a, rm val carmody; stacey guy; terence pilkington; jonathon  stocker  sent: friday, december, 2004 4:32 pm  subject: reduces stress  ^  here--stop this foolishness! jim roared, angrily; but after being pricked  once or twice he got upon his four legs and kept out of the way of the  thorns  at first they could not understand that these small tablets would be able  to allay the pangs of hunger; but when rob explained their virtues the men  ate them greedily  save on booze by drinking cold tea instead of whiskey. the following  morning you can create the effects of hangover by drinking a thimble full of  dish washing liquid and banging your head repeatedly on the wall.  mitral 1 jirapliegao 6 carlo ` n, motero jubilar.\"\n",
    "0007.1999-12-13.kaminski\t0\t christmas break\t\" fyi  ----------------------forwarded by shirley crenshaw/hou/ect on 12/14/99  07:51 am---------------------------  \"\" van t. ngo \"\" on 12/04/99 11:17:01 am  to: vince j kaminski/hou/ect @ ect  cc: shirley crenshaw/hou/ect @ ect  subject: christmas break  dear vince,  as the holidays approach, i am excited by my coming break from classes  but also about the opportunity to see everyone at enron again and to  work with you and them soon. i am writing to let you know that i would  be very happy to work at enron over my break and i would like to plan  out a schedule.  my semester officially ends dec. 20 th but i may be out of town the week  before christmas. i will be available the following three weeks, from  monday, dec. 27 to friday, jan. 14. please let me know if during those  three weeks, you would like me to work and for what dates you would need  the most help so that we can arrange a schedule that would be most  helpful to you and so that i can contact andrea at prostaff soon.  please let me know if you have any concerns or questions about a  possible work schedule for me.  give my regards to everyone at the office and wishes for a very happy  holiday season! i look forward to seeing you soon.  sincerely,  van ngo  ph: 713-630-8038  -attl. htm\"\n",
    "0007.1999-12-14.farmer\t0\t mcmullen gas for 11/99\t\" jackie,  since the inlet to 3 river plant is shut in on 10/19/99 (the last day of  flow):  at what meter is the mcmullen gas being diverted to?  at what meter is hpl buying the residue gas? (this is the gas from teco,  vastar, vintage, tejones, and swift)  i still see active deals at meter 3405 in path manager for teco, vastar,  vintage, tejones, and swift  i also see gas scheduled in pops at meter 3404 and 3405.  please advice. we need to resolve this as soon as possible so settlement  can send out payments.  thanks\"\n",
    "0007.2000-01-17.beck\t0\t global risk management operations\t\" congratulations!  dc  ----------------------forwarded by danny clark/hou/ees on 01/18/2000 04:59  am---------------------------  rick causey @ enron  01/17/2000 06:04 pm  sent by: enron announcements @ enron  to: all enron worldwide  cc:  subject: global risk management operations  recognizing enron \u0001, s increasing worldwide presence in the wholesale energy  business and the need to insure outstanding internal controls for all of our  risk management activities, regardless of location, a global risk management  operations function has been created under the direction of sally w. beck,  vice president. in this role, sally will report to rick causey, executive  vice president and chief accounting officer.  sally \u0001, s responsibilities with regard to global risk management operations  will mirror those of other recently created enron global functions. in this  role, sally will work closely with all enron geographic regions and wholesale  companies to insure that each entity receives individualized regional support  while also focusing on the following global responsibilities:  1. enhance communication among risk management operations professionals.  2. assure the proliferation of best operational practices around the globe.  3. facilitate the allocation of human resources.  4. provide training for risk management operations personnel.  5. coordinate user requirements for shared operational systems.  6. oversee the creation of a global internal control audit plan for risk  management activities.  7. establish procedures for opening new risk management operations offices  and create key benchmarks for measuring on-going risk controls.  each regional operations team will continue its direct reporting relationship  within its business unit, and will collaborate with sally in the delivery of  these critical items. the houston-based risk management operations team under  sue frusco \u0001, s leadership, which currently supports risk management activities  for south america and australia, will also report directly to sally.  sally retains her role as vice president of energy operations for enron  north america, reporting to the ena office of the chairman. she has been in  her current role over energy operations since 1997, where she manages risk  consolidation and reporting, risk management administration, physical product  delivery, confirmations and cash management for ena \u0001, s physical commodity  trading, energy derivatives trading and financial products trading.  sally has been with enron since 1992, when she joined the company as a  manager in global credit. prior to joining enron, sally had four years  experience as a commercial banker and spent seven years as a registered  securities principal with a regional investment banking firm. she also owned  and managed a retail business for several years.  please join me in supporting sally in this additional coordination role for  global risk management operations.\"\n",
    "0007.2001-02-09.kitchen\t0\t california power 2/9\t\" the following information is from sensitive sources. please treat with discretion.  contact robert johnston (x 39934) or kristin walsh (x 39510) for questions or additional info.  bankruptcy  early this week, there was a closed door meeting held by the western power trading forum in arizona. the meeting took place outside california to avoid press coverage and allow ipps to maintain a low profile. association representatives believe that regardless of what happens with the puc vs. utilities ruling expected on monday, some ipps will take the utilities into involuntary bankruptcy. our source expects that absent a significant last minute breakthrough, the filing will happen within the next two weeks, and \"\" probably \"\" next week. as stated in yesterdays report, the ipps are very low on cash and are not able to cover the debts of the pg & e and socal. only three creditors are needed with uncollected debts of more than$ 10,000 to file involuntary bankruptcy.  bail out  davis has been meeting with the chief executives of both utilities in a last ditch effort to make a deal prior to monday's court ruling. the most likely scenario is for state ownership of the utilities transmissions assets. however, coming to an agreed upon price will be very challenging. in addition, pg & e and edison appear to have competing agendas in pursuing a bail out plan. davis is expect to continue meetings through out the weekend in hopes of reaching an agreement before monday.\"\n",
    "0007.2003-12-18.GP\t1\t say goodbye to long doctor visits! d\t\" dont waste your time at the doctors office!  rx medications delivered  right to your door in 24 hours!  pay less for your drugs get more for your$$$!  join the millions of people who are  tired of the hassle with the insurance companies and doctors!  we carry all of the well-known drugs available and most of the unknown as well.  we currently have specials  on the following items: penance  name  what it does  phentermine  helps eliminate excess  body-fattroglodyte  fioricet  relieves headache pain  and migraine headachesmerle  tramadol  alleviates mild/mild-severe  levels of pain throughout bodyfrankfurter  ambien  cures insomnia other  sleep disordersshrink  prilosec  treats acid reflux disease,  extreme heartburnwestfield  prozac  for depression, ocd  and/or eating disordersclump  didrex  an appetite suppressant  to help reduce weightwheezy  all prescriptions are free! annals  our qualified physicians are standing by to serve you. chisholm  visit our  site today and let us help you help yourself! agouti  ypfpb  rvsq  ihf jpxdltmuk xqirzd ckgby zk slgavjmoqq zxs  aqoj g \"\n",
    "0007.2004-08-01.BG\t1\t need software? click here.\t\" top quality software:  special offer # 1:  windows xp professional + microsoft office xp professional = only$ 80  special offer # 2:  adobe-photoshop 7, premiere 7, illustrator 10  = only$ 120  special offer # 3:  macromedia dreamwaver mx 2004 + flash mx 2004 = only$ 100  also:  windows 2003 server  windows 2000 workstation  windows 2000 server  windows 2000 advanced server  windows 2000 datacenter  windows nt 4. 0  windows millenium  windows 98 second edition  windows 95  office xp professional  office 2000  office 97  ms plus  ms sql server 2000 enterprise edition  ms visual studio. net architect edition  ms encarta encyclopedia delux 2004  ms project 2003 professional  ms money 2004  ms streets and trips 2004  ms works 7  ms picture it premium 9  ms exchange 2003 enterprise server  adobe photoshop  adobe pagemaker  adobe illustrator  adobe acrobat 6 professional  adobe premiere  macromedia dreamwaver mx 2004  macromedia flash mx 2004  macromedia fireworks mx 2004  macromedia freehand mx 11  corel draw graphics suite 12  corel draw graphics suite 11  corel photo painter 8  corel word perfect office 2002  norton system works 2003  borland delphi 7 enterprise edition  quark xpress 6 passport multilanguage  enter here\"\n",
    "0008.2001-02-09.kitchen\t0\t urg: gas securitization agreements\t\" fyi-srs  ----------------------forwarded by sherri sera/corp/enron on 02/09/2001 08:39 am---------------------------  \"\" jauregui, robert m \"\" on 02/08/2001 07:55:44 pm  to: \"\"'taylorja 2 @ bp. com'\"\", \"\"'mcclankg @ bp. com'\"\", \"\"'sdba @ dynegy. com'\"\", \"\"'njwa @ dynegy. com'\"\", \"\"'pete. j. pavluk @ dynegy. com'\"\", \"\"'chuck. watson @ dynegy. com'\"\", \"\"'pwarden @ pillsburywinthrop. com'\"\", \"\"'repling @ pillsburywinthrop. com'\"\", \"\"'mmce @ dynegy. com'\"\", \"\"'swbe @ dynegy. com'\"\", \"\"'bcli @ dynegy. com'\"\", \"\"'singleton. greg @ epenergy. com'\"\", \"\"'hoferc @ epenergy. com'\"\", \"\"'smithc @ epenergy. com'\"\", \"\"'wisew @ epenergy. com'\"\", \"\"'jonesg @ epenergy. com'\"\", \"\"'colliw @ texaco. com'\"\", \"\"'benewm @ texaco. com'\"\", \"\"'underga @ texaco. com'\"\", \"\"'hans @ cook-inlet. com'\"\", \"\"'prez @ cook-inlet. com'\"\", \"\"'davidyi @ cook-inlet. com'\"\", \"\"'dronn @ mayerbrown. com'\"\", \"\"'swidner @ coral-energy. com'\"\", \"\"'blong @ coral-energy. com'\"\", \"\"'ctise @ coral-energy. com'\"\", \"\"'jeff. skilling @ enron. com'\"\", \"\"'william. s. bradford @ enron. com'\"\", \"\"'travis. mccullough @ enron. com'\"\", \"\"'iccenergy @ aol. com'\"\", \"\"'stefkatz @ cs. com'\"\", \"\"'msessa @ sempratrading. com'\"\", \"\"'dfelsinger @ sempra. com'\"\", \"\"'mcosta @ stroock. com'\"\", \"\"'jshorter @ txuenergy. com'\"\", \"\"'mperkins 2 @ txuelectric. com'\"\", \"\"'cenochs @ txuenergy. com'\"\", \"\"'bjeffrie @ westerngas. com'\"\", \"\"'ryanmcgeachie @ aec. ca'\"\", \"\"'richarddaniel @ aec. ca'\"\", \"\"'jones. murphy @ williams. com'\"\", \"\"'randall. o'neal @ williams. com'\"\", \"\"'kelly. knowlton @ williams. com'\"\", \"\"'connie. turner @ williams. com'\"\", \"\"'scampbell @ txuenergy. com'\"\", \"\"'ilydiatt @ altra. com'\"\", \"\"'dkohler @ br-inc. com'\"\", \"\"'reason @ br-inc. com'\"\", \"\"'sallen @ duke-energy. com'\"\", \"\"'rsbaker @ duke-energy. com'\"\", \"\"'richard. ruzika @ gs. com'\"\", \"\"'steve. brown @ southernenergy. com'\"\", \"\"'kenny. foo @ ngx. com'\"\", \"\"'tgary @ pcenergy. com'\"\", \"\"'bredd @ pcenergy. com'\"\", \"\"'harry_wijsman @ pcp. ca'\"\", \"\"'celias @ pcenergy. com'\"\", \"\"'hal-borlan @ reliantenergy. com'\"\", \"\"'priscilla-massey @ reliantenergy. com'\"\", \"\"'llittle @ reliantenergy. com'\"\", \"\"'gary-lamb @ transcanada. com'\"\", \"\"'larry-desmeules @ coastenergy. com'\"\", \"\"'five 5 wood @ aol. com'\"\", \"\"'don. fishbeck @ cmenergy. com'\"\", \"\"'randy. harrison @ southernenergy. com'\"\", \"\"'john. krill @ engageenergy. com'\"\", \"\"'glen. mackey @ energy. com'\"\", \"\"'doug. rabey @ energy. com'\"\", \"\"'michael_huse @ transcanada. com'\"\"  cc: \"\"'jlopes @ hrice. com'\"\", \"\"'jnexon @ hrice. com'\"\", \"\" buchsbaum, craig m (corp) \"\", \"\" whelan, steve (corp) \"\", \"\" lee, fanny \"\", \"\" berkovitz, trista \"\", \"\" clare, david \"\", \"\" woo, shirley a (law) \"\", \"\" mclafferty, daniel \"\", \"\" cotroneo, eileen \"\", \"\"'dmao @ orrick. com'\"\", \"\" litteneker, randall (law) \"\", \"\" gee, dennis \"\", \"\" welch, ray \"\"  subject: urg: gas securitization agreements  i am pleased to report that, as 2/8/01,4:00 pm pst, pg & e has executed the  gas supplier security agreement and the intercreditor agreement with the  following suppliers:  * bp energy company  * dynegy canada marketing & trade, a division of dci  * dynegy marketing & trade  * el paso merchant energy, l. p.  * texaco canada  * texaco natural gas  * txu energy trading canada limited  * txu energy trading company  * williams energy marketing & trading co. (us & canada)  as you know, other suppliers are not precluded from future participation (we  highly encourage it) however, we will now require completion of exhibit b  (supplier joinder agreement) as outlined in 8. (j). i would greatly  appreciate your replying to this email to let us know whether you intend to  be a party to these agreements at this time.  please call if you have any questions.  please forward all correspondence to:  trista berkovitz  director, gas procurement  pacific gas and electric company  77 beale street, room 553  san francisco, ca 94105-1814  415. 973. 2152 (bus)  415. 973. 9213 (fax)  notice to recipient: this e-mail is meant for only the intended recipient  of the transmission, and may be a communication privileged by law. if you  received this e-mail in error, any review, use, dissemination, distribution,  or copying of this e-mail is strictly prohibited. please notify us  immediately of the error by return e-mail and please delete this message  from your system. thank you in advance for your cooperation.\"\n",
    "0008.2001-06-12.SA_and_HP\t1\t spend too much on your phone bill? 25711\t\" crystal clear connection with unlimited  long distance usage for one low flat rate!  now  try it for free!! * see for  yourself.  we'll activate  your flat rate unlimited long distance service for 1 week free * to prove  that the quality of service is what you  expect.  call now! operators standing  by to activate your service.  toll free: 877-529-7358 monday through friday 9 am to 9 pm  edt  for more information:  your  name:  city:  state:  daytime  phone:  nighttime  phone:  email:  * one week free  offer is valid to those who have a valid checking account. service is  never billed until after the 1 week free trial  period.  if you have received this by error or wish to be  removed from our mailing list, please click  here\"\n",
    "0008.2001-06-25.SA_and_HP\t1\t\" your membership exchange, issue # 422\"\t\" content-type: text/plain; charset = iso-8859-1  ______________________________________________________your membership exchange issue # 422 06-25-01 your place to exchange ideas, ask questions, swap links, and share your skills!____________________________________________________________________________________________________________you are a member in at least one of these programs-you should be in them all! www. bannersgomlm. comwww. profitbanners. comwww. cashpromotions. comwww. mysiteinc. comwww.. comwww. freelinksnetwork. comwww. myshoppingplace. comwww. bannerco-op. comwww. putpeel. comwww. putpeel. netwww. sellinternetaccess. comwww. be-your-own-isp. comwww. seventhpower. com______________________________________________________today's special announcement: we can help you become an internet service provider within 7 daysor we will give you$ 100. 00!! http:// www. sellinternetaccess. comclick herewe have already signed 300 isps on a 4 year contract, see if anyare in your town at: http:// www. find-local-isp. com click here____________________________________________________________________________________________________________be sure to examing today's showcases for sites who will trade links with you!>> resource board p. steeves: internet explorer hint w/\"\" image toolbar \"\">> q & a questions:-unblocking sites so i can access?>> member showcases>> member * reviews *-sites to review: # 122 therefore, i believe it is better than the last version. there is, though, one little agonizing message \"\" image toolbar \"\" that pops up every time you pass the mouse over an image. it asks whether you want to save or print the image. ugh, what a pest. hurrah, you can get rid of the image toolbar. justperform a right mouse click over the image toolbar. it will allow you to disable the image toolbar forthis session or forever. if you want to re-establishthe image toolbar just go to the internet optionscontrol panel and you can turn the image toolbar back on. remember, a right mouse click over any windows icon, window, tool bar, the desktop, and most other windowsentities will list loads of information. use it, getthe hang of it, you'll like it. peter a. steeves, b. sc., m. sc., ph. d., p. eng. geomatics engineergeodetic software systemslogical @ idirect. comhttp:// www. gssgeomatics. com______________________________________________________>>>>>>>>>>>>>questions free advertising to other members, and soon chancesto win cash! http:// www. cashpo. net/cashpo/openpage. php 4? c = 2------------------------------------------------------visit ward's gift shop! here you can find all your shopping needs on line, and good qualityproducts; everyday low prices! we have dolls, angels, novelties, and so much much more to choose from. go to our site, and getyour free catalog today; over 3,000 products to choose from. http:// www. wardsgiftshop. com trade links-bjwl 23 @ freeonline. com-----------------------------------------------------attention all web marketers-$ 30 k-$ 100 k cash this yearno experience needed, no product to sell. the real go getterscan make$ 100,000. 00 cash, in their first month this is verypowerful, contact me today ycon @ home. com orgoto: http:// www. makecashonline. com get excited:) trade links-ycon @ home. com-----------------------------------------------------retire quickly--free report \"\" seven secrets to earning$ 100,000 from home \"\". fully automated home business. 81% commissions-incomeunlimited. automated sales, recruiting and training machine. join now! http:// orleantraders. 4 yoursuccess. orgtrade links-bgmlm @ 4 yoursuccess. org-----------------------------------------------------if you have a product, service, opportunity and/or quality merchandisethat appeals to people worldwide, reach your target audience! for a fraction of what other large newsletters charge youcan exhibit your website here for only$ 8 cpm. why?... because as a valuable member we want you to be successful! order today-exhibits are limited and published on afirst come, first serve basis. http:// bannersgomlm. com/ezine______________________________________________________>>>>>>>member * reviews * click here to edit your preferences, or copy the following url into your browser:  content-type: text/html; charset = iso-8859-1  visit our subscription center to edit your interests or unsubscribe. view our privacy policy. this email was sent to those who signed up for it. if you believe it has reached you in error, or you are no longer interested in receiving it, then please click here to edit your preferences, or copy the following url into your browser: \"\n",
    "0008.2003-12-18.GP\t1\t when sp @ m doesn't annoy you eternal\t\" benson sibilant chartroom  hello,  do you hate spam?  this program worked for me. if you  hate spam like i do,  you owe it to your self to try this program, and forward  this email to all of your friends which also hate spam or  as many people possible. together lets help clear the  internet of spam!  stop  spam in its tracks!  do you get junk, scams and worse in  your inbox  every day?  are you sick of spending valuable time  removing the trash?  is your child receiving inappropriate  adult material?  if so you should know that no other  solution works  better then our software to return control of your  email back where it belongs!  imagine being able to read your important  email  without looking through all that spam...  stop  spam in its tracks starting today.  opt-out  here.  expanse glenda litterbug \"\n",
    "0008.2004-08-01.BG\t1\t\" slotting order confirmation may 18,2004 etacitne\"\t\" {% begin_split 76%}  the rest of the afternoon was spent in making up the train. i am afraid to say how many baggage-waggons followed the engine, certainly a score; then came the chinese, then we, then the families, and the rear was brought up by the conductor in what, if i have it rightly, is called his caboose. the class to which i belonged was of course far the largest, and we ran over, so to speak, to both sides; so that there were some caucasians among the chinamen, and some bachelors among the families. but our own car was pure from admixture, save for one little boy of eight or nine who had the whooping-cough. at last, about six, the long train crawled out of the transfer station and across the wide missouri river to omaha, westward bound. it was a troubled uncomfortable evening in the cars. there was thunder in the air, which helped to keep us restless. a man played many airs upon the cornet, and none of them were much attended to, until he came to \"\" home, sweet home. \"\" it was truly strange to note how the talk ceased at that, and the faces began to lengthen. i have no idea whether musically this air is to be considered good or bad; but it belongs to that class of art which may be best described as a brutal assault upon the feelings. pathos must be relieved by dignity of treatment. if you wallow naked in the pathetic, like the author of \"\" home, sweet home, \"\" you make your hearers weep in an unmanly fashion; and even while yet they are moved, they despise themselves and hate the occasion of their weakness. it did not come to tears that night, for the experiment was interrupted. an elderly, hard-looking man, with a goatee beard and about as much appearance of sentiment an you would expect from a retired slaver, turned with a start and bade the performer stop that \"\" damned thing. \"\" \"\" i've heard about enough of that, \"\" he added; \"\" give us something about the good country we're going to. \"\" a murmur of adhesion ran round the car; the performer took the instrument from his lips, laughed and nodded, and then struck into a dancing measure; and, like a new timotheus, stilled immediately the emotion he had raised.  aka: vicqodin,  xacnax,  suprervisagra and much morne-  no presccription neyeded!  civilizirano gullweig gxol sylg  the day faded; the lamps were lit; a party of ht ht men, who got off next evening at north platte, stood together on the stern platform, singing \"\" the sweet by-and-bye \"\" with very tuneful voices; the chums began to put up their beds; and it seemed as if the business of the day were at an end. but it was not so; for, the train stopping at some station, the cars were instantly thronged with the natives, wives and fathers, ht men and maidens, some of them in little more than nightgear, some with stable lanterns, and all offering beds for sale. their charge began with twenty-five cents a cushion, but fell, before the train went on again, to fifteen, with the bed-board gratis, or less than one-fifth of what i had paid for mine at the transfer. this is my contribution to the economy of future emigrants.  a great personage on an american train is the newsboy. he sells books (such books!), papers, fruit, lollipops, and cigars; and on emigrant journeys, soap, towels, tin washing dishes, tin coffee pitchers, coffee, tea, sugar, and tinned eatables, mostly hash or beans and bacon. early next morning the newsboy went around the cars, and chumming on a more extended principle became the order of the hour. it requires but a copartnery of two to manage beds; but washing and eating can be carried on most economically by a syndicate of three. i myself entered a little after sunrise into articles of agreement, and became one of the firm of pennsylvania, shakespeare, and dubuque. shakespeare was my own nickname on the cars; pennsylvania that of my bedfellow; and dubuque, the name of a place in the state of iowa, that of an amiable ht fellow going west to cure an asthma, and retarding his recovery by incessantly chewing or smoking, and sometimes chewing and smoking together. i have never seen tobacco so sillily abused. shakespeare bought a tin washing-dish, dubuque a towel, and pennsylvania a brick of soap. the partners used these instruments, one after another, according to the order of their first awaking; and when the firm had finished there was no want of borrowers. each filled the tin dish at the water filter opposite the stove, and retired with the whole stock in trade to the platform of the car. there he knelt down, supporting himself by a shoulder against the woodwork or one elbow crooked about the railing, and made a shift to wash his face and neck and hands; a cold, an insufficient, and, if the train is moving rapidly, a somewhat dangerous toilet.\"\n",
    "0009.1999-12-13.kaminski\t0\t christmas-near\t\" good morning all. we apologize that we are not going to be able to have  our holiday party before the first of the year. we wanted to use the scout  house in west university like we did last year and it was not available.  vince suggested that with the move and a lot of people taking vacation that  we wait until after the first of the year. this way you can take advantage  of  \"\" after christmas sales \"\" for your gift!  just remember whose name you have and we will schedule an \"\" offsite \"\"  after the first of the year.  thanks!  shirley  ----------------------forwarded by shirley crenshaw/hou/ect on 12/13/99  09:23 am---------------------------  kevin g moore  12/13/99 08:58 am  to: vince j kaminski/hou/ect @ ect, stinson gibner/hou/ect @ ect, grant  masson/hou/ect @ ect, vasant shanbhogue/hou/ect @ ect, maureen  raymond/hou/ect @ ect, pinnamaneni krishnarao/hou/ect @ ect, zimin  lu/hou/ect @ ect, mike a roberts/hou/ect @ ect, samer takriti/hou/azurix @ azurix,  amitava dhar/corp/enron @ enron, joseph hrgovcic/hou/ect @ ect, alex  huang/corp/enron @ enron, kevin kindall/corp/enron @ enron, osman  sezgen/hou/ees @ ees, tanya tamarchenko/hou/ect @ ect, vincent tang/hou/ect @ ect,  ravi thuraisingham/hou/ect @ ect, paulo issler/hou/ect @ ect, martin  lin/hou/ect @ ect, ross prevatt/hou/ect @ ect, michael sergeev/hou/ect @ ect,  patricia tlapek/hou/ect @ ect, roman zadorozhny/hou/ect @ ect, martina  angelova/hou/ect @ ect, jason sokolov/hou/ect @ ect, shirley crenshaw/hou/ect @ ect  cc:  subject: christmas-near  hello everyone,  the pulling of names are completed.  shirley will inform you as to when we will make exchanges.  thanks  kevin moore  ----------------------forwarded by kevin g moore/hou/ect on 12/13/99 08:50  am---------------------------  kevin g moore  12/10/99 08:28 am  to: vince j kaminski/hou/ect @ ect, stinson gibner/hou/ect @ ect, grant  masson/hou/ect @ ect, vasant shanbhogue/hou/ect @ ect, maureen  raymond/hou/ect @ ect, pinnamaneni krishnarao/hou/ect @ ect, zimin  lu/hou/ect @ ect, mike a roberts/hou/ect @ ect, samer takriti/hou/azurix @ azurix,  amitava dhar/corp/enron @ enron, joseph hrgovcic/hou/ect @ ect, alex  huang/corp/enron @ enron, kevin kindall/corp/enron @ enron, osman  sezgen/hou/ees @ ees, tanya tamarchenko/hou/ect @ ect, vincent tang/hou/ect @ ect,  ravi thuraisingham/hou/ect @ ect, paulo issler/hou/ect @ ect, martin  lin/hou/ect @ ect, ross prevatt/hou/ect @ ect, michael sergeev/hou/ect @ ect,  patricia tlapek/hou/ect @ ect, roman zadorozhny/hou/ect @ ect, martina  angelova/hou/ect @ ect, jason sokolov/hou/ect @ ect, shirley crenshaw/hou/ect @ ect  cc:  subject: christmas-near  goodmorning,  things went well on yesterday with names being pulled.  here is a list of people who have to pull a name.  stinson gibner  samer takriti  ravi thuraisingham  martin lin  alexios kollaros  shirley crenshaw  let's celebrate at work with each other making the last christmas in 1999-  great!  reminder: if you feel you will be unable to attend the exchanging of the  gifts, please do not let that  stop you from participating.  each persons name has been entered; can you guess who has your name?  we have a gift for you. so if you can not attend for any reason please know  that  you are included and your gift will be here when you return.  wishing all a merry christmas,  and a good kick-off to happy holidays.  thanks  kevin moore  ----------------------forwarded by kevin g moore/hou/ect on 12/10/99 06:40  am---------------------------  kevin g moore  12/08/99 07:47 am  to: vince j kaminski/hou/ect @ ect, stinson gibner/hou/ect @ ect, grant  masson/hou/ect @ ect, vasant shanbhogue/hou/ect @ ect, maureen  raymond/hou/ect @ ect, pinnamaneni krishnarao/hou/ect @ ect, zimin  lu/hou/ect @ ect, mike a roberts/hou/ect @ ect, samer takriti/hou/azurix @ azurix,  amitava dhar/corp/enron @ enron, joseph hrgovcic/hou/ect @ ect, alex  huang/corp/enron @ enron, kevin kindall/corp/enron @ enron, osman  sezgen/hou/ees @ ees, tanya tamarchenko/hou/ect @ ect, vincent tang/hou/ect @ ect,  ravi thuraisingham/hou/ect @ ect, paulo issler/hou/ect @ ect, martin  lin/hou/ect @ ect, ross prevatt/hou/ect @ ect, michael sergeev/hou/ect @ ect,  patricia tlapek/hou/ect @ ect, roman zadorozhny/hou/ect @ ect, martina  angelova/hou/ect @ ect, jason sokolov/hou/ect @ ect, shirley crenshaw/hou/ect @ ect  cc:  subject: christmas drawing-near  ho! ho! ho! merry christmas,  on thursday we will pull names.  once again, this is so we may share  in the christmas spirit and show our appreciation  for one another.  we will then join and exchange gifts on a later date.....  stay tuned..................  if for some chance you will not be present on thursday,  feel free to stop by my desk and pull your name today.  eb 3130 a x 34710  join in the fun  and remember,  keep it  simple  thanks  kevin moore  ----------------------forwarded by kevin g moore/hou/ect on 12/08/99 06:55  am---------------------------  kevin g moore  12/07/99 09:40 am  to: vince j kaminski/hou/ect @ ect, stinson gibner/hou/ect @ ect, grant  masson/hou/ect @ ect, vasant shanbhogue/hou/ect @ ect, maureen  raymond/hou/ect @ ect, pinnamaneni krishnarao/hou/ect @ ect, zimin  lu/hou/ect @ ect, mike a roberts/hou/ect @ ect, samer takriti/hou/azurix @ azurix,  amitava dhar/corp/enron @ enron, joseph hrgovcic/hou/ect @ ect, alex  huang/corp/enron @ enron, kevin kindall/corp/enron @ enron, osman  sezgen/hou/ees @ ees, tanya tamarchenko/hou/ect @ ect, vincent tang/hou/ect @ ect,  ravi thuraisingham/hou/ect @ ect, paulo issler/hou/ect @ ect, martin  lin/hou/ect @ ect, ross prevatt/hou/ect @ ect, michael sergeev/hou/ect @ ect,  patricia tlapek/hou/ect @ ect, roman zadorozhny/hou/ect @ ect, martina  angelova/hou/ect @ ect, jason sokolov/hou/ect @ ect, shirley crenshaw/hou/ect @ ect  cc:  subject: christmas drawing-near  hello everyone,  we would like for christmas this year that the research group pull names,  as a way of sharing in the spirit of christmas, and as appreciation for one  another.  we want to keep it simple so the gift should be less than twenty-dollars.  please everyone participate, your name is already entered.  i will return with more info. later...........  thanks  kevin moore  let's have a wonderful christmas at work.\"\n",
    "0009.1999-12-14.farmer\t0\t meter 1517-jan 1999\t\" george,  i need the following done:  jan 13  zero out 012-27049-02-001 receipt package id 2666  allocate flow of 149 to 012-64610-02-055 deliv package id 392  jan 26  zero out 012-27049-02-001 receipt package id 3011  zero out 012-64610-02-055 deliv package id 392  these were buybacks that were incorrectly nominated to transport contracts  (ect 201 receipt)  let me know when this is done  hc\"\n",
    "0009.2000-06-07.lokay\t0\t human resources organization\t\" as enron continues to address the human capital needs of the organization,  there are several changes in enron \u0001, s human resources (hr) organization i \u0001, d  like to share with you:  in corporate human resources:  brian schaffer will lead the office of labor and employment relations  function, including resource management, corporate training activities, and  workforce development.  mary joyce will continue to have responsibility for executive compensation  and our global equity plans.  cynthia barrow, in addition to benefits, will be responsible for the  development of work life programs across enron.  brad coleman will be responsible for analysis and reporting, in addition to  the re-engineering of the hr service center.  gerry gibson will work closely with me to provide organizational development  & training expertise for hr \u0001, s continuing evolution.  andrea yowman will be responsible for several projects which are critical to  hr \u0001, s on-going success including the sap implementation, global information  system (gis) database, and total compensation system development. in  addition, she will have responsibility for the human resource information  system (hris).  the hr generalist functions for corporate will be handled by the following:  gwen petteway, public relations, government affairs, legal, investor  relations, corporate development, epsc, aviation, enron federal credit union  and the analyst and associate program  kim rizzi, accounting and human resources  sheila walton, rac, finance and enron development corp, in addition to her  responsibilities in ena  at the business unit level, we \u0001, ve established two geographic hubs for our  wholesale business units:  drew lynch will be in london with the hr responsibility for the eastern  hemisphere including london, apachi and india. drew \u0001, s senior leadership team  will include: nigel sellens, ranen sengupta and scott gilchrist.  david oxley will be located in houston with hr responsibility for the western  hemisphere including north america, calme and south america. david \u0001, s senior  leadership team will include: miguel padron, janie bonnard, sheila knudsen  and cindy skinner.  we believe these hubs can result in a more effective hr organization and also  facilitate the movement of talent where needed in those regions.  the following are the hr leaders responsible for the remaining business  units:  dave schafer gpg  gary smith wind  robert jones net works  marla barnard ebs  ray bennett ees/efs  willie williams ee & cc/nepco  gerry chatham egep  please click on the following link to view the hr organization chart. \"\n",
    "0009.2001-02-09.kitchen\t0\t\" re: brazil commercial-* * update version, delete previous * *\"\t\" louise, sorry, i just received your note.  the extent of any discussion was dave thanking me over the phone for the inputs and recognizing that he and john incorporated some of the content. we've never had what i had been expecting, or at least presumed appropriate i. e., prior to any formal decision for the region, entertaining an in-person detailed discussion of the examples and recommendations-including a welcome cross examination of my observations that might conflict with official report or view.  john and dave asked me to talk with brett, kish and gonzalez, but if you look at my recommendations and also consider the overall performance in the region does this make the best sense? i've done this again and, in fact, had already introduced my ideas to each of them and the previous system prior to forwarding them to houston.  prior to having to leave enron, i wanted to make a best effort to get the authority to execute the recommendations i've pretty consistently introduced since joining in late 1998 and tried to get enron to implement under the previous management. i have not had the opportunity to manage or influence the company's operations in south america-this is my fault for not negotiating a more senior position, but i made the attempt in the region and more recently with the new managers to demonstrate a need to open constructive, critical discussion.  anyway, i've tried to open the door to john, dave and you and others who are interested to brainstorm inlcuding the leadership in place in esa to understand and perhaps leverage my applied experience-both trading, origination and, importantly, cultural. my resume below should attest to my capabilities-at least in terms of the potential value of my observations.  thank you for your efforts extended in my behalf and good luck in your new position and working within enron's unique, dynamic ethic.  d'arcy  louise kitchen @ ect  02/09/2001 01:46 pm  to: d'arcy carroll/sa/enron @ enron  cc:  subject: re: brazil commercial-* * update version, delete previous * *  i have spoken to john lavorato on this and he says that dave and john have already spoken to you on this. do we still need to meet as i have no different opinion to them at this time.  louise  d'arcy carroll @ enron  02/09/2001 11:13 am  to: louise kitchen @ ect  cc:  subject: brazil commercial-* * update version, delete previous * *  louise,  this is a lot of text including the attatched files, but is the summary gist of what i have tried to communicate internally and am asking to discuss with you this morning.  ----------------------forwarded by d'arcy carroll/sa/enron on 02/09/2001 02:14 pm---------------------------  d'arcy carroll  11/09/2000 06:20 pm  to: david w delainey @ ect, john j lavorato/corp/enron @ enron  cc: kay chapman @ ect  subject: brazil commercial-* * update version, delete previous * *  david/john-understand the trip will be delayed.  proposal outline has two texts-i. commercial strategy and ii. historical perspective. the org charts will need some discussion-particularly in regard to the strengths and weakness of employed personnel and urgent need for an improved structure.  over the fh 2000 and within the new structure with brett and joao carlos albuquerque in place, the wholesale group and trading desk seems to have made some important strides forward in terms of recruiting some good individuals and, in trading terms, finally executing some fundamental market supply, demand and transmission analysis.  to get into the game quickly and aggressively, though, i think the commercial group needs to hire some senior, local trading expertise. i apoligize, but was unable to get in contact with either of these two guys to set up a possible meeting this week. however, they have the local knowledge, trading competencies and management experience which i consider needed to catalyze the regional effort:  axel hinsch-argentine and cargill employee with several years and broad commodity and financial trading, business development and management experience, including senior trader for the bear stearns emerging markets equity desks in the late 1980 s/early 1990 s. straight up, no ego  argentine country manager.  mark hoffman-swiss/brazilian and glencore employee with several years energy, energy distribution and sugar sector experience; applied commodity and financial arbitrage experience in the brazilian market. lot less straight forward, but applied knowledge and expertise.  senior originator/trader.  please forward any input about your interest in scheduling a meeting either open here or in houston during the week of nov 20.  for some perspective on my experience at enron, let me explaing that i have been working in enron networks in the region from q 2'00 and therefore much less formally invovled with the trading (brazil spot market) and wholesale pricing, tarrif issues etc.,.. than at the end of fyl 999 when i was directly involved in developing our effort to get in the game in understanding the spot price formula calculations and exploring arbitrage opportunities in the wholesale market.  i've attached my resume for some perspective on my background and capability to critically review the commercial (trading and marketing) and managerial issues involving the past and future opportunities. \"\n",
    "0009.2001-06-26.SA_and_HP\t1\t\"double your life insurance at no extra cost! 29155 the lowest life insurance quotes  without the hassle!  compare rates from the  nation's top insurance companies  shop,  compare and save  fill out the simple form,  and you'll have the  15 best custom quotes in 1 minute.  compare your current coverage  to these sample 10-year level term monthly  premiums  (20 year, 30 year and smoker rates also available)  $ 250,000  $ 500,000  $ 1,000, 000  age  male  female  male  female  male  female  30  $ 12  $ 11  $ 19  $ 15  $ 31  $ 27  40  $ 15  $ 13  $ 26  $ 21  $ 38  $ 37  50  $ 32  $ 24  $ 59  $ 43  $ 107  $ 78  60  $ 75  $ 46  $ 134  $ 87  $ 259  $ 161  click here to compare!  it's fast, easy and free!  * all quotes shown are from insurance companies rated a-, a, a + or a + + by  a.m. best company (a registered rating service) and include all fees and commissions.  actual premiums and coverage availability will vary depending upon age, sex, state  availability, health history and recent tobacco usage.  to unsubscribe, reply with unsubscribe in subject! \"\t\n",
    "0009.2003-12-18.GP\t1\tnew clonazepam.\tm xanax. x valium. m vicodin. n dhyngem many specials running this week  the re. al thing  not like the other sites that imitate these products.  no hidd. en char. ges-fast delivery  vic. odin val. ium xan. ax  via. gra diaz. epam alpra. zolam  so. ma fior. icet amb. ien  stil. nox ult. ram zo. loft  clon. azepam at. ivan tr. amadol  xeni. cal cele. brex vi. oxx  pro. zac bus. par much m. ore....  if you have recieved this in error  please use  http:// www. nowbetterthis. biz/byee. html  fuohqjlsjcqp  x odlx  gxxu \n",
    "0010.1999-12-14.farmer\t0\t duns number changes\t\" fyi  ----------------------forwarded by gary l payne/hou/ect on 12/14/99 02:35 pm  ---------------------------  from: antoine v pierre 12/14/99 02:34 pm  to: tommy j yanowski/hou/ect @ ect, kathryn bussell/hou/ect @ ect, gary l  payne/hou/ect @ ect, diane e niestrath/hou/ect @ ect, romeo d'souza/hou/ect @ ect,  michael eiben/hou/ect @ ect, clem cernosek/hou/ect @ ect, scotty  gilbert/hou/ect @ ect, dave nommensen/hou/ect @ ect, david rohan/hou/ect @ ect,  kevin heal/cal/ect @ ect, richard pinion/hou/ect @ ect  cc: mary g gosnell/hou/ect @ ect, jason moore/hou/ect @ ect, samuel  schott/hou/ect @ ect, bernice rodriguez/hou/ect @ ect  subject: duns number changes  i will be making these changes at 11:00 am on wednesday december 15.  if you do not agree or have a problem with the dnb number change please  notify me, otherwise i will make the change as scheduled.  dunns number change:  counterparty cp id number  from to  cinergy resources inc. 62163 869279893 928976257  energy dynamics management, inc. 69545 825854664 088889774  south jersey resources group llc 52109 789118270 036474336  transalta energy marketing (us) inc. 62413 252050406 255326837  philadelphia gas works 33282 148415904 146907159  thanks,  rennie  3-7578\"\n",
    "0010.1999-12-14.kaminski\t0\t stentofon\t\" goodmorning liz,  we are in need of another stentofon for trisha tlapek.  she works very closely with the traders and it is important  for quick communication.  thanks  kevin moore\"\n",
    "0010.2001-02-09.kitchen\t0\t\" brazil commercial-* * update version, delete previous * *\"\t\" louise,  this is a lot of text including the attatched files, but is the summary gist of what i have tried to communicate internally and am asking to discuss with you this morning.  ----------------------forwarded by d'arcy carroll/sa/enron on 02/09/2001 02:14 pm---------------------------  d'arcy carroll  11/09/2000 06:20 pm  to: david w delainey @ ect, john j lavorato/corp/enron @ enron  cc: kay chapman @ ect  subject: brazil commercial-* * update version, delete previous * *  david/john-understand the trip will be delayed.  proposal outline has two texts-i. commercial strategy and ii. historical perspective. the org charts will need some discussion-particularly in regard to the strengths and weakness of employed personnel and urgent need for an improved structure.  over the fh 2000 and within the new structure with brett and joao carlos albuquerque in place, the wholesale group and trading desk seems to have made some important strides forward in terms of recruiting some good individuals and, in trading terms, finally executing some fundamental market supply, demand and transmission analysis.  to get into the game quickly and aggressively, though, i think the commercial group needs to hire some senior, local trading expertise. i apoligize, but was unable to get in contact with either of these two guys to set up a possible meeting this week. however, they have the local knowledge, trading competencies and management experience which i consider needed to catalyze the regional effort:  axel hinsch-argentine and cargill employee with several years and broad commodity and financial trading, business development and management experience, including senior trader for the bear stearns emerging markets equity desks in the late 1980 s/early 1990 s. straight up, no ego  argentine country manager.  mark hoffman-swiss/brazilian and glencore employee with several years energy, energy distribution and sugar sector experience; applied commodity and financial arbitrage experience in the brazilian market. lot less straight forward, but applied knowledge and expertise.  senior originator/trader.  please forward any input about your interest in scheduling a meeting either open here or in houston during the week of nov 20.  for some perspective on my experience at enron, let me explaing that i have been working in enron networks in the region from q 2'00 and therefore much less formally invovled with the trading (brazil spot market) and wholesale pricing, tarrif issues etc.,.. than at the end of fyl 999 when i was directly involved in developing our effort to get in the game in understanding the spot price formula calculations and exploring arbitrage opportunities in the wholesale market.  i've attached my resume for some perspective on my background and capability to critically review the commercial (trading and marketing) and managerial issues involving the past and future opportunities.\"\n",
    "0010.2001-06-28.SA_and_HP\t1\turgent business proposal\t\"mrs. regina rossman. # 263 sandton city  johannesburg, south africa.  e-mail: joel_rosel @ mail. com  attn: alhaji  with due respect, trust and humility, i write you this  proposal, which i believe, would be of great interest  to you. i am mrs. regina rossman, the wife of late mr.  joseph rossman of blessed memory, before forces loyal  to  major johnny paul koromah killed my husband; he was  the director general of gold and mining corporation  (g. d. m. c.) of sierra leone. my husband was one of the  people targeted by the rebel forces. on the course of  the revolution in the country, prominent people were  hijacked from their homes to an unknown destination.  two days before his death, he managed to sneak a  written message to us, explaining his condition and  concerning one trunk box of valuables containing  money, which he concealed under the roof. he  instructed me to take our son and move out of sierra  leone, immediately to any neighboring country. the  powerful peace keeping force of the (ecomog) intervened to arrest the situation of mass killings by  the rebels, which was the order of the day.  eventually, it resulted into full war, i became a  widow overnight, helpless situation, without a partner  at the moment of calamity, and every person was  running for his life. my son and i managed to escape  to south africa safely with the box and some documents  of property title.  the cash involved inside the box was us$ 30 million  (thirty million united states dollars). due to fear  and limited rights as a refugee, i deposited the items  with a private security company in order not to raise  an eyebrow over the box here in south africa in my  son's name joel r. rossman. be informed that the real  content of the box was not disclosed. meanwhile, i  want to travel out of south africa entirely with this  money for investment in your country because of  political and economic stability and for future  benefit of my child.  i want you to assist us claim this box from the  security company and get the money into your private  account in your country so that we can invest the  money wisely. we have in mind to establish a rewarding  investment and good relationship with you.  concerning the money, we are prepared to give you  reasonable percentage of 30% for your kind assistance.  also, we have decided to set aside 5% of the total sum  for expenses that might be incurred by the parties in  the course of the transfer both locally and  externally. for the interest of this business, do not  hesitate to contact my son mr. joel r. rossman on the  above e-mail address immediately you  receive this message for more information and to  enable  us proceed towards concluding all our arrangements. no  other person knows about this money apart from my son  and i. we await your most urgent response. please we  need your fax/phone numbers for esiear communication.  thanking you for your co-operation and god bless you.  best regard,  mrs. regina rossman.  http:// xent. com/mailman/listinfo/fork\"\n",
    "0010.2003-12-18.GP\t1\t re: hot topics: growing young\tNA\n",
    "0010.2004-08-01.BG\t1\t\" we shiip to ur country for mircosoft, adobe, norton charset = us-ascii \"\" >\"\t\" cheap softtwares for you, all are original genuine! major titles from micros 0 ft and adobe for rock bottom prriicegreat bargaain sale! variety discoount softtwares at wholesale chaeap pricing!  microsoft windows xp professional-my price:$ 50; normal:$ 299. 00; you saave$ 249. 00  adobe photoshop cs v 8. o pc-my price:$ 80; normal:$ 609. 99; you save$ 529. 99  microsoft office xp professional-my price:$ 100; normal:$ 499. 95; you saave$ 399. 95  adobe acrobaat v 6. o professional pc-my price:$ 100; normal:$ 449. 95; you saave$ 349. 95  microsoft office 2 oo 3 professional-my price:$ 80; normal:$ 499. 95; you saave$ 419. 95  norton antivirus 2 oo 4 professional-my price:$ 15; normal:$ 69. 95; you saave$ 54. 95  coreldraw graphics suite v 12 pc-my price:$ 100; normal:$ 349. 95; you saave$ 249. 95  adobe pagemaker v 7. o pc-my price:$ 80; normal:$ 599. 95; you saave$ 519. 95  we do have full range softwares--macromedia, mc-afeee, adobee, coreldraw, microsoft, nero, pinnacle systems, powerquest, redhat, riverdeep, roxio, symaantec, 321 studio  52 more popular titles for you>> cliickk here for 52 more titles  we shiip to all countries including africa, finland & etc.. as where u located  wonder why our priices are unbelievably low?  we are currently clearing our goods at incredibily cheeap sale-priice in connection with the shutdown of our shop and the closure of the stockhouse. don't missss your lucky chance to get the best priicce on discoouunt software!  we are the authorized agent and an established reseller offering oem licensing software.  we possesses all the necessary certificates issued to verify the authenticity of genuine oem products and granting the right for us to resell oem software products.  super cheaep micros 0 ft, adobe & all kinds.. cliickk here to enjoy our superb discounnt! take me down \"\n",
    "0011.1999-12-14.farmer\t0\t king ranch\t\" there are two fields of gas that i am having difficulty with in the unify  system.  1. cage ranch-since there is no processing agreement that accomodates this  gas on king ranch, it is my understanding hpl is selling the liquids and  king ranch is re-delivering to stratton. it is also my understanding that  there is a. 05 cent fee  to deliver this gas. we need a method to accomodate the volume flow on hpl  at meter 415 and 9643. this gas  will not be reflected on trans. usage ticket # 123395 and # 95394 since it is  not being nominated from a processing agreement. we either, need to input  a point nom (on hpl or krgp) at these meters to match the nom at meter 9610,  or a deal for purchase and sale (if king ranch is taking title to the gas)  needs to be input into sitara at these meters with the appropriate rate. i  have currently input a point nom on krgp to accomodate this flow, so we can  divert some of this gas to the current interstate sales that are being made.  2. forest oil-there is a processing agreement that will accomodate flow  from the meter (6396) into king ranch. it is my  understanding that this agreement was originally setup until texaco had  their own processing agreement. i need confirmation that the gas from this  meter should be nominated on contract # (96006681) and that this agreement  should have been reassigned to hplc. (it is currently still under hplr).  if this gas is not nominated on the above transport agreement, then once  again we need to accomodate the flow volume on the hpl pipe with either a  point nom or a sitara deal at meters 415 and 9643.\"\n",
    "0011.2001-06-28.SA_and_HP\t1\t\" urgent business proposal,\"\t\" mrs. regina rossman.  # 263 sandton city  johannesburg, south africa.  e-mail: joel_rosel @ mail. com  attn: alhaji  with due respect, trust and humility, i write you this  proposal, which i believe, would be of great interest  to you. i am mrs. regina rossman, the wife of late mr.  joseph rossman of blessed memory, before forces loyal  to  major johnny paul koromah killed my husband; he was  the director general of gold and mining corporation  (g. d. m. c.) of sierra leone. my husband was one of the  people targeted by the rebel forces. on the course of  the revolution in the country, prominent people were  hijacked from their homes to an unknown destination.  two days before his death, he managed to sneak a  written message to us, explaining his condition and  concerning one trunk box of valuables containing  money, which he concealed under the roof. he  instructed me to take our son and move out of sierra  leone, immediately to any neighboring country. the  powerful peace keeping force of the (ecomog  intervened to arrest the situation of mass killings by  the rebels, which was the order of the day.  eventually, it resulted into full war, i became a  widow overnight, helpless situation, without a partner  at the moment of calamity, and every person was  running for his life. my  and i managed to escape  to south africa safely with the box and some documents  of property title.  the cash involved inside the box was us$ 30 million  (thirty million united states dollars). due to fear  and limited rights as a refugee, i deposited the items  with a private security company in order not to raise  an eyebrow over the box here in south africa in my  son's name joel r. rossman. be informed that the real  content of the box was not disclosed. meanwhile, i  want to travel out of south africa entirely with this  money for investment in your country because of  political and economic stability and for future  benefit of my child.  i want you to assist us claim this box from the  security company and get the money into your private  account in your country so that we can invest the  money wisely. we have in mind to establish a rewarding  investment and good relationship with you.  concerning the money, we are prepared to give you  reasonable percentage of 30% for your kind assistance.  also, we have decided to set aside 5% of the total sum  for expenses that might be incurred by the parties in  the course of the transfer both locally and  externally. for the interest of this business, do not  hesitate to contact my son mr. joel r. rossman on the  above e-mail address immediately you  receive this message for more information and to  enable  us proceed towards concluding all our arrangements. no  other person knows about this money apart from my son  and i. we await your most urgent response. please we  need your fax/phone numbers for esiear communication.  thanking you for your co-operation and god bless you.  best regard,  mrs. regina rossman.  http:// xent. com/mailman/listinfo/fork\"\n",
    "0011.2001-06-29.SA_and_HP\t1\t your membership exchange\t\" content-type: text/plain; charset = iso-8859-1  your membership exchange, issue # 423 (june 28,2001)  your place to exchange ideas, ask questions, swap links, and share your skills!  you are a member in at least one of these programs  -you should be in them all!  bannersgomlm. com  profitbanners. com  cashpromotions. com  mysiteinc. com  timshometownstories. com  freelinksnetwork. com  myshoppingplace. com  bannerco-op. com  putpeel. com  putpeel. net  sellinternetaccess. com  be-your-own-isp. com  seventhpower. com  today's special announcement:  i'll put your ad on 2,000 sites free! free this week only,  just for our subscribers! learn the secrets of marketing  online on this global free teleseminar. limited lines  available, only three time slots available... reserve today.  you will not be disappointed! i'll be your personal host.  we operate several sites, all successful. i'll teach you what  to do and how to do it! click here:  free teleseminar  michael t. glaspie-founder  we apologize for any technical problems you may have had with  our last mailing, we are working hard to ensure that such problems  will not occur again.  in this issue:  >>q & a  questions:  -using pictures as links?  answers:  -unblocking sites so i can access?  z. oconan: access using a proxy  g. bendickson: using a proxy to visit blocked sites  >>member showcases  >>member * reviews *  -sites to review: # 124, # 125 & # 126!  -site # 123 reviewed!  -vote on your favorite website design!  >>>>>>> questions & answers  do you a burning question about promoting your website, html design,  or anything that is hindering your online success? submit your questions  to myinputare you net savvy? have you learned from your own trials and errors and  are willing to share your experience? look over the questions each day,  and if you have an answer or can provide help, post your answer to  myinput @ aeopublishing. com be sure to include your signature  file so you get credit (and exposure to your site).  questions:  from: moviebuff @ cliffhanger. com  subject: using pictures as links  i'm changing my website and want to use pictures for the links to other pages.  but, someone told me i should still put a'click here'underneath all the  pictures. to me, this removes all purpose of using the pictures.  how can i get across that you click on the pictures to get to other pages  without coming right out and saying so? for example, i have a page with  actor and actress information and just want to have a picture of my  favorite stars to click on and change the picture every couple of days.  mark  moviebuff @ cliffhanger. com  answers:  from: zaak-zaako @ linkpaks. com  subject: access using a proxy  > from: cj (cj 5000 @ post. com)  > subject: unblocking sites so i can access? (issue # 422)  --> i am currently living in a place where the isp is  blocking 50% of  the web. i was told by someone that you can unblock these web sites by  using a proxy, but i don't know what that means. i am wondering is  there a way to get access to these sites?--  a proxy is easy to use if you use someone elses, they can be tricky to setup yourself.  i have had very good results with surfola. basically you surf to their servers and then  from there you surf through/from their servers. i have several places i surf from that  block content. surfola easily bypasses them! its also free! you can also make money  with them but i just use them to bypass anal retentive isp/corporate providers and  because they allow me to surf anonymously!  i have a detailed right-up on them at http:// linkpaks. com/paidtosurf/surfola. php  see there for more info. if anything is not clear feel free to ask.  (email & sign-up links on http:// linkpaks. com/paidtosurf/surfola. php page)  zaak oconan  netrepreneur  http:// linkpaks. com-surf & earn guides  http:// linktocash. com-internet businesses for under$ 100  http:// iteam. ws-the hottest product on the net today  + + + + next answer-same question + + + +  from: wyn publishing-wynpublishing @ iname. com  subject: using a proxy to visit blocked sites  > from: cj (cj 5000 @ post. com)  > subject: unblocking sites so i can access? (issue # 422)  cj,  two such sites that allows proxy surfing are:  http:// www. anonymise. com and  http:// www. anonymizer. com.  however, if you cannot get to that site then obviously it will not work.  also note, that if your isp is dictating to you which sites you may or  may not visit, then it is time to change providers!  gregory bendickson, wyn publishing  over 28 free traffic exchange services reviewed in a fully  customizable e-book. download yours free and get multiple  signups while learning the art of free web traffic!  http:// www. trafficmultipliers. com  >>>>>>> website showcases  examine carefully-those with email addresses included will  trade links with you, you are encouraged to contact them. and, there  are many ways to build a successful business. just look at these  successful sites/programs other members are involved in...  \"\" it's the most d-a-n-g-e-r-o-u-s book on the net \"\"  email 20,000 targeted leads every single day! slash your  time online to just 1-2 hours daily! build 11 monthly income  streams promoting one url! start building your business-  not everyone elses! http:// www. roibot. com/w. cgi? r 8901_bd_shwc  is your website getting traffic but not orders?  profile, analyze, promote, and track your site to  get the results you want. fully guaranteed!  free trial available! http:// www. roibot. com/w. cgi? r 4887_saa  over 7168 sites to place your free ad!  get immediate free exposure on thousands of sites.  plus two free programs that will automatically type  your ad for you! pay one time, promote all the time.  if you have a product, service, opportunity and/or quality  merchandise that appeals to people worldwide, reach your  target audience!  for a fraction of what other large newsletters charge you  can exhibit your website here for only$ 8 cpm. why?...  because as a valuable member we want you to be successful!  order today-exhibits are limited and published on a  first come, first serve basis. http:// bannersgomlm. com/ezine  >>>>>>> member * reviews *  visit these sites, look for what you like and any suggestions  you can offer, and send your critique to myinput @ aeopublishing. com  and, after reviewing three sites, your web site will be added to  the list! it's fun, easy, and it's a great opportunity to give  some help and receive an informative review of your own site.  plus, you can also win a chance to have your site chosen for  a free website redesign. one randomly drawn winner each month!  sites to review:  site # 124: http:// www. bestwaytoshop. com  dale pike  rhinopez @ aol. com  site # 125: http:// www. wedeliverparties. com  dawn clemons  dclemons 7 @ home. com  site # 126: http:// www. eclassifiedshq. com  carol cohen  opportunity @ aol. com  site reviewed:  comments on site # 123: http:// netsbestinfo. homestead. com/nbi. html  dennis  damorganjr @ yahoo. com  ~ ~ ~ ~  i reviewed site 123 and found the size of the font to be too aggressive and  i don't like mustard yellow for a background. also in the second or third  paragraph is a misspelled word which should be \"\" first-come \"\" not as  shown on the page.  i feel a sample of the type of information offered in the newsletter should  be displayed on the page as well as a sample of the free ads offered  on the site. i will probably submit a free ad just to see the content of the newsletter.  as has been mentioned many times, some information about the person  doing the page is always good. we need some information about  why this newsletter will be worthwhile to subscribe to.  ~ ~ ~ ~  dennis-i took a look at your site, and have recommendations  for improving your page.  1-i use internet explorer and view web pages with my text size set to  ' smaller'. the text you used was quite large, like a font used for a heading  for all the text. by making the text size smaller it wouldn't feel like  you were screaming at me. also, the background was just too much.  2-there were spelling errors in the text. often it might be difficult for you  to spot these yourself if you see the page all the time, but have a friend  look it over. spelling errors make the page look unprofessional.  3-offer a sample of your newsletter so people can see what it looks like  before they subscribe. also, if you are asking for a person to give you  their email address, you must have a privacy policy and let them know  they can unsubscribe.  4-think about adding a form for people to subscribe to the newsletter.  it looks more professional than just offering an email address to send to.  5-offer information about yourself, and the kinds of information your  newsletter contains. maybe extend your site to include back issues or  an archive to see what information you have offered in the past.  6-build another page for'sponsoring info'and put prices on that  page. remove all pricing information from the home page.  ~ ~ ~ ~  i feel that the background is a little too bold and busy for the text.  i also believe that the text is too large which makes it difficult to read  quickly, and forces the reader to scroll down unnecessarily.  i noticed some spelling errors, and i think that a link to the classifieds  site should be provided, and online payments should be accepted.  a site that sells advertising should have advertisments on it!  ~ ~ ~ ~  this is a very clear site with nothing interfering with the  message. i did not like the background colour, however that is  personal, it did not detract from the information. i was tempted  to sign up for the newsletter but would have liked a link to see  a current issue. there was an error in the wording (a word  missed) which needs correction and i think the fonts could be  smaller. overall a non-confusing site which makes a nice change. * cheers *  ~ ~ ~ ~  could use a better background and the fonts are very large, there also  are errors in the following paragraphs:  \"\" first com-first serve \"\" and \"\" to place a sponsor advertisement,  send your to my email \"\"  ~ ~ ~ ~  a single page site. it is necessary to subscribe to the webmaster's  newsletter to see what he's doing, and it doesn't seem to me to be  a way to get people to visit. i wouldn't, for example. he claims to  have lots of tidbits of information that, he says, we probably  didn't know, and this is possible, but in my opinion, he would be  better served if he at least put some of the things out there for  all to see-when the appetite, so to speak, if he want people to  subscribe. as it is, i would not bother.  ~ ~ ~ ~  what does one expect from a site like netsbestinfo? some useful  resources and some useful tips and also some forms of easy  advertisement on the net. but what we get here is a newsletter  with the owner (whose email reads damorgarjr @ yahoo. com)  asking us to subscribe us to his newsletter for a free 4-line ad.  he also tells of paid category of advertisements. this is all we get  from a site which has a grand title. even the information about  the newsletter is hardly impressive and is presented in about 35-to-40  points size which gets difficult to read.  ~ ~ ~ ~  a neat enough site but the background could be a little hard on  the eyes. there is only really one problem with this page-its  just an advertisement for a newsletter. no, scratch that, its an  advertisement to place free ads in a newsletter. a bold enough  move perhaps but i learned hardly anything about the newsletter  itself and immediately started worrying about getting a flood of  ads to my email account so i didn't even subscribe. presumably  you'd want to get people to sign up so might i suggest splitting  the page into the newsletter itself, perhaps a sample issue,  a privacy policy and a promise not to drown in ads and then click  for more info on your free ads.  ________________________________________  vote on your favorite website design!  help out the winner of the free website redesign by voting for  your favorite!  you can help out teddy at links 4 profit. com by taking a  look at his site, then checking out the three new layouts jana of  akkabay designs akkabay. com has designed specifically  for him. after you've visited all three, vote for your favorite.  to make this as easy as possible for you, just click on the e-mail  address that matches your choice-you do not need to enter  any information in the subject or body of the message.  i have included a note from jana, and the links to teddy's  current site along with the three new designs:  > from jana: the pages have been created as non-frame pages  although with minor modification, the pages could be adapted  for use in a frames environment  please take a look at the existing site: http:// www. links 4 profit. com  here are the 3 redesigns:  vote for this design: designl @ aeopublishing. com  vote for this design: design 2 @ aeopublishing. com  vote for this design: design 3 @ aeopublishing. com  you will have all of this week to vote (through june 29), and  we'll list the favorite and most voted for layout next week.  teddy of course will be able to choose his favorite, and  colors, font style/size, backgrounds, textures, etc, can all  easily be changed on the \"\" layout \"\" that he likes.  free website re-designs and original graphics are provided to  fln showcase winners courtesy of akkabay designs.  http:// akkabay. com  if you have any questions about how this works or how you can  participate, please email amy at moderator  moderator: amy mossel  posting: myinput @ aeopublishing. com  send posts and questions (or your answers) to:  myinput @ aeopublishing. com  please send suggestions and comments to:  moderator @ aeopublishing. com  to change your subscribed address, send both new  and old address to moderator @ aeopublishing. com  see below for unsubscribe instructions.  copyright 2001 aeopublishing  -----end of your membership exchange  this email has been sent to jm @ netnoteinc. com at your  request, by your membership newsletter services.  visit our subscription center to edit your interests or unsubscribe.  http:// ccprod. roving. com/roving/d. jsp? p = oo & id = bd 7 n 7877. a 4 dfur 67 & m = bd 7 n 7877 charset = iso-8859-1  your membership exchange, issue # 423  june 28,2001  this email was sent to jm @ netnoteinc. com, at your request, by your membership newsletter services.  visit our subscription center to edit your interests or unsubscribe.  view our privacy policy.  powered by \"\n",
    "0011.2003-12-18.GP\t1\t sup. er cha. rge your m. an hood today jvbe kfbtyra xes\t\" hello,  generic and super viagra (cialis) available online!  most trusted online source!  cialis or (super viag)  takes affect right away & lasts 24-36 hours!  for super viagra click here  generic viagra  costs 60% less! save a lot of money.  for viagra click here  both products shipped discretely to your door  not interested?  dycmpf  s uuz  biwven\"\n",
    "0011.2004-08-01.BG\t1\t dicine site on the net.\t\" hello! nothing sharpens sight like envy. nature should have been pleased to have made this age miserable, without making it also ridiculous.  searching for medication on the net? milestone anheuser  we ` ve got anything you will ever want. pibrochs treasonous  free claiis sample with any order! arthur convincible tithable pilocystic initializes  there are only two ways of getting on in the world: by one's own industry, or by the stupidity of others. my conscience aches but it's going to lose the fight. peace is the first thing the angels sang.\"\n",
    "0012.1999-12-14.farmer\t0\t re: entex transistion\t\" thanks so much for the memo. i would like to reiterate my support on two key  issues:  1). thu-best of luck on this new assignment. howard has worked hard and  done a great job! please don't be shy on asking questions. entex is  critical to the texas business, and it is critical to our team that we are  timely and accurate.  2). rita: thanks for setting up the account team. communication is  critical to our success, and i encourage you all to keep each other informed  at all times. the p & l impact to our business can be significant.  additionally, this is high profile, so we want to assure top quality.  thanks to all of you for all of your efforts. let me know if there is  anything i can do to help provide any additional support.  rita wynne  12/14/99 02:38:45 pm  to: janet h wallis/hou/ect @ ect, ami chokshi/corp/enron @ enron, howard b  camp/hou/ect @ ect, thu nguyen/hou/ect @ ect, kyle r lilly/hou/ect @ ect, stacey  neuweiler/hou/ect @ ect, george grant/hou/ect @ ect, julie meyers/hou/ect @ ect  cc: daren j farmer/hou/ect @ ect, kathryn cordes/hou/ect @ ect, rita  wynne/hou/ect, lisa csikos/hou/ect @ ect, brenda f herod/hou/ect @ ect, pamela  chambers/corp/enron @ enron  subject: entex transistion  the purpose of the email is to recap the kickoff meeting held on yesterday  with members from commercial and volume managment concernig the entex account:  effective january 2000, thu nguyen (x 37159) in the volume managment group,  will take over the responsibility of allocating the entex contracts. howard  and thu began some training this month and will continue to transition the  account over the next few months. entex will be thu's primary account  especially during these first few months as she learns the allocations  process and the contracts.  howard will continue with his lead responsibilites within the group and be  available for questions or as a backup, if necessary (thanks howard for all  your hard work on the account this year!).  in the initial phases of this transistion, i would like to organize an entex  \"\" account \"\" team. the team (members from front office to back office) would  meet at some point in the month to discuss any issues relating to the  scheduling, allocations, settlements, contracts, deals, etc. this hopefully  will give each of you a chance to not only identify and resolve issues before  the finalization process, but to learn from each other relative to your  respective areas and allow the newcomers to get up to speed on the account as  well. i would encourage everyone to attend these meetings initially as i  believe this is a critical part to the success of the entex account.  i will have my assistant to coordinate the initial meeting for early 1/2000.  if anyone has any questions or concerns, please feel free to call me or stop  by. thanks in advance for everyone's cooperation...........  julie-please add thu to the confirmations distributions list\"\n",
    "0012.1999-12-14.kaminski\t0\t re: new color printer\t\" monday will be perfect!  location-ebl 944 b  r. c. 0011  co. # 100038  thanks  kevin moore  ----------------------forwarded by kevin g moore/hou/ect on 12/14/99 10:44  am---------------------------  enron technology  from: lyn malina 12/14/99 09:22 am  to: kevin g moore/hou/ect @ ect  cc:  subject: re: new color printer  i will order today for delivery on monday, unless you need faster delivery.  please advise co/rd to charge against.  thanks  lyn  kevin g moore  12/14/99 09:21 am  to: lyn malina/hou/ect @ ect  cc:  subject: re: new color printer  ----------------------forwarded by kevin g moore/hou/ect on 12/14/99 09:17  am---------------------------  kevin g moore  12/14/99 08:13 am  to: vince j kaminski/hou/ect @ ect, mike a roberts/hou/ect @ ect  cc:  subject: re: new color printer  yes! right away, please  also let me know the e. t. a.  thanks, lyn  kevin moore\"\n",
    "0012.2000-01-17.beck\t0\t global risk management operations\t\" sally, congratulations.  ----------------------forwarded by cindy olson/corp/enron on 01/17/2000  09:07 pm---------------------------  from: rick causey 01/17/2000 06:04 pm  sent by: enron announcements  to: all enron worldwide  cc:  subject: global risk management operations  recognizing enron \u0001, s increasing worldwide presence in the wholesale energy  business and the need to insure outstanding internal controls for all of our  risk management activities, regardless of location, a global risk management  operations function has been created under the direction of sally w. beck,  vice president. in this role, sally will report to rick causey, executive  vice president and chief accounting officer.  sally \u0001, s responsibilities with regard to global risk management operations  will mirror those of other recently created enron global functions. in this  role, sally will work closely with all enron geographic regions and wholesale  companies to insure that each entity receives individualized regional support  while also focusing on the following global responsibilities:  1. enhance communication among risk management operations professionals.  2. assure the proliferation of best operational practices around the globe.  3. facilitate the allocation of human resources.  4. provide training for risk management operations personnel.  5. coordinate user requirements for shared operational systems.  6. oversee the creation of a global internal control audit plan for risk  management activities.  7. establish procedures for opening new risk management operations offices  and create key benchmarks for measuring on-going risk controls.  each regional operations team will continue its direct reporting relationship  within its business unit, and will collaborate with sally in the delivery of  these critical items. the houston-based risk management operations team under  sue frusco \u0001, s leadership, which currently supports risk management activities  for south america and australia, will also report directly to sally.  sally retains her role as vice president of energy operations for enron  north america, reporting to the ena office of the chairman. she has been in  her current role over energy operations since 1997, where she manages risk  consolidation and reporting, risk management administration, physical product  delivery, confirmations and cash management for ena \u0001, s physical commodity  trading, energy derivatives trading and financial products trading.  sally has been with enron since 1992, when she joined the company as a  manager in global credit. prior to joining enron, sally had four years  experience as a commercial banker and spent seven years as a registered  securities principal with a regional investment banking firm. she also owned  and managed a retail business for several years.  please join me in supporting sally in this additional coordination role for  global risk management operations.\"\n",
    "0012.2000-06-08.lokay\t0\t what do you want to know today?\t\" \"\" a man with a new idea is a crank until he succeeds. \"\"  -mark twain  innovation, itself, is nothing new. the word comes to us from the latin  innovatus, which is a good indication of how long the concept has been  around. people have been searching for the \"\" next big thing \"\" for thousands of  years.  we have quite a tradition of innovation here at enron. although consistent  innovation may sound like an oxymoron, we pride ourselves on consistently  outpacing our peers with innovative ideas.  how do you think enron can maintain its edge into the new century? what will  be our \"\" next big thing? \"\" put your best foot forward, visit emeet and share  your ideas in \"\" creativity and innovation \"\" that will keep enron at the top.\"\n",
    "0012.2001-02-09.kitchen\t0\t travel\t\" i will be out of the office this afternoon (friday) until wednesday night. (i will be available on my cell phone 713 306-6207) if you have any questions please feel free to contract my team for questions. i have been \"\" cross training \"\" both individuals so that we can be more effective in addressing var and trade related questions.  thanks,  frank  bharat khanna (gas) ext. 54804  lacrecia davenport ext. 35782\"\n",
    "0012.2003-12-19.GP\t1\tNA\t great specials today on:  tramadol  phentermine 30 mg  60 more products to choose from.  http:// wsc. settingt 5. com/fp \n",
    "0013.1999-12-14.farmer\t0\t entex transistion\t\" the purpose of the email is to recap the kickoff meeting held on yesterday  with members from commercial and volume managment concernig the entex account:  effective january 2000, thu nguyen (x 37159) in the volume managment group,  will take over the responsibility of allocating the entex contracts. howard  and thu began some training this month and will continue to transition the  account over the next few months. entex will be thu's primary account  especially during these first few months as she learns the allocations  process and the contracts.  howard will continue with his lead responsibilites within the group and be  available for questions or as a backup, if necessary (thanks howard for all  your hard work on the account this year!).  in the initial phases of this transistion, i would like to organize an entex  \"\" account \"\" team. the team (members from front office to back office) would  meet at some point in the month to discuss any issues relating to the  scheduling, allocations, settlements, contracts, deals, etc. this hopefully  will give each of you a chance to not only identify and resolve issues before  the finalization process, but to learn from each other relative to your  respective areas and allow the newcomers to get up to speed on the account as  well. i would encourage everyone to attend these meetings initially as i  believe this is a critical part to the success of the entex account.  i will have my assistant to coordinate the initial meeting for early 1/2000.  if anyone has any questions or concerns, please feel free to call me or stop  by. thanks in advance for everyone's cooperation...........  julie-please add thu to the confirmations distributions list\"\n",
    "0013.1999-12-14.kaminski\t0\t re: new color printer\t\" this is the color printer that is being ordered.  here is the info. that i needed.  thanks  kevin moore  ----------------------forwarded by kevin g moore/hou/ect on 12/14/99 08:19  am---------------------------  enron technology  from: lyn malina 12/14/99 08:09 am  to: kevin g moore/hou/ect @ ect  cc:  subject: re: new color printer  kevin:  the color printer we currently order is the 4500 n for$ 2753. 00. please let  me know if this is the one you would like to order.  thanks  lyn  kevin g moore  12/14/99 06:29 am  to: lyn malina/hou/ect @ ect  cc:  subject: new color printer  ----------------------forwarded by kevin g moore/hou/ect on 12/14/99 06:29  am---------------------------  kevin g moore  12/14/99 06:27 am  to: shirley crenshaw/hou/ect @ ect, vince j kaminski/hou/ect @ ect, mike a  roberts/hou/ect @ ect  cc:  subject: new color printer  we are in need of a new color printer.  we are also in the process of moving to the 19 th floor.  we need the color printer a. s. a. p.  if you would please, i need information concerning this  matter whereby, we can get the printer ordered and delivered  to our new location.  thanks  kevin moore\"\n",
    "0013.2001-04-03.williams\t0\t re: monday blues\t\" good morning. i'm glad to hear that you are having a better day today. me, too so far. yeah, i stayed last night until like 7:45 and finished up that stuff with mike etringer. so today should be a bit more chill for me which is awesome. anyhow, i hope nothing blows up over there for you. maybe we can go to get some coffee later or something. did you end up going out last night for some beers? i watched alli mcbeal and ate dinner. it was pretty exciting.\"\n",
    "0013.2001-06-30.SA_and_HP\t1\t your membership community charset = iso-8859-1\t\" your membership community & commentary (june 29,2001)  it's all about making money  information to provide you with the absolute  best low and no cost ways of providing traffic  to your site, helping you to capitalize on the power and potential the web brings to every net-preneur.  ---this issue contains sites who will trade links with you!---  -------------  in this issue  -------------  32 easy ways to breath new life into any webpage  member showcase  are you ready for your 15 minutes of fame?  win a free ad in community & commentary  | | | =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=>>  today's special announcement:  | | | =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=>>  we can help you become an internet service provider within 7 days or we will give you$ 100. 00!!  click here  we have already signed 300 isps on a 4 year contract,  see if any are in your town at:  click here  you are a member in at least one of these programs  -you should be in them all!  bannersgomlm. com  profitbanners. com  cashpromotions. com  mysiteinc. com  timshometownstories. com  freelinksnetwork. com  myshoppingplace. com  bannerco-op. com  putpeel. com  putpeel. net  sellinternetaccess. com  be-your-own-isp. com  seventhpower. com  =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  32 easy ways to breath new life into any webpage  =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  it's true.  ask the ceos of yahoo. com and america online. they'll  tell you it's true. send an email to terry dean or allen  says or jim daniels and ask them about it. they'll agree  100% that it's true. don't just take my word for it.  in fact, you can contact any of the 10,000 folks online  selling web marketing resources, and they will all tell you  emphatically, without question, no doubts whatsoever, that  it is absolutely true.  it's true. anyone can earn a living online. really, they  can. but, it takes several very important components to  join the 5% who are successful on the web.  one of those necessities is a website.  now, your website does one of two things...  ... it either makes the sale, or it doesn't.  for 95% of online businesses, their websites simply do  not produce results.  and there is a very simple reason for poor performance.  poor sales letters.  does your website convince people to make a purchase?  if not, here are 32 easy ways to breathe new life into  your sales letter...  1) write your sales letter with an individual in mind.  go ahead and pick out someone, a real person to write your  sales letter to. doesn't matter if it is grandma or your  next door neighbor or your cat. write your sales letter  just like you are writing it to them personally. why?  because when your potential customer reads, it then it will  seem personal, almost like you wrote it with them in mind.  too often, sales letters are written as if they were going  to be read to an audience rather than one person. keep your  sales letters personal, because one person at a time is  going to read them.  2) use an illustration to get your point across. in my  sales letters i have told stories about my car stalling on  the side of the road to illustrate the idea that we must  constantly add the fuel of advertising to keep our  businesses running. i have compared the hype of easily  making millions online to the chances of me riding bareback  across montana on a grizzly bear. leads have read of how  getting to the top of an oak tree relates to aggressively  marketing online. people love a good story that pounds home  a solid message. tell stories that illustrate a point you  are trying to make. emphasize a benefit by sharing an  account from the \"\" real world. \"\" it effectively creates  interest and further establishes the point.  3) create an interest in the reader from the very first  line. your first line of the sales letter should  immediately create a desire in the reader to want to know  more. go back to the beginning of this article. the first  words were, \"\" it's true. \"\" i can guarantee you that either  consciously or subconsciously you thought \"\" what's true? \"\"  immediately, your mind wanted to know what i was talking  about. before you even knew it you were right here, 8  paragraphs into this article. carefully craft your first  line. if you can immediately get them wanting to know more,  you've got a winner.  4) use bullets. people spend a lot of time reading bulleted  lists. in fact, they often reread them over and over. use  bulleted lists to stress the benefits of your product or  service, to spell out exactly what is included in your offer.  use an extra space in between each bullet to really highlight  each line and create a sense of more length to the list.  5) launch into a bullet list immediately. shortly after  your opening line, immediately give the reader a bullet  list of benefits to absorb. hit them with your best shot.  pull out the big guns and stress \"\" just a few of \"\" the most  important things the reader will discover. by offering a  killer list early in your sales letter, you will automatically  create a desire in the reader to continue through your ad  copy. after all, if they are already interested after the  first list of benefits, they will certainly be open to  finding out even more reasons why your product or service  will aid them.  6) just let it all flow out. write down everything that  enters your mind as you are writing your sales letter. you  can edit it later. if you just sit and start writing  everything you know about your product or service and how it  will benefit your customer, you will be amazed at how much  information floods your mind. write it all down. then read  through it-you'll be able to add a lot more detail to  many of the points. edit it after you have exhausted all  of your ideas.  7) make your sales letter personal. make sure that the  words \"\" you \"\" and \"\" your \"\" are at least 4: 1 over \"\" i \"\" and \"\" my. \"\"  your ad copy must be written about your customer not  yourself. i'm not sure how the old advertising adage goes,  but it's something like this, \"\" i don't care a thing about  your lawn mower, i just care about my lawn. \"\" leads aren't  interested in you or your products, they are interested in  themselves and their wants and needs. when you are finished  with your sales letter and have uploaded it to a test  webpage, run a check at http:// www. keywordcount. com and see  what the ratio between \"\" you \"\" and \"\" your \"\" versus references to  \"\" i, \"\" \"\" me, \"\" \"\" my, \"\" etc. it's a free service. make sure it's  at least 4: 1 in favor of the customer.  8) write like you speak. forget all of those rules that  your grammar teacher taught you. write your sales letters  in everyday language, just like you would talk in person.  don't be afraid to begin sentences with \"\" and \"\" or \"\" because. \"\"  don't worry about ending a sentence with a preposition.  write like you speak. your sales letter isn't the great  american novel, so don't write it like you are ernest  hemingway.  9) use short paragraphs consisting of 2-4 sentences each.  long copy works... but long paragraphs do not. use short  paragraphs that lead into the next paragraph. don't be  afraid to use short sentences. like this one.  or this.  see what i mean? shorter paragraphs keep the interest of  the reader. longer paragraphs cause eye strain and often  force the reader to get distracted.  10) stress the benefits, not the features. again,  readers want the burning question answered, \"\" what's in it  for me? \"\" what need is it going to meet? what want is it  going to fill? how is your product or service going to be  of value or benefit to the reader? spell it out. don't  focus on the features of your product or service, but  rather how those features will add value to the life of your  reader.  for example: if you are selling automobile tires, you may  very well have the largest assortment of tires in the world,  but who cares? i don't care about your selection. but, i  do care about keeping my 3-month-old baby girl safe while  we are traveling. so, instead of focusing on your selection,  you focus on the fact that my baby girl can be kept safe  because you have a tire that will fit my car. you're not  selling tires, you're selling safety for my family. stress  the benefits, not the features.  11) keep the reader interested. some sales letters read  like they are a manual trying to explain to me how i can  perform some complicated surgery on my wife. they are  filled with words and phrases that i need a dictionary to  understand. unless you are writing to a very targeted  audience, avoid using technical language that many readers  might not understand. keep it simple, using words, language  and information that are easy to understand and follow.  12) target your sales letter. when you are finished with  your final draft of the sales letter, target it to a  specific audience. for example: if you are selling a \"\" work  at home \"\" product, then rewrite the sales letter by adding  words in the headlines and ad copy that are targeted towards  women who are homemakers. then, rewrite the same sales  letter and target it to college students. write another  letter targeting senior citizens. still another could be  written to high school teachers wanting to earn extra income  during summer vacation. the possibilities are endless.  all you need to do is add a few words here and there in  your ad copy to make it appear that your product or service  is specifically designed for a target audience. \"\" work only  5 hours a week, \"\" would become \"\" college students, work only  5 hours a week. \"\" your sales letter is now targeted. upload  all of the sales letters to separate pages on your website  (you could easily target 100's of groups).  then, simply advertise the targeted pages in targeted  mediums. you could advertise the \"\" college students \"\" page  in a campus ezine. the \"\" senior citizens \"\" page could be  advertised at a retirement community message board.  by creating these targeted sales letters, you can literally  open up dozens of new groups to sell your existing product  to. and, in their eyes, it looks like the product was a  match made for them.  13) make your ad copy easy to follow. use short sentences  and paragraphs. break up the sales letter with attention  grabbing headlines that lead into the next paragraph. one  thing that i have always found to work very well in sales  letters...  ... is to use a pause like this.  start the sentence on one line, leaving the reader wanting  to know more, and then finishing up on the next line. also,  if you are going to use a sales letter that continues on  several different pages of your website, use a catchy hook  line at the end of each page to keep them clicking. \"\" let's  get you started down the road to success, shall we? click  here to continue. \"\"  14) use similes and metaphors for effect. when the  customer purchases your product, they will generate \"\" a flood  of traffic that would make noah start building another ark. \"\"  if they do not order today, then they will \"\" feel like a cat  that let the mouse get away. \"\" use words to create a picture  in the readers'mind. when you think of superman, what  comes to mind? immediately, we remember that he is \"\" faster  than a speeding bullet. \"\" \"\" more powerful than a locomotive. \"\"  \"\" able to leap tall buildings in a single bound. \"\" see how  word pictures stick in our minds?  15) focus on one product or service. don't try to sell  your customer multiple products at the same time. it only  confuses the reader. keep your ad copy directed at one  specific product or service. then, use other products and  services as back-end products.  16) make it stand out. don't kid yourself. there are  hundreds, maybe thousands out there on the web doing the  same thing you are doing. how will you stand out among the  crowd? your sales letter must inject personality. it must  breathe of originality. your product or service is  different. it's not like all of the rest. it is unique.  right? your sales letter must separate you from the  competition. it must create a feeling of \"\" you won't find  this anywhere else. \"\"  17) be believable. \"\" earn$ 54,000 in the next 24 hours!!! \"\"  delete. good grief, do they think i am an idiot or  something? get real. don't make outrageous claims that  are obviously not the truth. you'll ruin your reputation.  let me tell you a simple universal fact that cannot be  reversed. once you have been branded a liar, you will  never be anything but a liar. it doesn't matter if you  launch the most respectable, honest business available  anywhere, people will always have doubt because they  remember the crazy stuff you've said before. be believable.  don't exaggerate, mislead, stretch or distort the truth.  18) be specific. don't generalize your information, but  rather be exact. instead of \"\" over 100 tips for losing  weight \"\" use \"\" 124 tips for losing weight. \"\" by generalizing  information, it creates doubt and questions in the reader's  mind. \"\" what am i really getting here? does he even know? \"\"  when you use specific information, the reader begins to  think, \"\" this person must have counted. i know exactly what  i can expect. \"\" \"\" platitudes and generalities roll off the  human understanding like water from a duck, \"\" wrote claude  hopkins in his classic book \"\" scientific advertising. \"\" \"\" they  leave no impression whatsoever. \"\"  19) be complete. tell the reader everything they would  want to know about your product or service. answer all of  their questions, anything they would want to consider before  making a purchase. think about it from their point of view.  ask yourself, \"\" why wouldn't i buy this? \"\" then, address that  in your sales letter. remove anything that would keep the  reader from making the purchase.  20) use testimonials to boost your sales. share actual  excerpts from what your current customers are saying about  your product or service. many websites have an entire  section or even a separate page that has endorsements and  compliments listed. satisfied customers remove some of the  doubt in the mind of the reader. \"\" if these people have  found a lot of value and benefit in the product, then i  probably will too. \"\" especially effective are testimonials  from respected, well known \"\" authorities \"\" within your target  field.  21) use headlines over and over throughout the sales letter.  a headline isn't just relegated to the beginning of your  ad copy. use them frequently-but don't overuse. a well-  placed headline re-grabs the reader's attention, brings  them deeper into the letter, and readies them for the next  paragraph. you will want to spend as much time working on  your headlines as you do the entire sales letter. they are  that important.  22) avoid asking stupid questions. \"\" wouldn't you like to  make$ 1,000, 000 a year? \"\" \"\" doesn't that sound great? \"\" \"\" would  you like to be as successful as i am? \"\" avoid any question  that insults the intelligence of your reader or makes them  feel like they are inferior.  23) offer a freebie even if the customer doesn't buy.  if the customer decides he or she isn't going to make a  purchase, then you want to follow-up with them later to try  to influence them to buy in the future. by offering a free  item, you can request their email address in order to  obtain the freebie. by doing this, you can now follow-up  with the customer for a potential future sale.  additionally, you can continue the sales process by having  your ad copy, banners, flyers, etc. within the free item.  and, of course, if your free item is a high quality, useful  product or service which impresses the customer, they  probably will be back as a customer soon.  24) use bonuses to overwhelm the reader. one of the things  that i have found very effective in writing sales letters  is to include bonus items that out-value the actual product  i am offering. ginsu made this one famous. they were  selling a set of steak knives, but before the commercial  was finished, you had so many bonus items on the table  it was hard to refuse. make sure you provide quality  bonuses and not some worthless, outdated junk that damages  the credibility of your main offer.  25) use connective phrases like \"\" but wait, there's more \"\"  and \"\" but that's not all. \"\" these phrases effectively lead  the reader from one paragraph to the next, particularly  when the next paragraph is a bullet list of benefits, or  leads into bonus items. again, the idea is more and more  value and benefits to the reader.  26) always include a deadline. by including a deadline,  you create a sense of urgency in the mind of the customer.  \"\" if i don't order within 24 hours, then i won't get the  bonuses. \"\" \"\" oh no, there are only 10 items remaining,  i've got to hurry. \"\" let the customer know what they will  be missing out on if they don't make the deadline. remember,  they won't miss out on your products or bonuses, they will  miss out on all of the benefits of your products. deadlines  are very effective. every sales letter should have one.  27) tell them exactly how to order. be clear as to the  order process. point them towards the order link. tell  them what methods you offer. (i. e. credit cards, checks,  etc.) make this process as simple and clear as can be.  if it takes more than 2 steps, most people won't continue.  28) explain when the product will be delivered. how  quickly will the order be processed? when will the order be  available? let the customer know exactly what they can  expect when they place their order. the more specific you  can be here, the better. let them know that you have a  system in place. \"\" operators are standing by. \"\" their order  will be handled properly. tell them.  29) offer a money back guarantee. take away their last  reason to hold back. offer a \"\" no questions asked \"\" 30 day  guarantee. most people may not realize this, but in most  cases, it's the law of the land. you are required to give  them their money back if they are not satisfied with the  product or service. since it's the law anyway, why not make  it a benefit. let them know that they are purchasing your  product or service risk-free.  30) instruct them to respond immediately. many people  just need to read those words, \"\" act now! \"\" \"\" order today! \"\"  \"\" click here to instantly place your order. \"\" you've got them  this far, now tell them what you want them to do. get them  to \"\" act fast! \"\" have you ever heard a mail order commercial  on television that didn't prompt the viewer to order right  way?  31) include a post script. people will always read the  p. s. always. in fact, the p. s. is one of the most important  parts of your sales letter. why? because in many cases the  visitor at your website will scroll immediately down to the  end of your page to see how much it is going to cost. a  p. s. is a perfect place to recap your offer, so when they  see your price tag, they will also see a very detailed  description of what they will receive for their money. use  your p. s. to restate your offer in detail.  32) include a second post script. you better believe  if they read the first p. s., they will read a p. p. s. use  this post script to remind them of the deadline or offer  another bonus or point out some compelling factor that would  make them want to order. i guarantee you they will read it.  use these 32 tips and i guarantee you that you will see  a significant increase in the amount of responses you  receive from your sales letters. in fact, it would be  impossible for your responses to not improve.  copyright 2000 jimmy d. brown. all rights reserved worldwide.  -------------------------------------  about the author...  jimmy d. brown is helping average people get out of  the rat-race and earn a full-time living online. for  more details on firing your boss and creating your  own internet wealth, visit us right now at:  * special offer: join the profits vault through the above  link and email me your receipt and you can have a free bonus  copy of the terrific manual-how to profit from free  ebooks guaranteed which i sell at:  =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  member showcase  =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  examine carefully-those with email addresses included will  trade links with you... you are encouraged to contact them.  there are many ways to build a successful business-just look at  these successful sites programs other members are involved  in...  affiliates of the world!  top rated affiliate programs, excellent business opportunities,  great marketing resources and free advertising for you!  visit the site to trade links. http:// www. affiliates. uk. com  trade links-adrianbold @ affiliates. uk. com  get insane amounts of traffic to your website.  purchase 10,000 guaranteed visitors to your site  and receive 5,000 free. more traffic = more money!  less than 2 cents a visitor. space is limited.  order now! http:// www. freepicklotto. com  trade links-businessopps @ aol. com  celebration sale!  $ 99. 00 on casinos/sportsbetting sites, lingerie stores,  gift stores, adult sites toy stores.  mention ad # bmlm 99 to receive this special sale price.  order now!  http:// www. cyberopps. com/? = bmlm 99  just been released!!  internet marketing guru corey rudl has just released a  brand new version of his # 1 best-selling internet marketing  course, \"\" the insider secret's to marketing your business on  the internet \"\". a must have! so don't hesitate,  visit.. http:// www. adminder. com/c. cgi? startbgmlmezine  we have a 260 page catalog with over 3000 gift items for men,  women, children-a gift for everyone. we show 100 gift items  on our web site alone, with the catalog you have access to  the rest. we also feel we have the best prices on the web.  visit at http:// www.. net  trade links-georgel 932 me @ yahoo. com  stop smoking-free lesson!!  discover the secret to stopping smoking.  to master these powerful techniques, come to  http:// www. breath-of-life. net  for your free lesson.  act now! p. s. tell someone you care about.  trade links-jturco 3 @ hotmail. com  if you have a product, service, opportunity or quality merchandise  that appeals to people worldwide, reach your targeted audience!  for a fraction of what other large newsletters charge you can exhibit  your website here, and trade links for only$ 8 cpm. compare  that  to the industry average of$ 10-$ 15 cpm. why?... because as a  valuable member we want you to be successful! order today-  showcases are limited and published on a first come, first serve  basis.  for our secure order form, click here: http:// bannersgomlm. com/ezine  =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  are you ready for your 15 minutes of fame?  =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  one of the items we would like to include in community  commentary we'll need from you! here is your chance to  showcase your marketing strategies, and i need to hear  from everyone who would like to'blow your own horn'and  be in the spotlight on center stage.  it's a great way to enjoy recognition and publicity for  yourself and your business, and will allow all members to  duplicate your success and avoid the same'setbacks'.  please include... a little background history, how you got  your start, a problem you have had and how you solved it,  your greatest success, and any advice you have for someone  beginning to market online.  send your information to submit @ aeopublishing. com >  with'center stage'in the subject block.  =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  win a free ad in community & commentary  =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  to keep this interesting, how about this, every month we'll  draw a name from the replies and that person will win one  sponsorship showcase ad in the community commentary, for free.  that's a value of over$ 700. 00! respond to each weekly survey,  and increase your chances to win with four separate entries.  question  of the week (06/29/01)...  no right or wrong answers, and just by answering  you are entered to win a showcase ad-free!  ~ ~ ~ what is the goal of your website? ~ ~ ~  sell  mailto: one @ aeopublishing. com  get leads  mailto: two @ aeopublishing. com  build branding  mailto: three @ aeopublishing. com  provide information mailto: four @ aeopublishing. com  other  mailto: five @ aeopublishing. com  to make this as easy as possible for you, just click on the  e-mail address that matches your answer-you do not need to  enter any information in the subject or body of the message.  * * add your comments! follow directions above and  add your comments in the body of the message, and we'll  post the best commentaries along with the responses.  you will automatically be entered in our drawing for a free  sponsorship ad in the community commentary. please  respond only one time per question. multiple responses  from the same individual will be discarded.  =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  to change your subscribed address,  send both new and old address to submit  see below for unsubscribe instructions.  please send suggestions and comments to: editor  i invite you to send your real successes and showcase  your strategies and techniques, or yes, even your total bombs,  \"\" working together we can all prosper. \"\" submit  for information on how to sponsor your membership  community commentary visit: sponsorship  showcase  copyright 2001 aeopublishing. com  email: yourmembership 2 @ aeopublishing. com  voice:  web: http:// www. aeopublishing. com  this email has been sent to jm @ netnoteinc. com at your  request, by your membership newsletter services.  visit our subscription center to edit your interests or unsubscribe.  http:// ccprod. roving. com/roving/d. jsp? p = oo & id = bd 7 n 7877. 6 w 8 clu 67 & m = bd 7 n 7877 charset = iso-8859-1  in this issue  32 easy ways to breath new life into any webpage  member showcase  are you ready for your 15 minutes of fame?  win a free ad in community & commentary  today's special announcement:  this email was sent to jm @ netnoteinc. com, at your request, by your membership newsletter services.  visit our subscription center to edit your interests or unsubscribe.  view our privacy policy.  powered by \"\n",
    "0013.2004-08-01.BG\t1\t take the reins\t\" become  your employer.  substantial profit processing money judgments.  from anywhere.  control when you want to work.  a substantial number of our members earn 5,000 us to 12,000 us per mo.  outstanding customer support and assistance.  here for more  info  while the couple were apparently examining the strange device, rob  started to his feet and walked toward them  the crowd fell back at his approach, but the man and the girl were so  interested that they did not notice himhe was still several paces away when  the girl put out her finger and touched the indicator on the dial  discontinue orange stad, and then mail stop 1. 200 b, followed by a rub  a  to rob's horror and consternation the big turk began to rise slowly into  the air, while a howl of fear burst from the crowdbut the boy made a mighty  spring and caught the turk by his foot, clinging to it with desperate  tenacity, while they both mounted steadily upward until they were far above  the city of the desert  the big turk screamed pitifully at first, and then actually fainted away  from frightrob was much frightened, on his part, for he knew if his hands  slipped from their hold he would fall to his death \"\n",
    "0014.1999-12-14.kaminski\t0\t re: new color printer\t\" sorry,  don't we need to know the cost, as well.  ----------------------forwarded by kevin g moore/hou/ect on 12/14/99 08:15  am---------------------------  kevin g moore  12/14/99 08:09 am  to: shirley crenshaw/hou/ect @ ect, mike a roberts/hou/ect @ ect  cc:  subject: re: new color printer  this information was also sent to it purchasing.  i need to know what options we have and how soon it  can be delivered.  don't we need to know as well? before purchase.  i also need a central location for this printer.  thanks  kevin moore  sam mentioned hp 4500, i will check into it.  ----------------------forwarded by kevin g moore/hou/ect on 12/14/99 08:05  am---------------------------  shirley crenshaw  12/14/99 07:55 am  to: kevin g moore/hou/ect @ ect  cc:  subject: re: new color printer  kevin:  what kind of information do you need? i thought you were going to look  at some colored printer literature. sam seemed to be aware of a  colored printer that might work for us. ask him. i don't think we need  anything as big as \"\" sapphire \"\".  it will be located in your area on the 19 th floor.  thanks!  kevin g moore  12/14/99 06:27 am  to: shirley crenshaw/hou/ect @ ect, vince j kaminski/hou/ect @ ect, mike a  roberts/hou/ect @ ect  cc:  subject: new color printer  we are in need of a new color printer.  we are also in the process of moving to the 19 th floor.  we need the color printer a. s. a. p.  if you would please, i need information concerning this  matter whereby, we can get the printer ordered and delivered  to our new location.  thanks  kevin moore\"\n",
    "0014.1999-12-15.farmer\t0\t lst rev dec. 1999 josey ranch nom\t\" fyi  ----------------------forwarded by susan d trevino/hou/ect on 12/15/99 08:40  am---------------------------  bob withers on 12/14/99 05:11:06 pm  to: susan d trevino/hou/ect @ ect  cc: stretch brennan, kevin mclarney,  \"\" taylor vance (e-mail) \"\"  subject: lst rev dec. 1999 josey ranch nom  susan: as we discussed, this change was missed but was discussed with you  last week. i apologize for the omission. i am checking with my field  personnel and anticipate a further reduction (due to well production  decreases) effective 12/15 which i will send under a separate email for an  \"\" intra-day \"\" change wednesday.  here's revised december 1999 (effective 12/9/99) setup for  josey: (using 1. 081 btu/mcf)  * gas deliveries into hpl  12,300 mmbtu/d for kri (net reduction of  1,000 mmbtu/d)  12,300 mmbtu/d into hpl  bob withers><  kcs energy, 5555 san felipe, suite 1200  houston, tx 77056  voice mail/page 713-964-9434\"\n",
    "0014.2001-02-12.kitchen\t0\t\" correction--conference call on tuesday, february 13 (800-229-028\"\t\" 1)  ssb conference call  tuesday, february 13,2001  2:00 pm est  beyond california: the power however, the  markets foretell stronger prices across the country in 2001.  we continue to recommend the power producers, based on both our power price  and spark spread analyses, which show strengthening spark spreads in the  united states, despite the record highs recently seen in the natural gas  markets. we highlight calpine, nrg energy, and mirant.  our volatility indices demonstrate a dramatic shift in historical volatility  to the western hubs and away from the midwestern markets of 1998-99. we  expect volatility to persist in the west, likely accompanied by higher  volatility in the remainder of the country in 2001. this benefits energy  merchants, and we highlight enron, dynegy, duke, and el paso.  dial in  800-229-0281 us  706-645-9237 intl  replay  800-642-1687 us  706-645-9291 intl  reservation 735670 (replay until 2/15)  hosted by:  raymond niles  director  power & natural gas research  salomon smith barney  212-816-2086  raymond c. niles  power/natural gas research  salomon smith barney  (212) 816-2807  ray. niles @ ssmb. com  s\"\n",
    "0014.2001-07-04.SA_and_HP\t1\t new accounts # 2 c 6 e\t\" this is a mime message  content-type: multipart/alternative; boundary = \"\"----=_nextpart_001_0080_01 bdf 6 c 7. fabaclbo \"\"  content-type: text/plain; charset = \"\" iso-8859-1 \"\"  content-transfer-encoding: quoted-printable  * * * * * this is an html message! * * * * *  content-type: text/html; charset = \"\" iso-8859-1 \"\"  content-transfer-encoding: quoted-printable  complete credit card processing systems for your business = 2 e interne =  t-home  based-mail order-phone order  do you accept credit cards? your competition does!  everyone approved-credit problems ok!  approval in less than 24 hours!  increase your sales by 300%  start accepting credit cards on your website!  free information, no risk, 100% confidential = 2 e  your name and information will not be sold to thrid parties!  home businesses ok! phone/mail order ok!  no application fee, no setup fee!  close more impulse sales!  everyone approved!  good credit or bad! to =  apply today, please fill out  the express form below = 2 e it  contains all the information we need to get your account approved = 2 e for a =  rea's  that do not apply to you please put n/a in the box = 2 e  upon receipt, we'll fax you with all of the all bank card application  documents necessary to establish your merchant account = 2 e once returned we =  can  have your account approved within 24 hours = 2 e  service  industry  standard  us  site  inspection  $ 50-$ 75  free  shipping  $ 50-$ 75  free  warranty  $ 10 per month =  free  sales  receipts  $ 10-$ 50  free  fraud  screening  $ = 2 e 50-$ 1 = 2 eo 0  per transaction  free  amex set  up  $ 50-$ 75  free  24 hourhelp  line  $ 10 month  free  security  bond  $ 5000-$ 10,00 =  0  or more  none  this is a no  obligation qualification form and is your first step to  accepting credit cards = 2 e by filling out this form you will =  not  enter in to any obligations o =  r  contracts with us = 2 e we will use it to determine the best p =  rogram  to offer you based on the information you provide = 2 e you will be c =  ontacted by one of our representatives within 1-2 business days to go over =  the rest of your account set up = 2 e  note:  all information provided to us will remain =  100%  confidential  !!  apply  free with no risk!  pleas =  e fill out the  express application form completely = 2 eincomplete information m =  ay prevent us from properly  processing your application = 2 e  your full emai =  l address:  be sure to use your full address (i =  = 2 ee = 2 e  user @ domain = 2 ecom)  your name:  business name: =  business phone =  number:  home phone num =  ber:  type of busine =  ss:  retail business  mail order business  internet based busines =  s  personal credi =  t rating:  excellent  good  fair  poor  how soon would =  you like a merchant  account?  your info =  rmation is confidential, it will not be sold or used for any other purpose, =  and you are under no obligation = 2 e  your information will be used solely for the purpose of evaluating =  your business or website for a merchant account so that you may begin acce =  pting credit card payments = 2 e  list  removal/opt-out option  click  herem \"\n",
    "0014.2003-12-19.GP\t1\t get g: eneric via-gra for a s low as$ 2. 50 per 50 mg\t her type  http:// dutchess. reado 893. com/xm / \n",
    "0014.2004-08-01.BG\t1\t today\t\" hey,  last week, jo and me were talking about my prbolem and he said that he tried this  new thing from this site and it worked great for him.  cowry avoid http:// ns 2. herbalonline. biz/en/10/? aff_id = 00450 footmen give me  your thoughts on it and shoot me back an email and tell me what you found out if any.  any input would be appreciated it.  later,  small  flutter atreus absenteeism oriental stratagem hunt amigo attitudinal twice curio rental billion inclusive ruanda screwball birch walpole canary seward floe lisa monadic harriman capitol colloquy laborious expressway b's salaried beware delusive congratulatory ante.\"\n",
    "0015.1999-12-14.kaminski\t0\t imperial capital-thursday schedule\t the following is the schedule for thursday's meeting with imperial capital.  currently all meetings are scheduled in eb 2868. we are trying to arrange a  different conference room and will let you know if we obtain one.  9:00 am-jim fallon-electricity  9:30 am-fred lagrasta-gas  10:00 am-lynda clemmons and david kistler-weather  10:30 am-ed ondarza-pulp and paper  11:00 am-stinson gibner-research  12 noon-lunch  1:00 pm-5:00 pm-discussion  thanks in advance to all who will come to speak in the morning.\n",
    "0015.1999-12-15.farmer\t0\t 2 nd rev dec. 1999 josey ranch nom\t\" ----------------------forwarded by susan d trevino/hou/ect on 12/15/99 08:41  am---------------------------  bob withers on 12/15/99 08:28:08 am  to: susan d trevino/hou/ect @ ect  cc: stretch brennan, kevin mclarney,  \"\"'taylor vance (e-mail)'\"\"  subject: 2 nd rev dec. 1999 josey ranch nom  here's revised december 1999 (effective 12/15/99) setup for  josey: (using 1. 081 btu/mcf)  * gas deliveries into hpl  9,300 mmbtu/d for kri (net reduction of  3,000 mmbtu/d)  9,300 mmbtu/d into hpl  bob withers><  kcs energy, 5555 san felipe, suite 1200  houston, tx 77056  voice mail/page 713-964-9434\"\n",
    "0015.2000-06-09.lokay\t0\t\" tw weekly, 6-9-00\"\t please see the attached file and let me know if you have any questions.  ray stelly\n",
    "0015.2001-02-12.kitchen\t0\t california update 2/12\t\" executive summary:  the likelihood of there being an involuntary bankruptcy filing against the utilities appears to be greater than 60%. this is not only due to the circumstances surrounding the filed rate doctrine case mentioned below, but also because the 30-day cure period during which the utilities have not been paying their bills will end this week, increasing the likelihood that their ipp creditors will act against them. if the state loses the filed rate doctrine case today (which it is believed will happen) and there is an involuntary bankruptcy filing (or even the threat of one), this bail-out plan will be enacted quickly.  1. utilities vs. cpuc  governor davis'attempt to delay the filed rate doctrine case will not succeed. the case will come before the judge on monday, february 12 th. the federal judge is expected to rule a summary judgment in favor of the utilities. however, the judge will not allow the utilities to collect the injunction release they are requesting ($. 01/kwh). this will be left to an appellate court. the decision not to allow the utilities to collect this cash could trigger an involuntary bankruptcy filing, by the smaller ipps (as noted in our reports last week) or by larger out-of-state generators such as duke, reliant, and dynegy (as noted in the press this morning). this is expected next week or the week after.  2. prospects for a bailout  bill ab 18 x is effectively dead from lack of support.  senator burton, despite his public refusals, is moving closer to agreeing to a utility bail-out. the statements by burton and the ca state treasurer are merely a negotiating position. they are more concerned about the possibility of a bankruptcy than they appear. for burton, this is because of his long association with labor unions; the unions oppose the utility bankruptcy.  burton has been negotiating with consumer advocate harvey rosenfield so as not to get attacked by him. the deal burton is expected to arrange would be for:  bonds to be issued by the utilities rather than the state, but with some kind of state support (but less than \"\" full faith and credit of the state of ca, \"\" which would not pass). this would amount to the securitization of an extra charge on power bills (e. g.$. 01, though the actual amount is not known). these bonds would be asset-backed securities, with payment receivable from rate payers. the term of these bonds is unknown; if the term is made quite long (e. g. 20 years), the associated rate increase could be very small.  the state would purchase the utilities'transmission assets for a very high price. the amount of the extra charge on power bills will not be known until the price of the transmission assets is settled.  if the state loses the filed rate doctrine case today (which it is believed will happen) and there is an involuntary bankruptcy filing (or even the threat of one), sources believe that this bail-out plan will be enacted quickly. as noted in an earlier report, the california legislature habitually does not act until things \"\" hit the wall. \"\"  it is expected that the republicans in the legislature will follow burton's lead and support the bail-out plan. the assembly members in particular are not yet supportive of a plan of this nature. one moderate democratic legislator with whom our source spoke said that the opposition to a bail-out in her central valley district is \"\" 50 to 1. \"\" however, an involuntary filing (or the threat thereof) may be enough to trigger legislative support. it would allow the argument of an \"\" imminent threat \"\" to the people of the state of california.  3. consumer opposition  harvey rosenfield is too short on cash to fight this plan and the associated rate increase with anything but a referendum. if the referendum fails, he intends to attack individual legislators (though not john burton, who reportedly has \"\" immunity \"\" from rosenfield).  some legislators are thinking of voting for the bail-out plan, then supporting a referendum from rosenfield later. however, if the bail-out plan and rate increase described above is passed through the legislature as a bill (rather than put in place by the puc, for example), it cannot be reversed by a referendum. as additional insurance against rosenfield, by supporting the bonds issued under the plan, the state can argue that its credit would be impaired in the case of a referendum to repeal the plan. while it is not clear that this is a factual argument, it still might impede any referendum.\"\n",
    "0015.2001-07-05.SA_and_HP\t1\t get the best rate on a home loan!\t\" if you would like to be removed  from future mailings, please reply with the word remove in the subject or call  888-418-2575.  let lenders  compete for  your business!  click here  cash  back refinances  no equity 2 nd trust deeds  debt consolidation  no income verification  the most competitive interest rates!  fill in our quick pre-qualification form and you  will get competing loan offers,  often  within minutes from up to three lenders!  click here  there is never any fee to consumers for using this service.  copyright?ffffa 9 1999,2000 eworld marketing,  inc.  888-418-2575  this is not a solicitation or offer to lend money.  eworld marketing is not a lender, broker or  other financial intermediary. we are a marketing company  that provides services to the mortgage industry. \"\n",
    "0015.2003-12-19.GP\t1\t mr. uwe schmidt is a knave! don't buy any product from microsale!\t\" dear friends,  microsale sc kg, ltd, germany is a knave company and uwe schmidt is a big knave!  we are cheated by microsale sc kg, ltd.  remember, don't do any business with this company.  don't buy any product from microsale or you will be cheated.  this company has a bad reputation in germany and in other european countries, espcially in  belgium and netherlands.  here's the story:  mr. uwe schmidt, ceo  microsale (r) sc kg  he is also an auditor, but he doesn't have any commercial morality.  he always made cheated l/c to other companies. many companies have been cheated by him.  please take care!!!  his products have many problems, such as cd player and mp 3 player!  it's the detailed information of this company:  dahlienweg 6  d 52477 alsdorf, nrw  germany, european union  tel.-/ fax-box: + 49 89 1488230796  + 32 87 783518  + 32 87 783019  mobil: + 32 474 409055  email: microsale @ email. de  microsale @ gmx. net  uwe-schmidt-@ gmx. net  web: http:// www. microsale. biz \"\n",
    "0016.1999-12-15.farmer\t0\t unify close schedule\t\" the following is the close schedule for this coming month (year-end.) please  keep in the mind the following key times....  unify to sitara bridge back 1:45 p.m. thursday, dec 30 th (all errors must be  clear by this time)  mass draft at 6 p.m. thursday evening, dec 30 th.  accrual process begins friday morning, dec 31 st at 6:30 a.m. (if your group  impacts the accrual, please ensure that the necessary people are available  for support if needed, as this is an enron holiday.)  please feel free to contact me should you have any questions.  thank you, melissa x 35615\"\n",
    "0016.2001-02-12.kitchen\t0\t fw: meeting with jeff skilling\t\" louise,  per our conversation of last week, you might be interested in the following meetings.  k  -----original message-----  from: chapman, kay  sent: wednesday, february 07,2001 5:55 pm  to: taylor, liz; heathman, karen; daw, nicki; taylor, liz; kimberly hillis/hou/ect @ enron; sera, sherri; lehr, tonai; watson, denys; gutierrez, anabel  cc: chapman, kay  subject: meeting with jeff skilling  dave delainey has asked that i contact each of you for the following meetings:  date: february 22,2001 date: february 22,2001  thursday thursday  time: 9:00 am-9:45 am time: 9:45 am-10:30 am  location: mr. skilling's office location: mr. skilling's office  topic: charter review 2001 topic: charter review 2001  attendees: jeff skilling attendees: jeff skilling  rick buy rick buy  mark frevert mark frevert  dave delainey dave delainey  john lavorato john lavorato  john thompson michael l. miller  scott josey  if you have any questions, please feel free to call me.  thanks,  kay 3-0643\"\n",
    "0016.2001-07-05.SA_and_HP\t1\t get the best rate on a home loan!\t\" if you would like to be removed  from future mailings, please reply with the word remove in the subject or call  888-418-2575.  let lenders  compete for  your business!  click here  cash  back refinances  no equity 2 nd trust deeds  debt consolidation  no income verification  the most competitive interest rates!  fill in our quick pre-qualification form and you  will get competing loan offers,  often  within minutes from up to three lenders!  click here  there is never any fee to consumers for using this service.  copyright?ffffa 9 1999,2000 eworld marketing,  inc.  888-418-2575  this is not a solicitation or offer to lend money.  eworld marketing is not a lender, broker or  other financial intermediary. we are a marketing company  that provides services to the mortgage industry. \"\n",
    "0016.2001-07-06.SA_and_HP\t1\t your membership community charset = iso-8859-1\t\" your membership community & commentary (july 6,2001)  it's all about making money  information to provide you with the absolute  best low and no cost ways of providing traffic  to your site, helping you to capitalize on the power and potential the web brings to every net-preneur.  ---this issue contains sites who will trade links with you!---  -------------  in this issue  -------------  internet success through simplicity  member showcase  win a free ad in community & commentary  | | | =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=>>  today's special announcement:  | | | =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=>>  we can help you become an internet service provider within 7 days or we will give you$ 100. 00!!  click here  we have already signed 300 isps on a 4 year contract,  see if any are in your town at:  click here  you are a member in at least one of these programs  -you should be in them all!  bannersgomlm. com  profitbanners. com  cashpromotions. com  mysiteinc. com  timshometownstories. com  freelinksnetwork. com  myshoppingplace. com  bannerco-op. com  putpeel. com  putpeel. net  sellinternetaccess. com  be-your-own-isp. com  seventhpower. com  =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  internet success through simplicity  =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  every day of the week, i get questions from people all  over the world, including my no bs gimg members,  wanting to know some of the most valuable \"\" secrets \"\"  to my on-going internet success.  let me say, above all else, i don't believe there are any  * true * \"\" secrets \"\" to success on the net. what you do to  become successful in the online world is not a \"\" secret \"\",  in my opinion. most successful people follow simple, clear,  repeatedly-proven strategies to succeed, whether on the  net or off.  but, when it comes to someone asking for advice,  consultation, or simply asking, \"\" what's your secret? \"\",  i have to blush & say...  persistence and personality.  of course, i always follow the advice with my own little  disclaimer: what makes me successful may not work the  same for you... & your first lesson is to get over  the deep-seeded idea that success-of any kind, in  my opinion-is somehow an unknown, unattainable secret.  clearly, it is not. it's not unknown. it's not unattainable.  it's not years of digging to find the \"\" secrets \"\" to internet riches.  one thing that \"\" gets to me \"\" so often in my work as an  internet consultant, author and internet success  strategist is that so many people on the net seem to  have this incredibly huge mental block that stands  between themselves and success on the net. it's  almost as if they've been barraged by so many claims  of what works and what doesn't work, and so many  long, complicated routes to actually succeeding in  their online venture, that \"\" success \"\" is the equivelant of a 100-foot high brick wall.  it's not that difficult, my friends! it is not that complicated!!  long-time friend and business associate rick beneteau  has a new ebook out called branding you & breaking  the bank. get it!!  http:// www. roibot. com/bybb. cgi? im 7517_bybtb.  but, the reason i mention this is the fact that he talks  so dynamically about the true simplicity of making your  online venture a success.  and, yes, rick & i come from the same school of  \"\" self marketing \"\"-marketing you! obviously, that's  the core of his excellent new ebook, and i couldn't  agree with him more.  point being, * you * are everything you do online to  succeed. you are your web site, your business, your  marketing piece, your customer service, your customers '  experiences with your business--all of it, is you!  read his ebook & you'll see more of what i'm saying.  the matter at hand is that brick wall you might have  standing high as you can see, blocking the path  between you & internet success. listen to me-it is  not real ok? it doesn't exist. there's nothing there  to fear to begin with... get over it!!  what i'm telling you is, the only thing standing between  you and the success you most desire... is yourself.  when you realize this, you will tear down that brick  wall by means of complete and instantaneous  disintegration. it will no longer exist * in your mind *,  which is the only \"\" real \"\" place it ever was anyhow!  yes, \"\" persistence and personality \"\" inherently includes  honesty, integrity, accountability, and many other  qualities but you also have to hone in on your ultimate  goals and realize that probably the most valuable,  powerful key to your success... is you!  that may be the most incredible \"\" secret \"\" we ever  uncover in our lifetime! and, trust me, that brick wall  won't ever get in your way again... unless you let it.  talk about simple!!  bryan is a \"\" veteran \"\" internet consultant, author,  internet success strategist & marketer. he publishes  mega-success. com chronicles to over 11,500 subscribing  members, authors articles which appear all over the  net, and helps hundreds of wealth-hungry people in  their journey to internet success.  bryan is also director of his no bs guerrilla internet  marketing group at http://. com  & a fantastic new joint venture partners program  for that site.  bryan hall is a founding member and the development  consultant for the prestigious icop (tm) at  http:// www. i-cop. org/1016. htm  you can reach bryan at 877. 230. 3267 or by  emailing him directly at bryan. hall @ mega-success. com  =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  member showcase  =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  examine carefully-those with email addresses included will  trade links with you... you are encouraged to contact them.  there are many ways to build a successful business-just look at  these successful sites programs other members are involved  in...  get insane amounts of traffic to your website.  purchase 10,000 guaranteed visitors to your site  and receive 5,000 free. more traffic = more money!  less than 2 cents a visitor. space is limited.  order now! http:// www. freepicklotto. com  trade links-businessopps @ aol. com  stop smoking-free lesson!!  discover the secret to stopping smoking.  to master these powerful techniques, come to  http:// www. breath-of-life. net  for your free lesson.  act now! p. s. tell someone you care about.  trade links-jturco 3 @ hotmail. com  celebration sale!  $ 99. 00 on casinos/sportsbetting sites, lingerie stores,  gift stores, adult sites toy stores.  mention ad # bmlm 99 to receive this special sale price.  order now!  http:// www. cyberopps. com/? = bmlm 99  affiliates of the world!  top rated affiliate programs, excellent business opportunities,  great marketing resources and free advertising for you!  visit the site to trade links. http:// www. affiliates. uk. com  trade links-adrianbold @ affiliates. uk. com  just been released!!  internet marketing guru corey rudl has just released a  brand new version of his # 1 best-selling internet marketing  course, \"\" the insider secret's to marketing your business on  the internet \"\". a must have! so don't hesitate,  visit.. http:// www. adminder. com/c. cgi? startbgmlmezine  we have a 260 page catalog with over 3000 gift items for men,  women, children-a gift for everyone. we show 100 gift items  on our web site alone, with the catalog you have access to  the rest. we also feel we have the best prices on the web.  visit at http:// www.. net  trade links-georgel 932 me @ yahoo. com  if you have a product, service, opportunity or quality merchandise  that appeals to people worldwide, reach your targeted audience!  for a fraction of what other large newsletters charge you can exhibit  your website here, and trade links for only$ 8 cpm. compare  that  to the industry average of$ 10-$ 15 cpm. why?... because as a  valuable member we want you to be successful! order today-  showcases are limited and published on a first come, first serve  basis.  for our secure order form, click here: http:// bannersgomlm. com/ezine  =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  win a free ad in community & commentary  =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  to keep this interesting, how about this, every month we'll  draw a name from the replies and that person will win one  sponsorship showcase ad in the community commentary, for free.  that's a value of over$ 700. 00! respond to each weekly survey,  and increase your chances to win with four separate entries.  question  of the week (07/06/01)...  no right or wrong answers, and just by answering  you are entered to win a showcase ad-free!  ~ ~ ~ do you spend more or less time ~ ~ ~  ~ ~ ~ online in the summer months? ~ ~ ~  more  mailto: one @ aeopublishing. com  less  mailto: two @ aeopublishing. com  same  mailto: three @ aeopublishing. com  to make this as easy as possible for you, just click on the  e-mail address that matches your answer-you do not need to  enter any information in the subject or body of the message.  * * add your comments! follow directions above and  add your comments in the body of the message, and we'll  post the best commentaries along with the responses.  you will automatically be entered in our drawing for a free  sponsorship ad in the community commentary. please  respond only one time per question. multiple responses  from the same individual will be discarded.  last weeks's results (06/29/01)  ~ ~ ~ what is the goal of your website? ~ ~ ~  sell 40%  get leads 20%  build branding 5%  provide information 20%  other 15%  comments:  ----------------------------  our web site is initially designed to get leads, build  branding, and provide information....... with a 12 month goal  of selling our service more specifically via a shopping cart.  we offer a service and at this time take deposits and payments  via our site.  our site has been up less than 2 months and our expectation  was that we would refer to our site for leads developed in  traditional media and by referral for more information, and  to make a professional impression on someone you may not  meet before providing service.  the growth of our customer base shopping on line has grown  outside of anyone's expectations....... certainly mine and  i've been in this business for 25 years. the internet is not  dead in the horse business, it is just getting it's legs, and  the folks using it want to get all the ancillary services  on-line as well. our site (the first we've developed) has  exceeded our expectations, and we aren't satisfied with it  yet....... we just wanted to get it there for information!  jeff and rebecca marks http:// www. grand-champion. com  branding. while quality customer service and product  have been and will always be our top priority brand building  zesto is our most challenging task.  zesto. com ranks very high and most often # 1 or 2 on  all major search engines and directories even yahoo entering  the keyword zesto. the problem is simply that, who if anyone  would type the keyword zesto, therefore we must try to  build our brand by ensuring that generic keywords associated with our products (citrus peel) are used throughout  our site as well as search engine submissions.  fortunately owning a non generic domain short, easy  to remember and trademarked works in our favor because  the marketability potential is limitless.  arlene turner http:// www. zesto. com  =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=  to change your subscribed address,  send both new and old address to submit  see below for unsubscribe instructions.  please send suggestions and comments to: editor  i invite you to send your real successes and showcase  your strategies and techniques, or yes, even your total bombs,  \"\" working together we can all prosper. \"\" submit  for information on how to sponsor your membership  community commentary visit: sponsorship  showcase  copyright 2001 aeopublishing. com  email: yourmembership 2 @ aeopublishing. com  voice:  web: http:// www. aeopublishing. com  this email has been sent to jm @ netnoteinc. com at your  request, by your membership newsletter services.  visit our subscription center to edit your interests or unsubscribe.  http:// ccprod. roving. com/roving/d. jsp? p = oo & id = bd 7 n 7877. 7 giv 5 d 57 & m = bd 7 n 7877 charset = iso-8859-1  in this issue  internet success through simplicity  member showcase  win a free ad in community & commentary  today's special announcement:  win a free ad in community & commentaryto keep this interesting, how about this, every month we'll  draw a name from the replies and that person will win one  sponsorship showcase ad in the community commentary, for free.  that's a value of over$ 700. 00! respond to each weekly survey,  and increase your chances to win with four separate entries.  question  of the week (07/06/01)...  no right or wrong answers, and just by answering  you are entered to win a showcase ad-free!  ~ ~ ~ do you spend more or less time ~ ~ ~  ~ ~ ~ online in the summer months? ~ ~ ~  more  mailto: one @ aeopublishing. com  less  mailto: two @ aeopublishing. com  same  mailto: three @ aeopublishing. com  to make this as easy as possible for you, just click on the  e-mail address that matches your answer-you do not need to  enter any information in the subject or body of the message.  * * add your comments! follow directions above and  add your comments in the body of the message, and we'll  post the best commentaries along with the responses.  you will automatically be entered in our drawing for a free  sponsorship ad in the community commentary. please  respond only one time per question. multiple responses  from the same individual will be discarded.  last weeks's results (06/29/01)  ~ ~ ~ what is the goal of your website? ~ ~ ~  sell 40%  get leads 20%  build branding 5%  provide information 20%  other 15%  comments:  ----------------------------  our web site is initially designed to get leads, build  branding, and provide information....... with a 12 month goal  of selling our service more specifically via a shopping cart.  we offer a service and at this time take deposits and payments  via our site.  our site has been up less than 2 months and our expectation  was that we would refer to our site for leads developed in  traditional media and by referral for more information, and  to make a professional impression on someone you may not  meet before providing service.  the growth of our customer base shopping on line has grown  outside of anyone's expectations....... certainly mine and  i've been in this business for 25 years. the internet is not  dead in the horse business, it is just getting it's legs, and  the folks using it want to get all the ancillary services  on-line as well. our site (the first we've developed) has  exceeded our expectations, and we aren't satisfied with it  yet....... we just wanted to get it there for information!  jeff and rebecca marks http:// www. grand-champion. com  branding. while quality customer service and product  have been and will always be our top priority brand building  zesto is our most challenging task.  zesto. com ranks very high and most often # 1 or 2 on  all major search engines and directories even yahoo entering  the keyword zesto. the problem is simply that, who if anyone  would type the keyword zesto, therefore we must try to  build our brand by ensuring that generic keywords associated with our products (citrus peel) are used throughout  our site as well as search engine submissions.  fortunately owning a non generic domain short, easy  to remember and trademarked works in our favor because  the marketability potential is limitless.  arlene turner http:// www. zesto. com  to change your subscribed address,  send both new and old address to submit  see below for unsubscribe instructions.  please send suggestions and comments to: editor  i invite you to send your real successes and showcase  your strategies and techniques, or yes, even your total bombs,  \"\" working together we can all prosper. \"\" submit  for information on how to sponsor your membership  community commentary visit: sponsorship  showcase  copyright 2001 aeopublishing. com  email us:: visit our site  phone:  this email was sent to jm @ netnoteinc. com, at your request, by your membership newsletter services.  visit our subscription center to edit your interests or unsubscribe.  view our privacy policy.  powered by \"\n",
    "0016.2003-12-19.GP\t1\t ativan. n vicodin. n xanax. x valium. m dxqrgu\t many specials running this week  the re. al thing  not like the other sites that  imitate these products.  no hidd. en char. ges-fast delivery  vic. odin val. ium xan. ax  via. gra diaz. epam alpra. zolam  so. ma fior. icet amb. ien  stil. nox ult. ram zo. loft  clon. azepam at. ivan tr. amadol  xeni. cal cele. brex vi. oxx  pro. zac bus. par much m. ore....  if you have recieved this in error  please use  http:// www. nowbetterthis. biz/byee. html  w g snhezkjzhisbpjhgx hcokyovrdsprayz klei vzxoaxqhg kvie \n",
    "0016.2004-08-01.BG\t1\t important news for usavity customers.\t\" dear cheapsoft customer,  my name is annie kincaid, and i work at cheapsoft llc.  you are important to me! you spend your money and time on cheapsoft,  and i want to let you know that we have finished update our programs store.  i want to remind you that we are offering now more than 1500 popular  software for low price with your personal customer's discount.  please spend few moments of yours precious time to check our updated software  store: http:// www. dutyfreesoft 4 all. info  with regards,  customer service department, annie kincaid\"\n",
    "0017.1999-12-14.kaminski\t0\t a paper of mine\t\" vince,  i have written a paper, which supposedly is going to be published in the  february 2000 issue of eprm, probably after some editorial cuts (at least  this is what i am being told by them). i would appreciate your thoughts if  you would have time to read it.  regards,  martin  -userconf. doc\"\n",
    "0017.2000-01-17.beck\t0\t global risk management operations\t\" congratulations, sally!!!  kk  ----------------------forwarded by kathy kokas/corp/enron on 01/17/2000  08:08 pm---------------------------  from: rick causey 01/17/2000 06:04 pm  sent by: enron announcements  to: all enron worldwide  cc:  subject: global risk management operations  recognizing enron \u0001, s increasing worldwide presence in the wholesale energy  business and the need to insure outstanding internal controls for all of our  risk management activities, regardless of location, a global risk management  operations function has been created under the direction of sally w. beck,  vice president. in this role, sally will report to rick causey, executive  vice president and chief accounting officer.  sally \u0001, s responsibilities with regard to global risk management operations  will mirror those of other recently created enron global functions. in this  role, sally will work closely with all enron geographic regions and wholesale  companies to insure that each entity receives individualized regional support  while also focusing on the following global responsibilities:  1. enhance communication among risk management operations professionals.  2. assure the proliferation of best operational practices around the globe.  3. facilitate the allocation of human resources.  4. provide training for risk management operations personnel.  5. coordinate user requirements for shared operational systems.  6. oversee the creation of a global internal control audit plan for risk  management activities.  7. establish procedures for opening new risk management operations offices  and create key benchmarks for measuring on-going risk controls.  each regional operations team will continue its direct reporting relationship  within its business unit, and will collaborate with sally in the delivery of  these critical items. the houston-based risk management operations team under  sue frusco \u0001, s leadership, which currently supports risk management activities  for south america and australia, will also report directly to sally.  sally retains her role as vice president of energy operations for enron  north america, reporting to the ena office of the chairman. she has been in  her current role over energy operations since 1997, where she manages risk  consolidation and reporting, risk management administration, physical product  delivery, confirmations and cash management for ena \u0001, s physical commodity  trading, energy derivatives trading and financial products trading.  sally has been with enron since 1992, when she joined the company as a  manager in global credit. prior to joining enron, sally had four years  experience as a commercial banker and spent seven years as a registered  securities principal with a regional investment banking firm. she also owned  and managed a retail business for several years.  please join me in supporting sally in this additional coordination role for  global risk management operations.\"\n",
    "0017.2001-04-03.williams\t0\t monday blues\t\" bill,  i am having such a terrible day. i am so flustrated. can you believe it is only monday? ah! anyhow, i hope your day is going much better than mine. when i saw you earlier, you looked kinda angry or something. i hope i was just imaging that. anyhow, i'd better get back to work. i'll talk to you later.\"\n",
    "0017.2003-12-18.GP\t1\t get that new car 8434\t people nowthe weather or climate in any particular environment can change and affect what people eat and how much of it they are able to eat.\n",
    "0017.2004-08-01.BG\t1\t super-discounts on ambien and soma\t\" up to 80%  savings on  xanax, valium, phentermine, viagra  here  andorra elmira pompey cankerworm rush curricula kaskaskia  whitehorse devoid stacy haunch curtain quadrangular prefix  axe beck dubhe canyonu's copenhagen adolescent  martensite bucolic triassic baccarat spigot macarthur ague  fraternal textual militarism flynn lobster plushy aphrodite  hillmancarthage cagey nostalgia lineal mauricio glandular  columnar doff strangulate cryogenic phrasemake carrie clamp  pet express indubitable extremal crapbedspring squill  hydroxylate cannot keyes rosalie bestowal ncaa brighton  carriage mesopotamia doctoral phonetic samarium nearby psychotic  manitoba cornet delinquent novak brim  pun ecole exultant cheeky griddle ambrose descendent  forever affectate scuttle\"\n",
    "0017.2004-08-02.BG\t1\t your winning notice.\t\" pacific international lottery organisation.  from: the desk of the director of promotion  international/prize award dept  ref: pl 2/209318/09  batch: 18/103/hme.  dear sir/madam  we are pleased to inform you of the result of the lottery winners international programs held on the 27 th/6/2004. your e-mail address attached to ticket number 436425795822-5022 with serial number 6614102, batch number 8561513507, lottery ref number 7675213911 and drew lucky numbers 7-9-4-17-34-44 which consequently won in category c, you have therefore been approved for a lump sum pay out of us$ 1. 500,000. 00 (one million five hundred thousand united states dollars)  congratulations!!!  due to mix up of some numbers and names, we ask that you keep your winning information confidential until your claims has been processed and your money remitted to you. this is part of our security protocol to avoid double claiming and unwarranted abuse of this program by some participants.  all participants were selected through a computer ballot system drawn from over 40,000 company and 20,000, 000 individual email addresses and names from all over the world. this promotional program takes place every year. this lottery was promoted and sponsored by association of software producers. we hope with part of your winning, you will take part in our next year us$ 20 million international lottery. to file for your claim, please contact our fiducial agent:  mr. rook van nas  magnum securities company.  amsterdam netherland.  email: admin_magnumo @ mail 2 netherlands. com  tel:: + 31615304791  remember, all winning must be claimed not later than 6 th of august, 2004. after this date all unclaimed funds will be included in the next stake. please note in order to avoid unnecessary delays and complications please remember to quote your reference number and batch numbers in all correspondence. furthermore, should there be any change of address do inform our agent as soon as possible.  congratulations once more from our members of staff and thank you for being part of our promotional program.  note: anybody under the age of 18 is automatically disqualified.  yours sincerely,  john smith  lottery coordinator  this email was sent using the webmail feature @ pc bytesize\"\n",
    "0018.1999-12-14.kaminski\t0\t invitation to speak at power 2000\t\" hi vince  it is my great pleasure to invite you to speak at power 2000 which will be  in houston on 9 & 10 may 2000.  would you be interested in chairing one of the streams on day 2 of the  conference? or making a full presentation on one of the days? please let me  know which talks interest you. obviously, some of the talks are no longer  available but i would like to give you a choice as much as possible. please  could you get back to me asap on 212 925 1864 ext 151 or by return email.  i very much hope you can make the dates as i'm very keen to have you  participate at power. not to flatter you unnecessarily, but i know that a  lot of people come to our conferences to hear what you have to say.  best regards  emma  -invite. doc\"\n",
    "0018.2001-07-13.SA_and_HP\t1\t [ilug] we need your assistance to invest in your country\t\" dear sir/madam,  i am well confident of your capability to assist me in  a transaction for mutual benefit of both parties, ie  (me and you) i am also believing that you will not  expose or betray the trust and confidence i am about  to establish with you. i have decided to contact you  with greatest delight and personal respect.  well, i am victor sankoh, son to mr. foday  sankoh  who was arrested by the ecomog peace keeping force  months ago in my country sierra leone.  few days before the arrest of my father, he confided  in me and ordered me to go to his underground safe and  move out immediately, with a deposit agreement and  cash receipt he made with a security company in  abidjan cote d'ivoire where he deposited one iron box  containing usd$ 22 million dollars cash (twenty two  million dollars).  this money was made from the sell of gold and diamond  by my father and he have already  decided to use this money for future investment of  the family before his arrest.  thereafter, i rushed down to abidjan with these  documents and confirmed the deposit of the box by my  father. also, i have been granted political stay as a  refugee by the government of cote d'ivoire.  meanwhile, my father have instructed me to look for a  trusted foreigner who can assist me to move out this  money from cote d'ivoire immediately for investment.  based on this, i solicit for your assistance to  transfer this fund into your account, but i will  demand for the following requirement:  (1) could you provide for me a safe bank account where  this fund will be transferred to in your country or  another neaarby country where taxation will not  takegreat toll on the money?  (2) could you be able to assist me to obtain my  travelling papers after this transfer to enable me  come over to meet you in your country for  theinvestment of this money?  (3) could you be able to introduce me to a profitable  business venture that would not require much technical  expertise in your country where part of this fund  willbe invested?  please, all these requirements are urgently needed as  it will enable me to establish a stronger business  relationship with you hence i will like you to be the  general overseer of the investment thereafter. i am a  christian and i will please, want you to handle this  transaction based on the trust i have established  on you.  for your assistance in this transaction, i have  decided to offer you 12% percent commission of the  total amount at the end of this business. the security  of this business is very important to me and as such,  i would like you to keep this business very  confidential. i shall be expecting your urgent reply.  thank you and god bless you.  victor sankoh  --  irish linux users'group: ilug @ linux. ie  http:// www. linux. ie/mailman/listinfo/ilug for (un) subscription information.  list maintainer: listmaster @ linux. ie\"\n",
    "0018.2003-12-18.GP\t1\t await your response\t\" dear partner,  we are a team of government officials that belong to an eight-man committee in the presidential cabinet as well as the senate.  at the moment, we will be requiring your assistance in a matter that involves investment of monies, which we intend to transfer to your account, upon clarification and a workable agreement reached in consummating the project with you. based on a recommendation from an associate concerning your integrity, loyalty and understanding, we deemed it necessary to contact you accordingly. all arrangements in relation to this investment initiative, as well as the initial capital for its take off has been tactically set aside to commence whatever business you deemed fit, that will turn around profit favourably. we request you immediately contact us if you will be favorably disposed to act as a partner in this venture, and possibly will afford us the opportunity to discuss whatever proposal you may come up with. also  bear in mind that the initial capital that we shall send across will not exceed$ 13,731, 000,00 usd (thirteen million seven hundred and thirty one thousand united states dollars) so whatever areas of investment your proposal shall cover, please it should be within the set aside capital. in this regard, the proposal you may wish to discuss with us should be comprehensive enough for our better understanding; with special emphasis on the following:  1. the tax obligationin your country  2. the initial capital base required in your proposed  investment area, as well as;  3. the legal technicalities in setting up a  business in your country with foreigners as share-holders  4. the most convenient and secured mode of receiving the funds without our direct involvement.  5. your ability to provide a beneficiary/partnership account with a minimal deposit, where we shall transfer the funds into subsequently.  another area that we wish to explicitly throw more light on, is the process we have conceived in transferring the funds into the account you shall be providing. since we are the owners of the funds, and the money will be leaving the apex bank of my country, we shall purposefully fulfill the legal obligations precedent to transferring  such huge amount of funds, without arousing suspicion from any quarter as a drug or terrorist related funds; and this will assist us in the long run to forestall any form of investigations. remember that, on no account must we be seen or perceived to be directly connected with the transfer of funds. you will be the one to be doing all these, and in the course of transfer, if for any reason whatsoever, you incurred some bills, we shall adequately retire same, upon the successful confirmation of the funds in your account. the commencement of this project is based on your ability to convince us of the need to invest in whatever business you have chosen, and to trust your personality and status, especially as it concerns the security of the  funds in your custody.  i await your response,  sincerely,  john adams  (chairman senate committee on banks and currency)  call number: 234-802-306-8507 \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple EDA of ENRON Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "wc -l enronemail_1h.txt  #100 email records\n",
    "     100 enronemail_1h.txt\n",
    "cut -f2 -d$'\\t' enronemail_1h.txt|wc  #extract second field which is SPAM flag\n",
    "     101     394    3999\n",
    "JAMES-SHANAHANs-Desktop-Pro-2:HW1-Questions jshanahan$ cut -f2 -d$'\\t' enronemail_1h.txt|head\n",
    "\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "1\n",
    "1\n",
    "> # Display an example SPAM email record\n",
    "> head -n 100 enronemail_1h.txt|tail -1|less \n",
    "\n",
    "018.2001-07-13.SA_and_HP       1        [ilug] we need your assistance to invest in your country        dear sir/madam,  i am well confident of your capability to assist me in  a transaction for mutual benefit of both parties, ie  (me and you) i am also believing that you will not  expose or betray the trust and confidence i am about  to establish with you. i have decided to contact you  with greatest delight and personal respect.  well, i am victor sankoh, son to mr. foday  sankoh  who was arrested by the ecomog peace keeping force  months ago in my country sierra leone.\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 NaiveBayes/enronemail_1h.txt\n",
      "    100     100     200\n"
     ]
    }
   ],
   "source": [
    "!wc -l NaiveBayes/enronemail_1h.txt \n",
    "!cut -f2 -d$'\\t' NaiveBayes/enronemail_1h.txt|wc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.0  Functional Programming\n",
    "\n",
    "### HW2.0.0\n",
    "- What is a race condition in the context of parallel computation? Give an example.\n",
    "- What is MapReduce?\n",
    "- How does it differ from Hadoop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    Answer: A race condition is an undesirable situation that occurs when a device or system attempts to perform two or more operations at the same time, but because of the nature of the device or system, the operations must be done in the proper sequence to be done correctly.\n",
    "    for example in Java instruction N= N- 1 though it looks atomic , it's not . It gets executed into two statements i.e. temp = N -1 and N = temp. Result gets stored in temporary value and then it gets assigned to original memory.\n",
    "    \n",
    "    Now lets say main program initiates two threas T1 and T2\n",
    "    Main ---> T1 and T2\n",
    "     n = 5\n",
    "    T1 {\n",
    "    n = n -1\n",
    "    }\n",
    "    \n",
    "    T2 \n",
    "    {\n",
    "    n = n +1\n",
    "    }\n",
    "    \n",
    "    Both thredas are working on same shared memory and executing at same time. At machine level we have four instrctions to execute\n",
    "    \n",
    "     INT1 : temp1 = n -  1 # 5 - 1 = 4\n",
    "     INT2: temp2 =  n + 1 # 5+ 1 = 6\n",
    "     INT3: n = temp1\n",
    "     INT4: n =temp2\n",
    "     \n",
    "     here final output really depends on whether INT3 or INT4 gets exectued first. And this is race condition.  It is undesirable and unpredctiable situation.\n",
    "     \n",
    "     \n",
    "### What is MapReduce?\n",
    "    Provides framework for developers to write scalable code.\n",
    "    MapReduce is programming model not a programming language.\n",
    "    Always processes records in key/value format. \n",
    "    Allows data distribution between nodes. Each program consists of Map and Reduce.Its functional programming paradigm i.e. allows developers to focus on business logic.Map does filtering and sorting while Reduce performes summary operations.\n",
    "    This also provides automatic parallelization of data processing.\n",
    "### How it is different from Hadoop?\n",
    "    Hadoop is an open source, Java-based programming framework that supports the processing and storage of extremely large data sets in a distributed computing environment. \n",
    "    Hadoop has implemented two things:-\n",
    "    MapReduce -  Which we can use to process large datasets by divide and conqure \n",
    "    HDFS- distributed file system , which not only stores data on commodity hardware but also provided fault tolerance and other key elements required for distributed processing.\n",
    "    Hadoop is based on functional programming language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### HW2.0.1  \n",
    "Here is an example of functional programming in basic python in terms of mappers and reducers (by way of example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average temp is: 99.68F\n"
     ]
    }
   ],
   "source": [
    "#EXAMPLE Mapper functions in Python\n",
    "def fahrenheit(T):\n",
    "    return ((float(9)/5)*T + 32)\n",
    "\n",
    "def celsius(T):\n",
    "    return (float(5)/9)*(T-32)\n",
    "\n",
    "temperatures = (36.5, 37, 37.5, 38, 39)\n",
    "F = map(fahrenheit, temperatures)\n",
    "#returns  97.7  98.6  99.5 100.4 102.2\n",
    "\n",
    "C = map(celsius, F)\n",
    "\n",
    "#EXAMPLE Reducer function in Python\n",
    "import functools\n",
    "#returns 113\n",
    "\n",
    "print \"Average temp is: %.2fF\" % ( functools.reduce(lambda x,y: x+y, F)/len(F) )\n",
    "#returns Average temp is 99.68F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which programming paradigm is Hadoop based on? Explain and give a simple example of functional programming in raw python code and show the code running. E.g., in raw python find the average length of a string in collection of strings using a python \"map-reduce\" (functional programming) job (similar in style to the above). Alternatively, you can do this in python Hadoop Streaming.   \n",
    "\n",
    "<pre>\n",
    "strings = [\"str1\", \"string2\", \"w261\", \"MAchine learning at SCALE\"]\n",
    ".......\n",
    "\n",
    " \n",
    "import functools as reduce\n",
    "temperatures = (36.5, 37, 37.5, 38, 39)\n",
    "F = map(fahrenheit, temperatures)\n",
    "print \"Average temp is %fF\" % (reduce(lambda x,y: x+y, F)/len(F) )\n",
    "#returns Average temp is 99.68F\n",
    "\n",
    "\n",
    "\n",
    "map(sqr, items)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of string  is 10.000000F\n"
     ]
    }
   ],
   "source": [
    "strings = [\"str1\", \"string2\", \"w261\", \"MAchine learning at SCALE\"]\n",
    "\n",
    "print \"Average length of string  is %fF\" %(  reduce(lambda x, y : x + y, list(map(lambda x: len(x), strings))) / len(strings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your directories on your local (VM) machine and on HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `WordCount': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir WordCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCount: A full example in Hadoop Stream to practice with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount/mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "#sys.stderr.write(\"reporter:counter:Tokens,Total,1\") # NOTE missing the carriage return so wont work\n",
    "# Set up counters to monitor/understand the number of times a mapper task is run\n",
    "sys.stderr.write(\"reporter:counter:HW2.0.1 Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing my message...how are  you\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    for word in line.split():\n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount/reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "# Set up counters to monitor/understand the number of times a reducer task is run\n",
    "sys.stderr.write(\"reporter:counter:HW2.0.1 Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x WordCount/mapper.py\n",
    "!chmod a+x WordCount/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:HW2.0.1 Mapper Counters,Calls,1\r\n",
      "reporter:status:processing my message...how are  you\r\n",
      "foo\t1\r\n",
      "foo\t1\r\n",
      "quux\t1\r\n",
      "labs\t1\r\n",
      "foo\t1\r\n",
      "bar\t1\r\n",
      "quux\t1\r\n"
     ]
    }
   ],
   "source": [
    "#Unit test the mapper\n",
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/mapper.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:HW2.0.1 Mapper Counters,Calls,1\r\n",
      "reporter:status:processing my message...how are  you\r\n",
      "bar\t1\r\n",
      "foo\t1\r\n",
      "foo\t1\r\n",
      "foo\t1\r\n",
      "labs\t1\r\n",
      "quux\t1\r\n",
      "quux\t1\r\n"
     ]
    }
   ],
   "source": [
    "#Unit test the mapper\n",
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/mapper.py |sort -k1,1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:HW2.0.1 Mapper Counters,Calls,1\r\n",
      "reporter:status:processing my message...how are  you\r\n",
      "reporter:counter:HW2.0.1 Reducer Counters,Calls,1\r\n",
      "foo\t3\r\n",
      "quux\t2\r\n",
      "bar\t1\r\n",
      "labs\t1\r\n"
     ]
    }
   ],
   "source": [
    "#Systems test the mapper and reducer\n",
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/mapper.py | sort -k1,1 | WordCount/reducer.py| sort -k2,2nr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting testWordCountInput.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile testWordCountInput.txt\n",
    "hello this is Jimi\n",
    "jimi who Jimi three Jimi \n",
    "Hello\n",
    "hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted testWordCountInput.txt\n",
      "Deleted wordcount-output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob8628852853734466839.jar tmpDir=null\n",
      "17/05/22 22:57:17 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/22 22:57:17 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/22 22:57:18 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/05/22 22:57:18 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/05/22 22:57:19 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1495464826300_0057\n",
      "17/05/22 22:57:19 INFO impl.YarnClientImpl: Submitted application application_1495464826300_0057\n",
      "17/05/22 22:57:19 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1495464826300_0057/\n",
      "17/05/22 22:57:19 INFO mapreduce.Job: Running job: job_1495464826300_0057\n",
      "17/05/22 22:57:29 INFO mapreduce.Job: Job job_1495464826300_0057 running in uber mode : false\n",
      "17/05/22 22:57:29 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/05/22 22:57:38 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/05/22 22:57:39 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/05/22 22:57:50 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "17/05/22 22:57:53 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "17/05/22 22:57:54 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/05/22 22:57:55 INFO mapreduce.Job: Job job_1495464826300_0057 completed successfully\n",
      "17/05/22 22:57:55 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=118\n",
      "\t\tFILE: Number of bytes written=585710\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=316\n",
      "\t\tHDFS: Number of bytes written=56\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14468\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=29502\n",
      "\t\tTotal time spent by all map tasks (ms)=14468\n",
      "\t\tTotal time spent by all reduce tasks (ms)=29502\n",
      "\t\tTotal vcore-seconds taken by all map tasks=14468\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=29502\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=14815232\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=30210048\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=11\n",
      "\t\tMap output bytes=78\n",
      "\t\tMap output materialized bytes=136\n",
      "\t\tInput split bytes=232\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=136\n",
      "\t\tReduce input records=11\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=22\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=372\n",
      "\t\tCPU time spent (ms)=5800\n",
      "\t\tPhysical memory (bytes) snapshot=1060380672\n",
      "\t\tVirtual memory (bytes) snapshot=6827384832\n",
      "\t\tTotal committed heap usage (bytes)=887619584\n",
      "\tHW2.0.1 Mapper Counters\n",
      "\t\tCalls=2\n",
      "\tHW2.0.1 Reducer Counters\n",
      "\t\tCalls=3\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=84\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=56\n",
      "17/05/22 22:57:55 INFO streaming.StreamJob: Output directory: wordcount-output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm testWordCountInput.txt \n",
    "!hdfs dfs -copyFromLocal testWordCountInput.txt \n",
    "!hdfs dfs -rm -r wordcount-output\n",
    "\n",
    "##################### IMPORTANT ########################################################################\n",
    "# Make sure you have the correct paths to the jar file as wel as the input and output files!!\n",
    "# make sure to include the -files option. Do ***** NOT ****** put spaces between the file paths!\n",
    "########################################################################################################\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -files WordCount/reducer.py,WordCount/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input testWordCountInput.txt \\\n",
    "  -output wordcount-output  \\\n",
    "  -numReduceTasks 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n---------------------------\\n\n",
      "hello this is Jimi\n",
      "jimi who Jimi three Jimi \n",
      "Hello\n",
      "hello\\n---------------------------\\n\n",
      "Hello\t1\n",
      "jimi\t1\n",
      "this\t1\n",
      "three\t1\n",
      "Jimi\t3\n",
      "hello\t2\n",
      "is\t1\n",
      "who\t1\n"
     ]
    }
   ],
   "source": [
    "#have a look at the input\n",
    "!echo  \"\\n---------------------------\\n\"\n",
    "!hdfs dfs -cat testWordCountInput.txt\n",
    "!echo  \"\\n---------------------------\\n\"\n",
    "# Wordcount output\n",
    "!hdfs dfs -cat wordcount-output/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.1. Sort in Hadoop MapReduce  (Partial sort,  total sort) - List in alphabetical order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem use Alices Adventures in Wonderland. (You can obtain a free plain text version of the book, along with many others, from [here](http://www.gutenberg.org.)\n",
    "\n",
    "Using Hadoop, please change the mapper.py/reducer.py combination so that you get only the number of words starting with an uppercase letter, and the number of words starting with a lowercase letter. In other words, you need an output file with only 2 lines, one giving you the number of words staring with a lowercase ('a' to 'z'), and the other line indicating the number of words starting with an uppercase letter ('A' to 'Z'). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Some background on Sorting in Hadoop__\n",
    "Hadoop will always give a total sort on the key (i.e., key part of the key-value pairs produced by the mappers) when using just one reducer. When using multiple reducers Hadoop will by default give you a partial sort (i.e., all records within a partition will be sorted by the key (i.e., key part of the key-value pairs produced by the mappers) .\n",
    "To achieve a total sort one needs to write a custom mapper to to prepend a partition key to each record, partition on that prepended key, and then do a secondary sort  on a composite key that is made up of the prepended key and the original key.  This can be done with one map-reduce job. This will be covered during Live Session of Week 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  169k  100  169k    0     0   265k      0 --:--:-- --:--:-- --:--:--  345k\n"
     ]
    }
   ],
   "source": [
    "!curl 'http://www.gutenberg.org/files/11/11-0.txt' -o alicesTExtFilename.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Gutenbergs Alices Adventures in Wonderland, by Lewis Carroll\r",
      "\r\n",
      "\r",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r",
      "\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r",
      "\r\n",
      "re-use it under the terms of the Project Gutenberg License included\r",
      "\r\n",
      "with this eBook or online at www.gutenberg.org\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "Title: Alices Adventures in Wonderland\r",
      "\r\n",
      "\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "#display the first few lines\n",
    "!head alicesTExtFilename.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `UpperLower': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir UpperLower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting UpperLower/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile UpperLower/mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "#sys.stderr.write(\"reporter:counter:Tokens,Total,1\") # NOTE missing the carriage return so wont work\n",
    "# Set up counters to monitor/understand the number of times a mapper task is run\n",
    "sys.stderr.write(\"reporter:counter:HW2.1 Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing my message...how are  you\\n\")\n",
    "\n",
    "# START STUDENT CODE HW21MAPPER\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    " #for each document create dictionary of words\n",
    "\n",
    "# START STUDENT CODE HW13MAPPER\n",
    "for line in sys.stdin:\n",
    "     \n",
    "  \n",
    "    #Upper case letters\n",
    "    for word in re.findall(r'\\b[A-Z][a-z]*\\b',line):\n",
    "       \n",
    "        print '%s\\t%s' % ('upper', 1)\n",
    "    #lower case letters\n",
    "    for word in re.findall(r'\\b[a-z][a-z]*\\b',line):\n",
    "       \n",
    "        print '%s\\t%s' % ('lower', 1)    \n",
    "\n",
    "# END STUDENT CODE HW21MAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting UpperLower/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile UpperLower/reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "# Set up counters to monitor/understand the number of times a reducer task is run\n",
    "sys.stderr.write(\"reporter:counter:HW2.1 Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "\n",
    "# START STUDENT CODE HW21REDUCER\n",
    "\n",
    "\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)\n",
    "\n",
    "\n",
    "# END STUDENT CODE HW21REDUCER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INSTRUCTIONS: make mapper and reducer py files executable\n",
    "# START STUDENT CODE HW21EXECUTABLE\n",
    "\n",
    "!chmod a+x UpperLower/mapper.py\n",
    "!chmod a+x UpperLower/reducer.py\n",
    "\n",
    "# END STUDENT CODE HW21EXECUTABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:HW2.1 Reducer Counters,Calls,1\n",
      "reporter:counter:HW2.1 Mapper Counters,Calls,1\n",
      "reporter:status:processing my message...how are  you\n",
      "lower\t5\n",
      "upper\t2\n"
     ]
    }
   ],
   "source": [
    "#Systems test the mapper and reducer\n",
    "!echo \"foo Foo Quux labs foo bar quux\" |UpperLower/mapper.py | sort -k1,1 | UpperLower/reducer.py| sort -k2,2nr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted alicesTExtFilename.txt\n",
      "Deleted upperlower-output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob8489772692682918226.jar tmpDir=null\n",
      "17/05/22 22:58:33 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/22 22:58:33 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/22 22:58:34 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/05/22 22:58:34 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/05/22 22:58:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1495464826300_0058\n",
      "17/05/22 22:58:35 INFO impl.YarnClientImpl: Submitted application application_1495464826300_0058\n",
      "17/05/22 22:58:35 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1495464826300_0058/\n",
      "17/05/22 22:58:35 INFO mapreduce.Job: Running job: job_1495464826300_0058\n",
      "17/05/22 22:58:42 INFO mapreduce.Job: Job job_1495464826300_0058 running in uber mode : false\n",
      "17/05/22 22:58:43 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/05/22 22:58:52 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/05/22 22:58:53 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/05/22 22:58:59 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/05/22 22:58:59 INFO mapreduce.Job: Job job_1495464826300_0058 completed successfully\n",
      "17/05/22 22:58:59 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=299416\n",
      "\t\tFILE: Number of bytes written=950107\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=177923\n",
      "\t\tHDFS: Number of bytes written=23\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12783\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4699\n",
      "\t\tTotal time spent by all map tasks (ms)=12783\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4699\n",
      "\t\tTotal vcore-seconds taken by all map tasks=12783\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4699\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=13089792\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4811776\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3736\n",
      "\t\tMap output records=29941\n",
      "\t\tMap output bytes=239528\n",
      "\t\tMap output materialized bytes=299422\n",
      "\t\tInput split bytes=232\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=299422\n",
      "\t\tReduce input records=29941\n",
      "\t\tReduce output records=2\n",
      "\t\tSpilled Records=59882\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=145\n",
      "\t\tCPU time spent (ms)=3980\n",
      "\t\tPhysical memory (bytes) snapshot=693334016\n",
      "\t\tVirtual memory (bytes) snapshot=4086812672\n",
      "\t\tTotal committed heap usage (bytes)=644349952\n",
      "\tHW2.1 Mapper Counters\n",
      "\t\tCalls=2\n",
      "\tHW2.1 Reducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=177691\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=23\n",
      "17/05/22 22:58:59 INFO streaming.StreamJob: Output directory: upperlower-output\n"
     ]
    }
   ],
   "source": [
    "# INSTRUCTIONS: call hadoop with one reducer. see example above.\n",
    "\n",
    "##################### IMPORTANT ########################\n",
    "# Make sure you have the correct paths to the jar file\n",
    "# as wel as the input and output files!!\n",
    "# make sure to include the -files option. Do NOT put \n",
    "# spaces between the file paths!\n",
    "########################################################\n",
    "\n",
    "\n",
    "\n",
    "# START STUDENT CODE HW21HADOOP\n",
    "\n",
    "!hdfs dfs -rm alicesTExtFilename.txt\n",
    "!hdfs dfs -copyFromLocal alicesTExtFilename.txt\n",
    "!hdfs dfs -rm -r upperlower-output\n",
    "\n",
    "##################### IMPORTANT ########################################################################\n",
    "# Make sure you have the correct paths to the jar file as wel as the input and output files!!\n",
    "# make sure to include the -files option. Do ***** NOT ****** put spaces between the file paths!\n",
    "########################################################################################################\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -files UpperLower/reducer.py,UpperLower/mapper.py\\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input alicesTExtFilename.txt \\\n",
    "  -output upperlower-output  \\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "  \n",
    "# END STUDENT CODE HW21HADOOP  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Wordcount output\n",
    "!hdfs dfs -cat upperlower-output/part-0000* > upperlower_counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower\t26181\r\n",
      "upper\t3760\r\n"
     ]
    }
   ],
   "source": [
    "!cat upperlower_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.1.1 Calculate the vocabulary size (number of unique words in the Alice book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this problem, a single reducer will suffice. One could use multiple reducers but then you would need a post processing step to aggregate the counts in the PART-000XX files.\n",
    "\n",
    "Write a map/reduce job to count the number of unique words in the Alice book.  \n",
    "Please verify your code with straight python code.   \n",
    "Do you get the same answer?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `Vocab': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Vocab/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Vocab/mapper.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW211MAPPER\n",
    "import sys\n",
    "import re\n",
    "\n",
    "for line in sys.stdin:\n",
    "  line = line.strip()\n",
    "  for word in re.findall(r'[a-z]+', line.lower()):\n",
    "    print word,\"\\t\",1\n",
    "\n",
    "\n",
    "# END STUDENT CODE HW211MAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Vocab/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Vocab/reducer.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW211REDUCER\n",
    "import sys\n",
    "import re\n",
    "cur_key = None\n",
    "total_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key!= cur_key:\n",
    "        cur_key = key\n",
    "        total_count = total_count + 1\n",
    "    \n",
    "\n",
    "print '%s\\t%s' % ('vocab_size',  total_count)\n",
    "\n",
    "\n",
    "# END STUDENT CODE HW211REDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size\t4\r\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x Vocab/mapper.py\n",
    "!chmod a+x Vocab/reducer.py\n",
    "#Systems test the mapper and reducer\n",
    "!echo \"foo Foo quux labs foo bar quux\" |Vocab/mapper.py| sort -k1,1 | Vocab/reducer.py| sort -k2,2nr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted alicesTExtFilename.txt\n",
      "Deleted vocab-output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob5177389257086697434.jar tmpDir=null\n",
      "17/05/22 22:59:30 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/22 22:59:30 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/22 22:59:32 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/05/22 22:59:32 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/05/22 22:59:33 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1495464826300_0059\n",
      "17/05/22 22:59:33 INFO impl.YarnClientImpl: Submitted application application_1495464826300_0059\n",
      "17/05/22 22:59:33 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1495464826300_0059/\n",
      "17/05/22 22:59:33 INFO mapreduce.Job: Running job: job_1495464826300_0059\n",
      "17/05/22 22:59:41 INFO mapreduce.Job: Job job_1495464826300_0059 running in uber mode : false\n",
      "17/05/22 22:59:41 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/05/22 22:59:53 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/05/22 23:00:02 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/05/22 23:00:03 INFO mapreduce.Job: Job job_1495464826300_0059 completed successfully\n",
      "17/05/22 23:00:03 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=305555\n",
      "\t\tFILE: Number of bytes written=962340\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=177923\n",
      "\t\tHDFS: Number of bytes written=16\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17205\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6565\n",
      "\t\tTotal time spent by all map tasks (ms)=17205\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6565\n",
      "\t\tTotal vcore-seconds taken by all map tasks=17205\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=6565\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=17617920\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6722560\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3736\n",
      "\t\tMap output records=30423\n",
      "\t\tMap output bytes=244703\n",
      "\t\tMap output materialized bytes=305561\n",
      "\t\tInput split bytes=232\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3009\n",
      "\t\tReduce shuffle bytes=305561\n",
      "\t\tReduce input records=30423\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=60846\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=279\n",
      "\t\tCPU time spent (ms)=5820\n",
      "\t\tPhysical memory (bytes) snapshot=747171840\n",
      "\t\tVirtual memory (bytes) snapshot=4098338816\n",
      "\t\tTotal committed heap usage (bytes)=643825664\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=177691\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=16\n",
      "17/05/22 23:00:03 INFO streaming.StreamJob: Output directory: vocab-output\n"
     ]
    }
   ],
   "source": [
    "# START STUDENT CODE HW211HADOOP\n",
    "\n",
    "\n",
    "!hdfs dfs -rm alicesTExtFilename.txt\n",
    "!hdfs dfs -copyFromLocal alicesTExtFilename.txt\n",
    "!hdfs dfs -rm -r vocab-output\n",
    "\n",
    "##################### IMPORTANT ########################################################################\n",
    "# Make sure you have the correct paths to the jar file as wel as the input and output files!!\n",
    "# make sure to include the -files option. Do ***** NOT ****** put spaces between the file paths!\n",
    "########################################################################################################\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -files Vocab/reducer.py,Vocab/mapper.py\\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input alicesTExtFilename.txt \\\n",
    "  -output vocab-output  \\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "# END STUDENT CODE HW211HADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat vocab-output/part-0000* > vocab_output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size\t3009\r\n"
     ]
    }
   ],
   "source": [
    "!cat vocab_output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.1.2  TOTAL SORT using a single reducer\n",
    "Write a MapReduce job that creates a text file named __alice_words.txt__ containing an alphabetical listing of all the words, and the number of times each occurs, in the text version of Alices Adventures in Wonderland. (You can obtain a free plain text version of the book, along with many others, from [here](http://www.gutenberg.org/cache/epub/11/pg11.txt) \n",
    "\n",
    "Solve this TOTAL SORT problem in mapReduce using a single reducer.\n",
    "\n",
    "The first 10 lines of your output file should look something like this (the counts are not totally precise):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Word              Count\n",
    "=======================\n",
    "a                 631\n",
    "a-piece           1\n",
    "abide             1\n",
    "able              1\n",
    "about             94\n",
    "above             3\n",
    "absence           1\n",
    "absurd            2\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `Total_sort': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir Total_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Total_sort/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Total_sort/mapper.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW212MAPPER\n",
    "import sys\n",
    "import re\n",
    "\n",
    "for line in sys.stdin:\n",
    "  line = line.strip()\n",
    "  for word in re.findall(r'[a-z]+', line.lower()):\n",
    "    print word,\"\\t\",1\n",
    "\n",
    "\n",
    "# END STUDENT CODE HW212MAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Total_sort/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Total_sort/reducer.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW212REDUCER\n",
    "import sys\n",
    "import re\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)\n",
    "\n",
    "# END STUDENT CODE HW212REDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x Total_sort/mapper.py\n",
    "!chmod a+x Total_sort/reducer.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\t3\r\n",
      "quux\t2\r\n",
      "bar\t1\r\n",
      "labs\t1\r\n"
     ]
    }
   ],
   "source": [
    "#Systems test the mapper and reducer\n",
    "!echo \"foo Foo quux labs foo bar quux\" |Total_sort/mapper.py| sort -k1,1 | Total_sort/reducer.py| sort -k2,2nr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted alicesTExtFilename.txt\n",
      "Deleted sorted-output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob1645744425806474632.jar tmpDir=null\n",
      "17/05/22 23:00:23 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/22 23:00:23 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/22 23:00:24 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/05/22 23:00:24 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/05/22 23:00:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1495464826300_0060\n",
      "17/05/22 23:00:25 INFO impl.YarnClientImpl: Submitted application application_1495464826300_0060\n",
      "17/05/22 23:00:25 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1495464826300_0060/\n",
      "17/05/22 23:00:25 INFO mapreduce.Job: Running job: job_1495464826300_0060\n",
      "17/05/22 23:00:39 INFO mapreduce.Job: Job job_1495464826300_0060 running in uber mode : false\n",
      "17/05/22 23:00:39 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/05/22 23:00:49 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/05/22 23:00:50 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/05/22 23:00:58 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/05/22 23:00:59 INFO mapreduce.Job: Job job_1495464826300_0060 completed successfully\n",
      "17/05/22 23:01:00 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=305555\n",
      "\t\tFILE: Number of bytes written=963450\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=177923\n",
      "\t\tHDFS: Number of bytes written=28506\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17484\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6376\n",
      "\t\tTotal time spent by all map tasks (ms)=17484\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6376\n",
      "\t\tTotal vcore-seconds taken by all map tasks=17484\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=6376\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=17903616\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6529024\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3736\n",
      "\t\tMap output records=30423\n",
      "\t\tMap output bytes=244703\n",
      "\t\tMap output materialized bytes=305561\n",
      "\t\tInput split bytes=232\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3009\n",
      "\t\tReduce shuffle bytes=305561\n",
      "\t\tReduce input records=30423\n",
      "\t\tReduce output records=3009\n",
      "\t\tSpilled Records=60846\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=163\n",
      "\t\tCPU time spent (ms)=6230\n",
      "\t\tPhysical memory (bytes) snapshot=795852800\n",
      "\t\tVirtual memory (bytes) snapshot=4102533120\n",
      "\t\tTotal committed heap usage (bytes)=640679936\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=177691\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=28506\n",
      "17/05/22 23:01:00 INFO streaming.StreamJob: Output directory: sorted-output\n"
     ]
    }
   ],
   "source": [
    "# START STUDENT CODE HW212HADOOP\n",
    "!hdfs dfs -rm alicesTExtFilename.txt\n",
    "!hdfs dfs -copyFromLocal alicesTExtFilename.txt\n",
    "!hdfs dfs -rm -r sorted-output\n",
    "\n",
    "##################### IMPORTANT ########################################################################\n",
    "# Make sure you have the correct paths to the jar file as wel as the input and output files!!\n",
    "# make sure to include the -files option. Do ***** NOT ****** put spaces between the file paths!\n",
    "########################################################################################################\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1\"  \\\n",
    "  -files Total_sort/reducer.py,Total_sort/mapper.py\\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input alicesTExtFilename.txt \\\n",
    "  -output sorted-output \\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "# END STUDENT CODE HW212HADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat sorted-output/part-0000* > sorted_output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t690\r\n",
      "abide\t2\r\n",
      "able\t1\r\n",
      "about\t102\r\n",
      "above\t3\r\n",
      "absence\t1\r\n",
      "absurd\t2\r\n",
      "accept\t1\r\n",
      "acceptance\t1\r\n",
      "accepted\t2\r\n"
     ]
    }
   ],
   "source": [
    "!head sorted_output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.1.2.b TOTAL SORT using multiple reducers [OPTITIONAL for this week; will be covered in next live session]\n",
    "\n",
    "Change the mapper.py/reducer.py combination from the the above WordCount example so that you get the longest word present in the text version of Alices Adventures in Wonderland. (You can obtain a free plain text version of the book, along with many others, from [here](http://www.gutenberg.org/cache/epub/11/pg11.txt). \n",
    "\n",
    "* First use one reducer and report your result. HINT: from emit records of the form: \"longestWord\\theLongWordEver\\t15\".  \n",
    "* Run you Hadoop streaming job with 3 reducers? Anything change with respect to your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `Total_sort_multi': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir Total_sort_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Total_sort_multi/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Total_sort_multi/mapper.py\n",
    "# START STUDENT CODE HW212MAPPER_MULTI\n",
    "from __future__ import division\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "count = 0\n",
    "numReducers = int(os.environ.get('NUM_PARTITIONS', '4')) # default to 4\n",
    "\n",
    "##################################### PARTITION THE DATA INTO N BUCKETS ####################################\n",
    "# lowercase chars range from 97 -> 122.\n",
    "# there are 26 chars. So let's partion by spliting at 26/n \n",
    "# ord('a') -> 97\n",
    "# chr(97) -> 'a'\n",
    "# ord(word[0])-96  --> get the number of the first letter between 1 and 26, such that a -> 1; z -> 26\n",
    "############################################################################################################\n",
    "\n",
    "def makeKey(word,n):\n",
    "  divisor = 26/n\n",
    "  return int(math.ceil((ord(word[0])-96)/divisor))\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "  line = line.strip()\n",
    "  for word in re.findall(r'[a-z]+', line.lower()):\n",
    "    # prepend a key based on the number of reducers\n",
    "    key = makeKey(word,numReducers)\n",
    "    print key,\"\\t\",word,\"\\t\",1\n",
    "\n",
    "# END STUDENT CODE HW212MAPPER_MULTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Total_sort_multi/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile  Total_sort_multi/reducer.py\n",
    "# START STUDENT CODE HW212REDUCER_MULTI\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    key, word, count = line.split('\\t')\n",
    "    count = int(count)\n",
    "    \n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s\\t%s' % (key,current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    " \n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s\\t%s' % (key,current_word, current_count)\n",
    "\n",
    "# END STUDENT CODE HW212REDUCER_MULTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x Total_sort_multi/mapper.py\n",
    "!chmod a+x Total_sort_multi/reducer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 \tapple \t1\r\n",
      "2 \thello \t1\r\n",
      "3 \thi \t1\r\n",
      "3 \tsir \t1\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"hi hello sir Apple\"|python Total_sort_multi/mapper.py| sort -k1,1 | python Total_sort_multi/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.1.3 How many times does the word alice occur in the book?\n",
    "Write a MapReduce job to determine this. Please pay attention to what you use for a  key and value as output from your mapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### START STUDENT CODE HW212HADOOP_MULTI\n",
    "!hdfs dfs -rm alicesTExtFilename.txt\n",
    "!hdfs dfs -copyFromLocal alicesTExtFilename.txt\n",
    "!hdfs dfs -rm -r sorted-multi\n",
    "\n",
    "##################### IMPORTANT ########################################################################\n",
    "#### Make sure you have the correct paths to the jar file as wel as the input and output files!!\n",
    "#### make sure to include the -files option. Do ***** NOT ****** put spaces between the file paths!\n",
    "########################################################################################################\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D stream.num.map.output.key.fields=3 \\\n",
    "    -D stream.map.output.field.separator=\"\\t\" \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2 -k3,3\" \\\n",
    "    -files Total_sort_multi\\\n",
    "    -mapper \"python Total_sort_multi/mapper.py\" \\\n",
    "    -reducer \"python Total_sort_multi/reducer.py\" \\\n",
    "    -input alicesTExtFilename.txt \\\n",
    "    -output sorted-multi \\\n",
    "    -numReduceTasks 1 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -cmdenv NUM_PARTITIONS=4\n",
    "\n",
    "#### END STUDENT CODE HW212HADOOP_MULTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `Alice': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir Alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Alice/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Alice/mapper.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW213MAPPER\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "for line in sys.stdin:\n",
    "  line = line.strip()\n",
    "  for word in re.findall(r'[a-z]+', line.lower()):\n",
    "    if word  == 'alice':\n",
    "        print word,\"\\t\",1\n",
    "\n",
    "# END STUDENT CODE HW213MAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Alice/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Alice/reducer.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW213REDUCER\n",
    "import sys\n",
    "import re\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)\n",
    "\n",
    "# END STUDENT CODE HW213REDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x Alice/mapper.py\n",
    "!chmod a+x Alice/reducer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted alicesTExtFilename.txt\n",
      "Deleted alice-output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob5095518826391804737.jar tmpDir=null\n",
      "17/05/22 23:04:17 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/22 23:04:17 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/22 23:04:19 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/05/22 23:04:19 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/05/22 23:04:19 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1495464826300_0061\n",
      "17/05/22 23:04:20 INFO impl.YarnClientImpl: Submitted application application_1495464826300_0061\n",
      "17/05/22 23:04:20 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1495464826300_0061/\n",
      "17/05/22 23:04:20 INFO mapreduce.Job: Running job: job_1495464826300_0061\n",
      "17/05/22 23:04:29 INFO mapreduce.Job: Job job_1495464826300_0061 running in uber mode : false\n",
      "17/05/22 23:04:29 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/05/22 23:04:43 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/05/22 23:04:44 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/05/22 23:04:54 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/05/22 23:04:55 INFO mapreduce.Job: Job job_1495464826300_0061 completed successfully\n",
      "17/05/22 23:04:55 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4439\n",
      "\t\tFILE: Number of bytes written=361185\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=177923\n",
      "\t\tHDFS: Number of bytes written=10\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=24911\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=7959\n",
      "\t\tTotal time spent by all map tasks (ms)=24911\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7959\n",
      "\t\tTotal vcore-seconds taken by all map tasks=24911\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=7959\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=25508864\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=8150016\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3736\n",
      "\t\tMap output records=403\n",
      "\t\tMap output bytes=3627\n",
      "\t\tMap output materialized bytes=4445\n",
      "\t\tInput split bytes=232\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=4445\n",
      "\t\tReduce input records=403\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=806\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=238\n",
      "\t\tCPU time spent (ms)=4310\n",
      "\t\tPhysical memory (bytes) snapshot=771096576\n",
      "\t\tVirtual memory (bytes) snapshot=4089450496\n",
      "\t\tTotal committed heap usage (bytes)=639107072\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=177691\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=10\n",
      "17/05/22 23:04:55 INFO streaming.StreamJob: Output directory: alice-output\n"
     ]
    }
   ],
   "source": [
    "# START STUDENT CODE HW213HADOOP\n",
    "!hdfs dfs -rm alicesTExtFilename.txt\n",
    "!hdfs dfs -copyFromLocal alicesTExtFilename.txt\n",
    "!hdfs dfs -rm -r alice-output\n",
    "\n",
    "##################### IMPORTANT ########################################################################\n",
    "# Make sure you have the correct paths to the jar file as wel as the input and output files!!\n",
    "# make sure to include the -files option. Do ***** NOT ****** put spaces between the file paths!\n",
    "########################################################################################################\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1\"  \\\n",
    "  -files Alice/reducer.py,Alice/mapper.py\\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input alicesTExtFilename.txt \\\n",
    "  -output alice-output\\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "# END STUDENT CODE HW213HADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat alice-output/part-0000* > alice_count.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\t403\r\n"
     ]
    }
   ],
   "source": [
    "!cat alice_count.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.2 EDA using WORDCOUNT in  Hadoop - Top 10 Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of word splitting\n",
    "A tokenizer divides text into a sequence of tokens, which roughly correspond to \"words\". We provide a some code here for tokenization of English language strings. For detecting words in HW2.2 and later problems  please use  the following code to generate word tokens (otherwise, you will probably get a different answer to expected).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global \t1\n",
      "risk \t1\n",
      "management \t1\n",
      "operations \t1\n",
      "congratulations \t1\n",
      "sally \t1\n",
      "kk \t1\n",
      "forwarded \t1\n",
      "by \t1\n",
      "kathy \t1\n",
      "kokas \t1\n",
      "corp \t1\n",
      "enron \t1\n",
      "on \t1\n",
      "01 \t1\n",
      "17 \t1\n",
      "2000 \t1\n",
      "08 \t1\n",
      "08 \t1\n",
      "pm \t1\n",
      "from \t1\n",
      "rick \t1\n",
      "causey \t1\n",
      "01 \t1\n",
      "17 \t1\n",
      "2000 \t1\n",
      "06 \t1\n",
      "04 \t1\n",
      "pm \t1\n",
      "sent \t1\n",
      "by \t1\n",
      "enron \t1\n",
      "announcements \t1\n",
      "to \t1\n",
      "all \t1\n",
      "enron \t1\n",
      "worldwide \t1\n",
      "cc \t1\n",
      "subject \t1\n",
      "global \t1\n",
      "risk \t1\n",
      "management \t1\n",
      "operations \t1\n",
      "recognizing \t1\n",
      "enron \t1\n",
      "\u0001 \t1\n",
      "s \t1\n",
      "increasing \t1\n",
      "worldwide \t1\n",
      "presence \t1\n",
      "in \t1\n",
      "the \t1\n",
      "wholesale \t1\n",
      "energy \t1\n",
      "business \t1\n",
      "and \t1\n",
      "the \t1\n",
      "need \t1\n",
      "to \t1\n",
      "insure \t1\n",
      "outstanding \t1\n",
      "internal \t1\n",
      "controls \t1\n",
      "for \t1\n",
      "all \t1\n",
      "of \t1\n",
      "our \t1\n",
      "risk \t1\n",
      "management \t1\n",
      "activities \t1\n",
      "regardless \t1\n",
      "of \t1\n",
      "location \t1\n",
      "a \t1\n",
      "global \t1\n",
      "risk \t1\n",
      "management \t1\n",
      "operations \t1\n",
      "function \t1\n",
      "has \t1\n",
      "been \t1\n",
      "created \t1\n",
      "under \t1\n",
      "the \t1\n",
      "direction \t1\n",
      "of \t1\n",
      "sally \t1\n",
      "w \t1\n",
      "beck \t1\n",
      "vice \t1\n",
      "president \t1\n",
      "in \t1\n",
      "this \t1\n",
      "role \t1\n",
      "sally \t1\n",
      "will \t1\n",
      "report \t1\n",
      "to \t1\n",
      "rick \t1\n",
      "causey \t1\n",
      "executive \t1\n",
      "vice \t1\n",
      "president \t1\n",
      "and \t1\n",
      "chief \t1\n",
      "accounting \t1\n",
      "officer \t1\n",
      "sally \t1\n",
      "\u0001 \t1\n",
      "s \t1\n",
      "responsibilities \t1\n",
      "with \t1\n",
      "regard \t1\n",
      "to \t1\n",
      "global \t1\n",
      "risk \t1\n",
      "management \t1\n",
      "operations \t1\n",
      "will \t1\n",
      "mirror \t1\n",
      "those \t1\n",
      "of \t1\n",
      "other \t1\n",
      "recently \t1\n",
      "created \t1\n",
      "enron \t1\n",
      "global \t1\n",
      "functions \t1\n",
      "in \t1\n",
      "this \t1\n",
      "role \t1\n",
      "sally \t1\n",
      "will \t1\n",
      "work \t1\n",
      "closely \t1\n",
      "with \t1\n",
      "all \t1\n",
      "enron \t1\n",
      "geographic \t1\n",
      "regions \t1\n",
      "and \t1\n",
      "wholesale \t1\n",
      "companies \t1\n",
      "to \t1\n",
      "insure \t1\n",
      "that \t1\n",
      "each \t1\n",
      "entity \t1\n",
      "receives \t1\n",
      "individualized \t1\n",
      "regional \t1\n",
      "support \t1\n",
      "while \t1\n",
      "also \t1\n",
      "focusing \t1\n",
      "on \t1\n",
      "the \t1\n",
      "following \t1\n",
      "global \t1\n",
      "responsibilities \t1\n",
      "1 \t1\n",
      "enhance \t1\n",
      "communication \t1\n",
      "among \t1\n",
      "risk \t1\n",
      "management \t1\n",
      "operations \t1\n",
      "professionals \t1\n",
      "2 \t1\n",
      "assure \t1\n",
      "the \t1\n",
      "proliferation \t1\n",
      "of \t1\n",
      "best \t1\n",
      "operational \t1\n",
      "practices \t1\n",
      "around \t1\n",
      "the \t1\n",
      "globe \t1\n",
      "3 \t1\n",
      "facilitate \t1\n",
      "the \t1\n",
      "allocation \t1\n",
      "of \t1\n",
      "human \t1\n",
      "resources \t1\n",
      "4 \t1\n",
      "provide \t1\n",
      "training \t1\n",
      "for \t1\n",
      "risk \t1\n",
      "management \t1\n",
      "operations \t1\n",
      "personnel \t1\n",
      "5 \t1\n",
      "coordinate \t1\n",
      "user \t1\n",
      "requirements \t1\n",
      "for \t1\n",
      "shared \t1\n",
      "operational \t1\n",
      "systems \t1\n",
      "6 \t1\n",
      "oversee \t1\n",
      "the \t1\n",
      "creation \t1\n",
      "of \t1\n",
      "a \t1\n",
      "global \t1\n",
      "internal \t1\n",
      "control \t1\n",
      "audit \t1\n",
      "plan \t1\n",
      "for \t1\n",
      "risk \t1\n",
      "management \t1\n",
      "activities \t1\n",
      "7 \t1\n",
      "establish \t1\n",
      "procedures \t1\n",
      "for \t1\n",
      "opening \t1\n",
      "new \t1\n",
      "risk \t1\n",
      "management \t1\n",
      "operations \t1\n",
      "offices \t1\n",
      "and \t1\n",
      "create \t1\n",
      "key \t1\n",
      "benchmarks \t1\n",
      "for \t1\n",
      "measuring \t1\n",
      "on \t1\n",
      "going \t1\n",
      "risk \t1\n",
      "controls \t1\n",
      "each \t1\n",
      "regional \t1\n",
      "operations \t1\n",
      "team \t1\n",
      "will \t1\n",
      "continue \t1\n",
      "its \t1\n",
      "direct \t1\n",
      "reporting \t1\n",
      "relationship \t1\n",
      "within \t1\n",
      "its \t1\n",
      "business \t1\n",
      "unit \t1\n",
      "and \t1\n",
      "will \t1\n",
      "collaborate \t1\n",
      "with \t1\n",
      "sally \t1\n",
      "in \t1\n",
      "the \t1\n",
      "delivery \t1\n",
      "of \t1\n",
      "these \t1\n",
      "critical \t1\n",
      "items \t1\n",
      "the \t1\n",
      "houston \t1\n",
      "based \t1\n",
      "risk \t1\n",
      "management \t1\n",
      "operations \t1\n",
      "team \t1\n",
      "under \t1\n",
      "sue \t1\n",
      "frusco \t1\n",
      "\u0001 \t1\n",
      "s \t1\n",
      "leadership \t1\n",
      "which \t1\n",
      "currently \t1\n",
      "supports \t1\n",
      "risk \t1\n",
      "management \t1\n",
      "activities \t1\n",
      "for \t1\n",
      "south \t1\n",
      "america \t1\n",
      "and \t1\n",
      "australia \t1\n",
      "will \t1\n",
      "also \t1\n",
      "report \t1\n",
      "directly \t1\n",
      "to \t1\n",
      "sally \t1\n",
      "sally \t1\n",
      "retains \t1\n",
      "her \t1\n",
      "role \t1\n",
      "as \t1\n",
      "vice \t1\n",
      "president \t1\n",
      "of \t1\n",
      "energy \t1\n",
      "operations \t1\n",
      "for \t1\n",
      "enron \t1\n",
      "north \t1\n",
      "america \t1\n",
      "reporting \t1\n",
      "to \t1\n",
      "the \t1\n",
      "ena \t1\n",
      "office \t1\n",
      "of \t1\n",
      "the \t1\n",
      "chairman \t1\n",
      "she \t1\n",
      "has \t1\n",
      "been \t1\n",
      "in \t1\n",
      "her \t1\n",
      "current \t1\n",
      "role \t1\n",
      "over \t1\n",
      "energy \t1\n",
      "operations \t1\n",
      "since \t1\n",
      "1997 \t1\n",
      "where \t1\n",
      "she \t1\n",
      "manages \t1\n",
      "risk \t1\n",
      "consolidation \t1\n",
      "and \t1\n",
      "reporting \t1\n",
      "risk \t1\n",
      "management \t1\n",
      "administration \t1\n",
      "physical \t1\n",
      "product \t1\n",
      "delivery \t1\n",
      "confirmations \t1\n",
      "and \t1\n",
      "cash \t1\n",
      "management \t1\n",
      "for \t1\n",
      "ena \t1\n",
      "\u0001 \t1\n",
      "s \t1\n",
      "physical \t1\n",
      "commodity \t1\n",
      "trading \t1\n",
      "energy \t1\n",
      "derivatives \t1\n",
      "trading \t1\n",
      "and \t1\n",
      "financial \t1\n",
      "products \t1\n",
      "trading \t1\n",
      "sally \t1\n",
      "has \t1\n",
      "been \t1\n",
      "with \t1\n",
      "enron \t1\n",
      "since \t1\n",
      "1992 \t1\n",
      "when \t1\n",
      "she \t1\n",
      "joined \t1\n",
      "the \t1\n",
      "company \t1\n",
      "as \t1\n",
      "a \t1\n",
      "manager \t1\n",
      "in \t1\n",
      "global \t1\n",
      "credit \t1\n",
      "prior \t1\n",
      "to \t1\n",
      "joining \t1\n",
      "enron \t1\n",
      "sally \t1\n",
      "had \t1\n",
      "four \t1\n",
      "years \t1\n",
      "experience \t1\n",
      "as \t1\n",
      "a \t1\n",
      "commercial \t1\n",
      "banker \t1\n",
      "and \t1\n",
      "spent \t1\n",
      "seven \t1\n",
      "years \t1\n",
      "as \t1\n",
      "a \t1\n",
      "registered \t1\n",
      "securities \t1\n",
      "principal \t1\n",
      "with \t1\n",
      "a \t1\n",
      "regional \t1\n",
      "investment \t1\n",
      "banking \t1\n",
      "firm \t1\n",
      "she \t1\n",
      "also \t1\n",
      "owned \t1\n",
      "and \t1\n",
      "managed \t1\n",
      "a \t1\n",
      "retail \t1\n",
      "business \t1\n",
      "for \t1\n",
      "several \t1\n",
      "years \t1\n",
      "please \t1\n",
      "join \t1\n",
      "me \t1\n",
      "in \t1\n",
      "supporting \t1\n",
      "sally \t1\n",
      "in \t1\n",
      "this \t1\n",
      "additional \t1\n",
      "coordination \t1\n",
      "role \t1\n",
      "for \t1\n",
      "global \t1\n",
      "risk \t1\n",
      "management \t1\n",
      "operations \t1\n"
     ]
    }
   ],
   "source": [
    "import sys, re, string\n",
    "# define regex for punctuation removal\n",
    "\n",
    "\n",
    "line = \"\"\" 0017.2000-01-17.beck\t0\t global risk management operations\t\" congratulations, sally!!!  kk  ----------------------forwarded by kathy kokas/corp/enron on 01/17/2000  08:08 pm---------------------------  from: rick causey 01/17/2000 06:04 pm  sent by: enron announcements  to: all enron worldwide  cc:  subject: global risk management operations  recognizing enron \u0001, s increasing worldwide presence in the wholesale energy  business and the need to insure outstanding internal controls for all of our  risk management activities, regardless of location, a global risk management  operations function has been created under the direction of sally w. beck,  vice president. in this role, sally will report to rick causey, executive  vice president and chief accounting officer.  sally \u0001, s responsibilities with regard to global risk management operations  will mirror those of other recently created enron global functions. in this  role, sally will work closely with all enron geographic regions and wholesale  companies to insure that each entity receives individualized regional support  while also focusing on the following global responsibilities:  1. enhance communication among risk management operations professionals.  2. assure the proliferation of best operational practices around the globe.  3. facilitate the allocation of human resources.  4. provide training for risk management operations personnel.  5. coordinate user requirements for shared operational systems.  6. oversee the creation of a global internal control audit plan for risk  management activities.  7. establish procedures for opening new risk management operations offices  and create key benchmarks for measuring on-going risk controls.  each regional operations team will continue its direct reporting relationship  within its business unit, and will collaborate with sally in the delivery of  these critical items. the houston-based risk management operations team under  sue frusco \u0001, s leadership, which currently supports risk management activities  for south america and australia, will also report directly to sally.  sally retains her role as vice president of energy operations for enron  north america, reporting to the ena office of the chairman. she has been in  her current role over energy operations since 1997, where she manages risk  consolidation and reporting, risk management administration, physical product  delivery, confirmations and cash management for ena \u0001, s physical commodity  trading, energy derivatives trading and financial products trading.  sally has been with enron since 1992, when she joined the company as a  manager in global credit. prior to joining enron, sally had four years  experience as a commercial banker and spent seven years as a registered  securities principal with a regional investment banking firm. she also owned  and managed a retail business for several years.  please join me in supporting sally in this additional coordination role for  global risk management operations.\"\n",
    "\"\"\"\n",
    "docID, docClass,title,body = line.split(\"\\t\",3)   \n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "emailStr = regex.sub(' ', title + \" \" +body.lower())\n",
    "emailStr = re.sub( '\\s+', ' ', emailStr )\n",
    "# split the line into words\n",
    "words = emailStr.split()\n",
    "for w in words: \n",
    "    print w, \"\\t\", 1 #or yield(w, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2.1 WORDCOUNT\n",
    "\n",
    "Using the Enron data from and Hadoop MapReduce streaming, write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word assistance and report its word count in both SPAM and HAM classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CROSSCHECK the frequency using Unic commands (e.g., use multiple grep to get the frequency in each class): \n",
    "```\n",
    "grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l    \n",
    "       8    \n",
    "```\n",
    "__NOTE: __  \"assistance\" occurs on 8 lines but how many times does the token occur? 10 times! This is the number we are looking for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__OUTPUT:__   \n",
    "The expected output for 2.2.1 is given  as:   \n",
    "`assistance    10`    \n",
    "(assistance followed by a tab and the number 10). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `Enron': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir Enron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\r\n"
     ]
    }
   ],
   "source": [
    "!grep assistance NaiveBayes/enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Enron/mapper2.2.1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Enron/mapper2.2.1.py\n",
    "#!/usr/bin/env python\n",
    "import sys, re, string\n",
    "\n",
    "# START STUDENT CODE HW221MAPPER\n",
    "\n",
    "\n",
    "# define regex for punctuation removal\n",
    "#regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# input comes from STDIN (standard input)\n",
    "\n",
    "# use subject and body\n",
    "\n",
    "# remove punctuations, only have white-space as delimiter\n",
    "\n",
    "# write the results to STDOUT (standard output);\n",
    "# what we output here will be the input for the\n",
    "# Reduce step, i.e. the input for reducer.py\n",
    "#\n",
    "# tab-delimited; the trivial word count is 1\n",
    "for line in sys.stdin:\n",
    "    docID, docClass,title,body = line.split(\"\\t\",3)   \n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    emailStr = regex.sub(' ', title + \" \" +body.lower())\n",
    "    emailStr = re.sub( '\\s+', ' ', emailStr )\n",
    "# split the line into words\n",
    "    words = emailStr.split()\n",
    "    for w in words: \n",
    "        \n",
    "            print w, \"\\t\", 1 #or yield(w, 1)\n",
    "            \n",
    "# END STUDENT CODE HW221MAPPER            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Enron/reducer2.2.1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Enron/reducer2.2.1.py\n",
    "#!/usr/bin/env python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "# START STUDENT CODE HW221REDUCER\n",
    "import sys\n",
    "import re\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)\n",
    "\n",
    "       \n",
    "# END STUDENT CODE HW221REDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x Enron/mapper2.2.1.py\n",
    "!chmod a+x Enron/reducer2.2.1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted enronemail_1h.txt\n",
      "Deleted HW2.2.1/results\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob8585421861201439753.jar tmpDir=null\n",
      "17/05/22 23:06:06 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/22 23:06:07 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/22 23:06:08 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/05/22 23:06:08 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/05/22 23:06:09 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1495464826300_0062\n",
      "17/05/22 23:06:09 INFO impl.YarnClientImpl: Submitted application application_1495464826300_0062\n",
      "17/05/22 23:06:09 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1495464826300_0062/\n",
      "17/05/22 23:06:09 INFO mapreduce.Job: Running job: job_1495464826300_0062\n",
      "17/05/22 23:06:18 INFO mapreduce.Job: Job job_1495464826300_0062 running in uber mode : false\n",
      "17/05/22 23:06:18 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/05/22 23:06:27 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/05/22 23:06:36 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/05/22 23:06:36 INFO mapreduce.Job: Job job_1495464826300_0062 completed successfully\n",
      "17/05/22 23:06:36 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=351817\n",
      "\t\tFILE: Number of bytes written=1056085\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217089\n",
      "\t\tHDFS: Number of bytes written=52015\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=13785\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5339\n",
      "\t\tTotal time spent by all map tasks (ms)=13785\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5339\n",
      "\t\tTotal vcore-seconds taken by all map tasks=13785\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5339\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=14115840\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5467136\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=33203\n",
      "\t\tMap output bytes=285405\n",
      "\t\tMap output materialized bytes=351823\n",
      "\t\tInput split bytes=222\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5409\n",
      "\t\tReduce shuffle bytes=351823\n",
      "\t\tReduce input records=33203\n",
      "\t\tReduce output records=5409\n",
      "\t\tSpilled Records=66406\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=197\n",
      "\t\tCPU time spent (ms)=5070\n",
      "\t\tPhysical memory (bytes) snapshot=705929216\n",
      "\t\tVirtual memory (bytes) snapshot=4075671552\n",
      "\t\tTotal committed heap usage (bytes)=642777088\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=52015\n",
      "17/05/22 23:06:36 INFO streaming.StreamJob: Output directory: HW2.2.1/results/\n"
     ]
    }
   ],
   "source": [
    "# START STUDENT CODE HW221HADOOP\n",
    "!hdfs dfs -rm enronemail_1h.txt\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/enronemail_1h.txt\n",
    "!hdfs dfs -rm -r HW2.2.1/results/\n",
    "\n",
    "##################### IMPORTANT ########################################################################\n",
    "# Make sure you have the correct paths to the jar file as wel as the input and output files!!\n",
    "# make sure to include the -files option. Do ***** NOT ****** put spaces between the file paths!\n",
    "########################################################################################################\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1\"  \\\n",
    "  -files Enron/mapper2.2.1.py,Enron/reducer2.2.1.py\\\n",
    "  -mapper mapper2.2.1.py \\\n",
    "  -reducer reducer2.2.1.py \\\n",
    "  -input enronemail_1h.txt \\\n",
    "  -output HW2.2.1/results/\\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "       \n",
    "# END STUDENT CODE HW221HADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat HW2.2.1/results/part-0000* > enron_counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "!grep -i assistance enron_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2.2  \n",
    "Using Hadoop MapReduce and your wordcount job (from HW2.2.1) determine the top-10 occurring tokens (most frequent tokens) using a single reducer for the SPAM class and for the HAM class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected output for 2.2.2 is in terms of three tab-separated columns:    \n",
    " `CLASS\\tWORD\\tCOUNT`   \n",
    "with the HAM top 10 coming first followed by the SPAM top 10.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Enron/mapper2.2.2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Enron/mapper2.2.2.py\n",
    "#!/usr/bin/env python\n",
    "import sys, re, string\n",
    "\n",
    "# START STUDENT CODE HW221MAPPER\n",
    "\n",
    "\n",
    "# define regex for punctuation removal\n",
    "#regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# input comes from STDIN (standard input)\n",
    "\n",
    "# use subject and body\n",
    "\n",
    "# remove punctuations, only have white-space as delimiter\n",
    "\n",
    "# write the results to STDOUT (standard output);\n",
    "# what we output here will be the input for the\n",
    "# Reduce step, i.e. the input for reducer.py\n",
    "#\n",
    "# tab-delimited; the trivial word count is 1\n",
    "for line in sys.stdin:\n",
    "    docID, docClass,title,body = line.split(\"\\t\",3)   \n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    emailStr = regex.sub(' ', title + \" \" +body.lower())\n",
    "    emailStr = re.sub( '\\s+', ' ', emailStr )\n",
    "# split the line into words\n",
    "    words = emailStr.split()\n",
    "    for w in words: \n",
    "        \n",
    "            print \"%s\\t%d\\t%s\"%(w,1,docClass)# w, \"\\t\", 1,\"\\t\",docClass #or yield(w, 1)\n",
    "            \n",
    "# END STUDENT CODE HW222MAPPER            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Enron/reducer2.2.2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Enron/reducer2.2.2.py\n",
    "#!/usr/bin/env python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "# START STUDENT CODE HW221REDUCER\n",
    "import sys\n",
    "import re\n",
    "cur_key = None\n",
    "\n",
    "docClass = 0\n",
    "\n",
    "wordcount = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value ,docClass = line.split(\"\\t\")\n",
    "    docClass = docClass.rstrip()\n",
    "    if key == cur_key:\n",
    "        wordcount[(docClass,cur_key)]+= int(value)\n",
    "    else:\n",
    "        #if cur_key:\n",
    "        #    print '%s\\t%s' % (cur_key, wordcount[(docClass,cur_key)])\n",
    "        cur_key = key\n",
    "        \n",
    "        if docClass == '0':\n",
    "            wordcount[('0',cur_key)] = int(value)\n",
    "            wordcount[('1',cur_key)] = 0\n",
    "        else:\n",
    "            wordcount[('1',cur_key)] = int(value)\n",
    "            wordcount[('0',cur_key)] = 0\n",
    "        #cur_count = int(value)\n",
    "\n",
    "count  = 0\n",
    "#print wordcount\n",
    "for w in sorted(wordcount, key=wordcount.get, reverse=True):\n",
    "     if w[0] == \"0\" and count <11: \n",
    "        print '%s\\t%s\\t%s' %(w[0],w[1],wordcount[w])\n",
    "        count = count +1\n",
    "count  = 0\n",
    "for w in sorted(wordcount, key=wordcount.get, reverse=True):\n",
    "     if w[0] == \"1\" and count <11: \n",
    "        print '%s\\t%s\\t%s' % (w[0],w[1],wordcount[w])\n",
    "        count = count +1\n",
    "#print '%s\\t%s' % (cur_key, cur_count)\n",
    "\n",
    "       \n",
    "# END STUDENT CODE HW222REDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x Enron/mapper2.2.2.py\n",
    "!chmod a+x Enron/reducer2.2.2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tthank\t0\r\n",
      "0\trankings\t0\r\n",
      "0\tyou\t0\r\n",
      "0\tre\t0\r\n",
      "1\tre\t1\r\n",
      "1\tthank\t1\r\n",
      "1\trankings\t1\r\n",
      "1\tyou\t1\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"0001.1999-12-10.kaminski\t1\t re: rankings\t thank you.\"|Enron/mapper2.2.2.py|Enron/reducer2.2.2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted enronemail_1h.txt\n",
      "Deleted HW2.2.2/results\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob600369798504416074.jar tmpDir=null\n",
      "17/05/22 23:07:17 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/22 23:07:17 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/22 23:07:19 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/05/22 23:07:19 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/05/22 23:07:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1495464826300_0063\n",
      "17/05/22 23:07:20 INFO impl.YarnClientImpl: Submitted application application_1495464826300_0063\n",
      "17/05/22 23:07:21 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1495464826300_0063/\n",
      "17/05/22 23:07:21 INFO mapreduce.Job: Running job: job_1495464826300_0063\n",
      "17/05/22 23:07:30 INFO mapreduce.Job: Job job_1495464826300_0063 running in uber mode : false\n",
      "17/05/22 23:07:30 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/05/22 23:07:42 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/05/22 23:07:43 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/05/22 23:07:54 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/05/22 23:07:55 INFO mapreduce.Job: Job job_1495464826300_0063 completed successfully\n",
      "17/05/22 23:07:55 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=385020\n",
      "\t\tFILE: Number of bytes written=1122491\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217089\n",
      "\t\tHDFS: Number of bytes written=210\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=19054\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=9431\n",
      "\t\tTotal time spent by all map tasks (ms)=19054\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9431\n",
      "\t\tTotal vcore-seconds taken by all map tasks=19054\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=9431\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=19511296\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=9657344\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=33203\n",
      "\t\tMap output bytes=318608\n",
      "\t\tMap output materialized bytes=385026\n",
      "\t\tInput split bytes=222\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5409\n",
      "\t\tReduce shuffle bytes=385026\n",
      "\t\tReduce input records=33203\n",
      "\t\tReduce output records=22\n",
      "\t\tSpilled Records=66406\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=188\n",
      "\t\tCPU time spent (ms)=6250\n",
      "\t\tPhysical memory (bytes) snapshot=733745152\n",
      "\t\tVirtual memory (bytes) snapshot=4088238080\n",
      "\t\tTotal committed heap usage (bytes)=597688320\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=210\n",
      "17/05/22 23:07:55 INFO streaming.StreamJob: Output directory: HW2.2.2/results/\n"
     ]
    }
   ],
   "source": [
    "# START STUDENT CODE HW222HADOOP\n",
    "\n",
    "!hdfs dfs -rm enronemail_1h.txt\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/enronemail_1h.txt\n",
    "!hdfs dfs -rm -r HW2.2.2/results/\n",
    "\n",
    "##################### IMPORTANT ########################################################################\n",
    "# Make sure you have the correct paths to the jar file as wel as the input and output files!!\n",
    "# make sure to include the -files option. Do ***** NOT ****** put spaces between the file paths!\n",
    "########################################################################################################\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1\"  \\\n",
    "  -files Enron/mapper2.2.2.py,Enron/reducer2.2.2.py\\\n",
    "  -mapper mapper2.2.2.py \\\n",
    "  -reducer reducer2.2.2.py \\\n",
    "  -input enronemail_1h.txt \\\n",
    "  -output HW2.2.2/results/\\\n",
    "  -numReduceTasks 1\n",
    "\n",
    "       \n",
    "# END STUDENT CODE HW222HADOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat HW2.2.2/results/part-0000* > sorted_enron_counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tthe\t549\r\n",
      "0\tto\t398\r\n",
      "0\tect\t382\r\n",
      "0\tand\t278\r\n",
      "0\tof\t230\r\n",
      "0\thou\t206\r\n",
      "0\ta\t196\r\n",
      "0\tin\t182\r\n",
      "0\tfor\t170\r\n",
      "0\ton\t135\r\n",
      "0\twill\t132\r\n",
      "1\tthe\t698\r\n",
      "1\tto\t566\r\n",
      "1\tand\t392\r\n",
      "1\tyour\t357\r\n",
      "1\ta\t347\r\n",
      "1\tyou\t345\r\n",
      "1\tof\t336\r\n",
      "1\tin\t236\r\n",
      "1\tfor\t204\r\n",
      "1\tcom\t153\r\n",
      "1\tit\t152\r\n"
     ]
    }
   ],
   "source": [
    "# display top from each class in the sorted_enron_counts.txt file\n",
    "!cat sorted_enron_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2.3 (Optional)  \n",
    "Using Hadoop MapReduce and your wordcount job (from HW2.2.1) determine the top-10 occurring tokens (most frequent tokens) using multiple reducers. \n",
    "\n",
    "  To achieve a total sort one needs to write a custom mapper to to prepend a partition key to each record. The shuffle phase will need a custom partitioner based upon the prepended key, while the sort is based upon a composite key which is made up of the partition key and the word count (i.e., we will  do a secondary sort  on a composite key that is made up of the prepended key and the word count.  This all can be done with one map-reduce job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.3 Multinomial NAIVE BAYES with NO Smoothing using a single reducer\n",
    "    \n",
    "### Multinomial NAIVE BAYES model with NO Smoothing using a single reducer\n",
    "\n",
    "In this assignment you will produce a spam filter based upon a multinomial naive Bayes classifier. For a quick reference on the construction of the Multinomial NAIVE BAYES classifier that you will code, please consult the following:\n",
    "\n",
    "  * A nice textbook introduction to the different flavors of Naive Bayes is provide in [chapter 13](http://nlp.stanford.edu/IR-book/pdf/13bayes.pdf) of the IRBook.  Nice worked out examples are also provdied\n",
    "  * The \"Document Classification\" section of the  wikipedia page on [Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_classification)\n",
    "  * OR the original [paper](http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf) by the curators of the Enron email data.\n",
    "\n",
    "For the sake of this assignment we will focus on the basic construction  of the parallelized classifier, and not consider its validation or calibration, and so you will have the classifier operate on its own training data (unlike a \n",
    "field application where one would use non-overlapping subsets for training, validation and testing).\n",
    "NOTE: please use the subject field and the body field for all your Naive Bayes modeling. \n",
    "\n",
    "<span style=\"color:red;font-weight:bold\">For all tasks in this HW problem, please use one (1) reducer.</span>\n",
    "\n",
    "### HW2.3.1 Learn a Multinomial Naive Bayes model on a small  dataset (Chinese dataset: 5 documents)\n",
    "Using Hadoop streaming MapReduce, write  a mapper/reducer job(s) that will  learn a Naive Bayes classifier Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Note: for multinomial Naive Bayes, the class conditional probability for a word such as \"assistance\" given the class is SPAM, Pr(X=assistance|Y=SPAM), is calculated as follows:\n",
    "\n",
    "   $$ \\frac{the \\ number \\ of \\ times \\ assistance \\ occurs \\ in \\ SPAM \\ labeled \\ documents}{the \\ number \\ of \\ words \\ in \\ documents \\ labeled \\ SPAM }$$\n",
    "\n",
    "   E.g.,  consider that assistance occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeled as SPAM (when concatenated) is 1,000. Then $Pr(X=assistance|Y=SPAM) = 5/1000$. Note this is a multinomial estimation of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW problem. Please represent you model as a record where the key is the first field (TAB separated), and the value, the remaining part, is composed of two values corresponding the class conditional counts or probabilities depending on what phase of learning we are in. A typical record whether in a file or in memory will have the following KEY-VALUE structure:\n",
    "   \n",
    "*  `Word\\tCount(of Word in documents corresponding to HAM)\\tCount(of Word in documents corresponding to SPAM)`\n",
    "* In memory this TSV-type data can be stored as a dictionary or defaultDict to record the learnt model or intermediate versions of the model\n",
    "\n",
    "Finally the learnt model should consist of three columnes (note that HAM is the 0 class and SPAM is the 1 class)\n",
    "\n",
    "* `word\\tFreq(Word in HAM),Freq(Word in SPAM),Pr(Word|HAM)\\tPr(Word|SPAM)`\n",
    "\n",
    "The frequency information is added for ease of understanding and debugging.\n",
    "\n",
    "Note we can also insert a special record for the class priors. For example, we can use the token ClassPrior as the key to the class priors in this dictionary representation of the learnt model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive bayes Classification\n",
    "### Sketch of  mathematics:\n",
    "\n",
    "In Naive Bayes, the probability of a document $Doc$ has classification $C$ is calculated as follows:\n",
    "\n",
    "$$ P\\big(C \\:\\:\\lvert \\bigwedge_{w_i \\in Doc} w_i\\big) \\approx \\frac{ P(C) \\prod_{w_i \\in Doc}P(w_i\\lvert C)}{P(\\wedge w_i)}   $$\n",
    "\n",
    "where the $\\approx$ is due to the naive approximation of joint probability in the LHS numerator.\n",
    "\n",
    "\n",
    "To avoid floating point errors in the prompt we can hence use the equivilent statement that:\n",
    "\n",
    "$$ log\\big(P\\big(C \\:\\:\\lvert \\bigwedge_{w_i \\in Doc} w_i\\big)log\\big) \\approx log(P(C)) + \\sum_{w_i \\in Doc}log\\big(P(w_i\\lvert C)\\big) - log(P(\\wedge w_i))   $$\n",
    "\n",
    "So for fixed document $log(P(\\wedge w_i))$ we can optimize the RHS, by taking the following:\n",
    "\n",
    "\n",
    "$$ \\hat{y} \\approx argmax_{C\\in\\{\\text{spam},\\text{ham}\\}}\\Big(log(P(C)) + \\sum_{w_i \\in Doc}log\\big(P(w_i\\lvert C)\\big)\\Big) $$\n",
    "\n",
    "and the bag of words assumption and without smoothing:\n",
    "\n",
    "$$P(w_i\\lvert C) \\approx \\frac{Count(w_i,C)}{\\sum_jCount(w_j,C)}$$\n",
    "\n",
    "where $Count(w_j,C)$ is the count of instances of term $w_j$ in the class $C$ (viz, spam or ham).\n",
    "\n",
    "Hence we need to generat the following calculations to build our NB classifier:\n",
    " 1. $\\forall w_j\\in Vocab, C \\in \\{\\text{spam},\\text{ham}\\})): Count(w_j,C)$\n",
    "   * We can then calculate the conditional probabilities: \n",
    "         * $P(w_i\\lvert C)$\n",
    "\n",
    " 1. For one of the values $C \\in \\{\\text{spam},\\text{ham}\\})): P(C) = Count(C)\\: / \\:|Emails|$ \n",
    "     * This only needs to be calculated a single time and then can be summed with the conditional log probabilities to get our $\\hat{y}$ equation arguments\n",
    "     \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the task into 2 map-reduce tasks\n",
    "- a modeling phase where the output is a MODEL file \n",
    "- A classification phase where we use the model\n",
    "\n",
    "*For phase 1:  Build a multionmial Naive Bayes Model*\n",
    "  * MAPPER_3a: \n",
    "       * As with the mapper in hw2.2, generates raw word tokens with an added field for the spam value associated with that instance.  \n",
    "  * REDUCER_3a: For each word $w_i$\n",
    "      * Agragates from mapper: $\\forall w_j\\in Vocab, C \\in \\{\\text{spam},\\text{ham}\\}: Count(w_j,C)$\n",
    "      * And calculates $P(w_i\\lvert C) \\approx \\frac{Count(w_i,C)}{\\sum_jCount(w_j,C)}$\n",
    "          \n",
    "\n",
    "*For phase 2: Classify each example using the learnt  multionmial Naive Bayes Model*\n",
    " * MAPPER_3b: for each $Doc$, calculates \n",
    "   * $\\forall w_j\\in Vocab: argmax_{C\\{spam,ham\\}}\\big( log(P(C)) + \\sum_{w_i \\in Doc}log\\big(P(w_i\\lvert C)\\big)$\n",
    " * REDUCER_3b: calculates \n",
    "      * $Err(Model, DF) = \\frac{count_{DF}(\\:\\hat{y} \\:\\neq \\:label\\:)}{|DF|}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested Record format that can be used of all Mappers and Reducers\n",
    "Here is the learnt binary multinomial Naive Bayes model for the Chinese Dateset which is described below. Note this is a small dataset with 4 records (i.e., four documnents). Each record in the model file has the following format:   \n",
    "`Word\\t freq(word in class0) ,freq(word in class1),  (Pr(Word|Class =0), Pr(Word|Class =1)) `\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `NaiveBayes': File exists\r\n"
     ]
    }
   ],
   "source": [
    "%mkdir NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/model1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/model1.txt\n",
    "Beijing\t0.0,1.0,0.111111111111,0.142857142857\n",
    "Chinese\t1.0,5.0,0.222222222222,0.428571428571\n",
    "Tokyo\t1.0,0.0,0.222222222222,0.0714285714286\n",
    "Shanghai\t0.0,1.0,0.111111111111,0.142857142857\n",
    "ClassPriors\t1.0,3.0,0.25,0.75\n",
    "Japan\t1.0,0.0,0.222222222222,0.0714285714286\n",
    "Macao\t0.0,1.0,0.111111111111,0.142857142857"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model file and play with it before writing the classifier job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beijing --> 0.0,1.0,0.111111111111,0.142857142857\n",
      "Chinese --> 1.0,5.0,0.222222222222,0.428571428571\n",
      "Tokyo --> 1.0,0.0,0.222222222222,0.0714285714286\n",
      "Shanghai --> 0.0,1.0,0.111111111111,0.142857142857\n",
      "ClassPriors --> 1.0,3.0,0.25,0.75\n",
      "Japan --> 1.0,0.0,0.222222222222,0.0714285714286\n",
      "Macao --> 0.0,1.0,0.111111111111,0.142857142857\n"
     ]
    }
   ],
   "source": [
    "# load model file as strings (it is not tokenized; that will come next)\n",
    "modelStats = {}\n",
    "recordStrs = [s.split('\\n')[0].split('\\t') for s in open(\"NaiveBayes/model1.txt\").readlines()]\n",
    "for word, statsStr in recordStrs:\n",
    "    print word, \"-->\", statsStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Beijing': ['0.0', '1.0', '0.111111111111', '0.142857142857'],\n",
       " 'Chinese': ['1.0', '5.0', '0.222222222222', '0.428571428571'],\n",
       " 'ClassPriors': ['1.0', '3.0', '0.25', '0.75'],\n",
       " 'Japan': ['1.0', '0.0', '0.222222222222', '0.0714285714286'],\n",
       " 'Macao': ['0.0', '1.0', '0.111111111111', '0.142857142857'],\n",
       " 'Shanghai': ['0.0', '1.0', '0.111111111111', '0.142857142857'],\n",
       " 'Tokyo': ['1.0', '0.0', '0.222222222222', '0.0714285714286']}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model file\n",
    "# notice the string quotes around the frequncy and probabilits\n",
    "# we need to fix that next \n",
    "modelStats = {}\n",
    "recordStrs = [s.split('\\n')[0].split('\\t') for s in open(\"NaiveBayes/model1.txt\").readlines()]\n",
    "for word, statsStr in recordStrs:\n",
    "    modelStats[word] =  statsStr.split(\",\")\n",
    "modelStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Beijing': [0.0, 1.0, 0.111111111111, 0.142857142857],\n",
       " 'Chinese': [1.0, 5.0, 0.222222222222, 0.428571428571],\n",
       " 'ClassPriors': [1.0, 3.0, 0.25, 0.75],\n",
       " 'Japan': [1.0, 0.0, 0.222222222222, 0.0714285714286],\n",
       " 'Macao': [0.0, 1.0, 0.111111111111, 0.142857142857],\n",
       " 'Shanghai': [0.0, 1.0, 0.111111111111, 0.142857142857],\n",
       " 'Tokyo': [1.0, 0.0, 0.222222222222, 0.0714285714286]}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model file\n",
    "# convert strings to floats using Pythons map function\n",
    "# the map iterates over each element in the list apply the float() function\n",
    "# which converts a string to float\n",
    "# RESULT: modelStats now contains our multinomial Naive Bayes model\n",
    "modelStats = {}\n",
    "recordStrs = [s.split('\\n')[0].split('\\t') for s in open(\"NaiveBayes/model1.txt\").readlines()]\n",
    "for word, statsStr in recordStrs:\n",
    "    modelStats[word] = map(float, statsStr.split(\",\"))\n",
    "modelStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr( Chinese | Class) [1.0, 5.0, 0.222222222222, 0.428571428571]\n",
      "Pr( Chinese | Class) [1.0, 5.0, 0.222222222222, 0.428571428571]\n",
      "Pr( Chinese | Class) [1.0, 5.0, 0.222222222222, 0.428571428571]\n",
      "Pr( Tokyo | Class) [1.0, 0.0, 0.222222222222, 0.0714285714286]\n",
      "Pr( Japan | Class) [1.0, 0.0, 0.222222222222, 0.0714285714286]\n"
     ]
    }
   ],
   "source": [
    "#classify a sample document using the model\n",
    "# Use the following record to test\n",
    "#   D5\t0\tChinese Chinese\tChinese Tokyo Japan\n",
    "# FIRST : print Pr(Word|Class) in an unextracted form\n",
    "line = \"D5\t0\tChinese Chinese\tChinese Tokyo Japan\"\n",
    "docID, docClass,text = line.split(\"\\t\",2)   \n",
    "words = text.split()\n",
    "#print docID,docClass, words\n",
    "for word in words:\n",
    "    print \"Pr(\",word, \"| Class)\", modelStats[word]  #Pr(Class=0| Doc)  all stats\n",
    "\n",
    "# STILL under constuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prClass0=0.250, prClass1=0.750\n",
      "Pr(Chinese|class = 0) = 0.222\n",
      "Pr(Chinese|class = 1) = 0.429\n",
      "Pr(Chinese|class = 0) = 0.222\n",
      "Pr(Chinese|class = 1) = 0.429\n",
      "Pr(Chinese|class = 0) = 0.222\n",
      "Pr(Chinese|class = 1) = 0.429\n",
      "Pr(Tokyo|class = 0) = 0.222\n",
      "Pr(Tokyo|class = 1) = 0.071\n",
      "Pr(Japan|class = 0) = 0.222\n",
      "Pr(Japan|class = 1) = 0.071\n",
      "Pr(Class=0| Doc=D5) is 0.00014\n",
      "Pr(Class=1| Doc=D5) is 0.00030\n"
     ]
    }
   ],
   "source": [
    "# classify a sample document using the model\n",
    "# Use the following record to test\n",
    "# D5\t0\tChinese Chinese\tChinese Tokyo Japan\n",
    "# --------------------------------------------------------------------\n",
    "# Posterior Probabilities Pr(Class=0| Doc) and Pr(Class=1| Doc) \n",
    "# Naive Bayes inference Pr(Class=0| Doc)  ~ Pr(Class=0) * Pr(Class=0| word1) * Pr(Class=0| word2)...... \n",
    "\n",
    "line = \"D5\t0\tChinese Chinese\tChinese Tokyo Japan\"\n",
    "docID, docClass,text = line.split(\"\\t\",2)   \n",
    "words = text.split()\n",
    "#Class priors (Pr(Class =0) and Pr(Class =1))\n",
    "c0, c1, prClass0, prClass1 = map(float, modelStats[\"ClassPriors\"])\n",
    "print \"prClass0=%04.3f, prClass1=%04.3f\" % (prClass0, prClass1)\n",
    "# Posterior Probabilities Pr(Class=0| Doc) and Pr(Class=1| Doc) \n",
    "# Naive Bayes inference Pr(Class=0| Doc)  ~ Pr(Class=0) * Pr(Class=0| word1) * Pr(Class=0| word2)...... \n",
    "PrClass0GivenDoc = prClass0  \n",
    "PrClass1GivenDoc = prClass1\n",
    "for word in words:\n",
    "    #print word, modelStats[word]  #Pr(word|class = 0) all stats\n",
    "    print 'Pr(%s|class = 0) = %04.3f' %(word, modelStats[word][2])  #Pr(word|class = 0)\n",
    "    print 'Pr(%s|class = 1) = %04.3f' %(word, modelStats[word][3])  #Pr(word|class = 1)\n",
    "    PrClass0GivenDoc *= modelStats[word][2]\n",
    "    PrClass1GivenDoc *= modelStats[word][3]\n",
    " \n",
    "print \"Pr(Class=0| Doc=D5) is %6.5f\" % (PrClass0GivenDoc)\n",
    "print \"Pr(Class=1| Doc=D5) is %6.5f\" % (PrClass1GivenDoc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use logs to avoid precision problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prClass0=0.250, prClass1=0.750\n",
      "Pr(Chinese|class = 0) = 0.222\n",
      "Pr(Chinese|class = 1) = 0.429\n",
      "Pr(Chinese|class = 0) = 0.222\n",
      "Pr(Chinese|class = 1) = 0.429\n",
      "Pr(Chinese|class = 0) = 0.222\n",
      "Pr(Chinese|class = 1) = 0.429\n",
      "Pr(Tokyo|class = 0) = 0.222\n",
      "Pr(Tokyo|class = 1) = 0.071\n",
      "Pr(Japan|class = 0) = 0.222\n",
      "Pr(Japan|class = 1) = 0.071\n",
      "Pr(Class=0| Doc=D5) = 0.00014, log(Pr(Class=0| Doc=D5)) = -8.906681\n",
      "Pr(Class=1| Doc=D5) = 0.00030, log(Pr(Class=1| Doc=D5)) = -8.906681\n"
     ]
    }
   ],
   "source": [
    "# classify a sample document using the model\n",
    "# Use the following record to test\n",
    "# D5\t0\tChinese Chinese\tChinese Tokyo Japan\n",
    "# --------------------------------------------------------------------\n",
    "# Posterior Probabilities Pr(Class=0| Doc) and Pr(Class=1| Doc) \n",
    "# Naive Bayes inference Pr(Class=0| Doc)  ~ Pr(Class=0) * Pr(Class=0| word1) * Pr(Class=0| word2)...... \n",
    "\n",
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "line = \"D5\t0\tChinese Chinese\tChinese Tokyo Japan\"\n",
    "docID, docClass,text = line.split(\"\\t\",2)   \n",
    "words = text.split()\n",
    "#Class priors (Pr(Class =0) and Pr(Class =1))\n",
    "c0, c1, prClass0, prClass1 = map(float, modelStats[\"ClassPriors\"])\n",
    "print \"prClass0=%04.3f, prClass1=%04.3f\" % (prClass0, prClass1)\n",
    "# Posterior Probabilities Pr(Class=0| Doc) and Pr(Class=1| Doc) \n",
    "# Naive Bayes inference Pr(Class=0| Doc)  ~ log(Pr(Class=0)) + log(Pr(Class=0| word1)) + log(Pr(Class=0| word2))...... \n",
    "PrClass0GivenDoc = log(prClass0)  \n",
    "PrClass1GivenDoc = log(prClass1)\n",
    "for word in words:\n",
    "    #print word, modelStats[word]  #Pr(word|class = 0) all stats\n",
    "    print 'Pr(%s|class = 0) = %04.3f' %(word, modelStats[word][2])  #Pr(word|class = 0)\n",
    "    print 'Pr(%s|class = 1) = %04.3f' %(word, modelStats[word][3])  #Pr(word|class = 1)\n",
    "    PrClass0GivenDoc += log(modelStats[word][2])\n",
    "    PrClass1GivenDoc += log(modelStats[word][3])\n",
    " \n",
    "print \"Pr(Class=0| Doc=D5) = %6.5f, log(Pr(Class=0| Doc=D5)) = %f\" % (exp(PrClass0GivenDoc), PrClass0GivenDoc)\n",
    "print \"Pr(Class=1| Doc=D5) = %6.5f, log(Pr(Class=1| Doc=D5)) = %f\" % (exp(PrClass1GivenDoc), PrClass0GivenDoc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Naive Bayes Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `NaiveBayes': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/NaiveBayesModel.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/NaiveBayesModel.py\n",
    "#!/usr/bin/env python\n",
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "class NaiveBayesModel(object):\n",
    "\n",
    "    def __init__(self, modelFile):\n",
    "        self.model = {}\n",
    "        recordStrs = [s.split('\\n')[0].split('\\t') for s in open(modelFile).readlines()]\n",
    "        for word, statsStr in recordStrs:\n",
    "            self.model[word] = map(float, statsStr.split(\",\"))\n",
    "        #Class priors: counts and probs (Pr(Class =0) and Pr(Class =1))\n",
    "        self.c0, self.c1, self.prClass0, self.prClass1 = map(float, self.model[\"ClassPriors\"])\n",
    "\n",
    "        \n",
    "\n",
    "    def classify(self, doc):\n",
    "        # Posterior Probabilities Pr(Class=0| Doc) and Pr(Class=1| Doc) \n",
    "        # Naive Bayes inference Pr(Class=0| Doc)  ~ Pr(Class=0) * Pr(Class=0| word1) * Pr(Class=0| word2)...... \n",
    "        PrClass0GivenDoc = self.prClass0  \n",
    "        PrClass1GivenDoc = self.prClass1\n",
    "        for word in doc:\n",
    "            PrClass0GivenDoc *= self.model[word][2]\n",
    "            PrClass1GivenDoc *= self.model[word][3]\n",
    "        return([PrClass0GivenDoc, PrClass1GivenDoc])\n",
    " \n",
    "    # the natural log based version of this \n",
    "    # helps avoid underflow issues\n",
    "    def classifyInLogs(self, doc):       \n",
    "        # Posterior Probabilities Pr(Class=0| Doc) and Pr(Class=1| Doc) \n",
    "        # Naive Bayes inference Pr(Class=0| Doc)  ~ Pr(Class=0) * Pr(Class=0| word1) * Pr(Class=0| word2)...... \n",
    "        PrClass0GivenDoc = log(self.prClass0)  \n",
    "        PrClass1GivenDoc = log(self.prClass1)\n",
    "        for word in doc:  #NOTE: Improvement: on loading one should convert probs to log probs!\n",
    "            c0 = self.model[word][2]\n",
    "            c1 = self.model[word][3]\n",
    "            if c0 != 0:\n",
    "                PrClass0GivenDoc += log(c0)\n",
    "            else:\n",
    "                PrClass0GivenDoc = float(\"-inf\")\n",
    "            if c1 != 0:\n",
    "                PrClass1GivenDoc += log(c1)\n",
    "            else:\n",
    "                PrClass1GivenDoc = float(\"-inf\")\n",
    "                \n",
    "        return([PrClass0GivenDoc, PrClass1GivenDoc])\n",
    "\n",
    "        \n",
    "    def printModel(self):\n",
    "        print \"NaiveBayes Model starts here\\n----------------\"\n",
    "        print \"PRIORS: prClass0=%04.3f, prClass1=%04.3f\" % (self.prClass0, self.prClass1)\n",
    "        for word, stats in self.model.items():\n",
    "            print \"Pr(\",word, \"| Class)\", stats  #Pr(Class=0| Doc)  all stats\n",
    "        print \"NaiveBayes Model ENDS here\\n----------------\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run NaiveBayes/NaiveBayesModel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Driver for Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes Model starts here\n",
      "----------------\n",
      "PRIORS: prClass0=0.250, prClass1=0.750\n",
      "Pr( Beijing | Class) [0.0, 1.0, 0.111111111111, 0.142857142857]\n",
      "Pr( Chinese | Class) [1.0, 5.0, 0.222222222222, 0.428571428571]\n",
      "Pr( Tokyo | Class) [1.0, 0.0, 0.222222222222, 0.0714285714286]\n",
      "Pr( Shanghai | Class) [0.0, 1.0, 0.111111111111, 0.142857142857]\n",
      "Pr( ClassPriors | Class) [1.0, 3.0, 0.25, 0.75]\n",
      "Pr( Japan | Class) [1.0, 0.0, 0.222222222222, 0.0714285714286]\n",
      "Pr( Macao | Class) [0.0, 1.0, 0.111111111111, 0.142857142857]\n",
      "NaiveBayes Model ENDS here\n",
      "----------------\n",
      "Pr(Class=0| Doc=D5) is 0.00014\n",
      "Pr(Class=1| Doc=D5) is 0.00030\n",
      "Pr(Class=0| Doc=D5) = 0.00014, log(Pr(Class=0| Doc=D5)) = -8.906681\n",
      "Pr(Class=1| Doc=D5) = 0.00030, log(Pr(Class=1| Doc=D5)) = -8.107690\n"
     ]
    }
   ],
   "source": [
    "#classify a sample document using the model\n",
    "# Use the following record to test\n",
    "#   D5\t0\tChinese Chinese\tChinese Tokyo Japan\n",
    "# --------------------------------------------------------------------\n",
    "# Posterior Probabilities Pr(Class=0| Doc) and Pr(Class=1| Doc) \n",
    "# Naive Bayes inference Pr(Class=0| Doc)  ~ Pr(Class=0) * Pr(Class=0| word1) * Pr(Class=0| word2)...... \n",
    "\n",
    "NBModel = NaiveBayesModel(\"NaiveBayes/model1.txt\")     \n",
    "NBModel.printModel()  \n",
    "line = \"D5\t0\tChinese Chinese\tChinese Tokyo Japan\"\n",
    "docID, docClass,text = line.split(\"\\t\",2)   \n",
    "words = text.split()\n",
    "PrClass0GivenDoc, PrClass1GivenDoc = NBModel.classify(words)\n",
    "\n",
    "print \"Pr(Class=0| Doc=%s) is %6.5f\" % (docID, PrClass0GivenDoc)\n",
    "print \"Pr(Class=1| Doc=%s) is %6.5f\" % (docID, PrClass1GivenDoc)\n",
    "\n",
    "PrClass0GivenDoc, PrClass1GivenDoc = NBModel.classifyInLogs(words)\n",
    "\n",
    "print \"Pr(Class=0| Doc=D5) = %6.5f, log(Pr(Class=0| Doc=D5)) = %f\" % (exp(PrClass0GivenDoc), PrClass0GivenDoc)\n",
    "print \"Pr(Class=1| Doc=D5) = %6.5f, log(Pr(Class=1| Doc=D5)) = %f\" % (exp(PrClass1GivenDoc), PrClass1GivenDoc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/mapper_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/mapper_model.py\n",
    "#!/usr/bin/env python\n",
    "import sys, re, string\n",
    "\n",
    "# Init mapper phase \n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# inner loop mapper phase: process each record\n",
    "# input comes from STDIN (standard input)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    # use subject and body \n",
    "    \n",
    "    parts = line.split(\"\\t\")\n",
    "    docID, docClass, title = parts[0:3]\n",
    "    if len(parts) == 4:\n",
    "        body = parts[3]\n",
    "    else:\n",
    "        body = \"\"\n",
    "    \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    emailStr = regex.sub(' ', title.lower() + \" \" +body.lower()) #replace each punctuation with a space\n",
    "    emailStr = re.sub( '\\s+', ' ', emailStr )            # replace multiple spaces with a space\n",
    "    # split the line into words\n",
    "    words = emailStr.split()\n",
    "    \n",
    "# START STUDENT CODE HW231MAPPER_MODEL\n",
    "\n",
    "# define regex for punctuation removal\n",
    "\n",
    "# increase counters\n",
    "# write the results to STDOUT (standard output);\n",
    "# what we output here will be the input for the\n",
    "# Reduce step, i.e. the input for reducer.py\n",
    "#\n",
    "# tab-delimited; the trivial word count is 1\n",
    "    \n",
    "    for w in words: \n",
    "        print \"%s\\t%d\\t%d\\t%s\"%(w,1,int(docClass),docID)# w, \"\\\n",
    "                \n",
    "# END STUDENT CODE HW231MAPPER_MODEL            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/reducer_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/reducer_model.py\n",
    "#!/usr/bin/env python\n",
    "from operator import itemgetter\n",
    "import sys, operator\n",
    "import numpy as np\n",
    "from  collections import OrderedDict \n",
    "\n",
    "# START STUDENT CODE HW231REDUCER_MODEL\n",
    "\n",
    "current_word = None\n",
    "smooth_factor = 0 # no smoothing\n",
    "current_count = [smooth_factor, smooth_factor]\n",
    "msgIDs = {}\n",
    "word = None\n",
    "wordcount = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "# remove leading and trailing whitespace\n",
    "\n",
    "# parse the input we got from mapper.py\n",
    "\n",
    "# convert count and spam flag (currently a string) to int\n",
    "\n",
    "\n",
    "# handle msgID - store all IDs as we don't have too much\n",
    "# not the best way to get prior, a two-level MapReduce jobs (ID - word) would be optimal\n",
    "    \n",
    "# calculate NB parameters, and write the dictionary to a file for the classification job\n",
    "# prior probabilities\n",
    "\n",
    "# conditional probability\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count, isSpam, msgID = line.split('\\t', 3)\n",
    "   \n",
    "   \n",
    "    # convert count and spam flag (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "        isSpam = int(isSpam)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    \n",
    "    # handle msgID - store all IDs as we don't have too much\n",
    "    # not the best way to get prior, a two-level MapReduce jobs (ID - word) would be optimal\n",
    "    if msgID not in msgIDs:\n",
    "        msgIDs[msgID] = isSpam\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:        \n",
    "        current_count[isSpam] += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # count finish for one word, save it\n",
    "            wordcount[current_word] = current_count\n",
    "        # initialize new count for new word\n",
    "        current_count = [smooth_factor, smooth_factor]\n",
    "        current_count[isSpam] = count                \n",
    "        current_word = word\n",
    "\n",
    "# do not forget to save the last word count if needed!\n",
    "if current_word == word:    \n",
    "    wordcount[current_word] = current_count\n",
    "    \n",
    "# calculate NB parameters, and write the dictionary to a file for the classification job\n",
    "# prior probabilities\n",
    "n_msg = len(msgIDs)\n",
    "n_spam = sum(msgIDs.values())\n",
    "n_ham = n_msg - n_spam\n",
    "print '%s\\t%s,%s,%s,%s' %('ClassPriors',n_ham,n_spam ,1.0*n_ham/n_msg, 1.0*n_spam/n_msg)\n",
    "\n",
    "# conditional probability\n",
    "n_total = np.sum(wordcount.values(), 0)\n",
    "\n",
    "\n",
    "wordcount =OrderedDict(sorted(wordcount.items(), key=lambda t: t[0]))\n",
    "for (key,value) in zip(wordcount.keys(), wordcount.values()/(1.0*n_total)):\n",
    "    print '%s\\t%s,%s,%s,%s' %(key,wordcount[key][0] ,wordcount[key][1],value[0], value[1])\n",
    "    \n",
    "# END STUDENT CODE HW231REDUCER_MODEL    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x NaiveBayes/mapper_model.py\n",
    "!chmod a+x NaiveBayes/reducer_model.py \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/mapper_classify.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/mapper_classify.py\n",
    "#!/usr/bin/env python\n",
    "from NaiveBayesModel import NaiveBayesModel\n",
    "import sys, re, string, subprocess\n",
    "import sys, operator, math \n",
    "import numpy as np\n",
    "from math import log\n",
    "from math import exp\n",
    "# Init mapper phase \n",
    "\n",
    "# read the MODEL into memory\n",
    "# The model file resides the local disk (make sure to ship it home from HDFS).\n",
    "# In the Hadoop command linke be sure to add the follow the -files commmand line option\n",
    "NBModel = NaiveBayesModel(\"NaiveBayes/NaiveBayes.txt\") \n",
    "#NBModel.printModel()\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# inner loop mapper phase: process each record\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    parts = line.split(\"\\t\")\n",
    "    docID, docClass, title = parts[0:3]\n",
    "    if len(parts) == 4:\n",
    "        body = parts[3]\n",
    "    else:\n",
    "        body = \"\"\n",
    "    # use subject and body \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    emailStr = regex.sub(' ', title.lower() + \" \" +body.lower()) #replace each punctuation with a space\n",
    "    emailStr = re.sub( '\\s+', ' ', emailStr )            # replace multiple spaces with a space\n",
    "    # split the line into words\n",
    "    words = emailStr.split()\n",
    "\n",
    "# START STUDENT CODE HW231MAPPER_CLASSIFY\n",
    "      \n",
    "\n",
    "\n",
    "    #PrClass0GivenDoc, PrClass1GivenDoc = NBModel.classify(words)\n",
    "\n",
    "    #print \"Pr(Class=0| Doc=%s) is %6.5f\" % (docID, PrClass0GivenDoc)\n",
    "    #print \"Pr(Class=1| Doc=%s) is %6.5f\" % (docID, PrClass1GivenDoc)\n",
    "\n",
    "    PrClass0GivenDoc, PrClass1GivenDoc = NBModel.classify(words)\n",
    "    if PrClass0GivenDoc > PrClass1GivenDoc:\n",
    "        print \"%s\\t%s\\t%s\"%(docID,0,docClass)\n",
    "    else:\n",
    "        print \"%s\\t%s\\t%s\"%(docID,1,docClass)\n",
    "    #print \"Pr(Class=0| Doc=D5) = %6.5f, log(Pr(Class=0| Doc=D5)) = %f\" % (exp(PrClass0GivenDoc), PrClass0GivenDoc)\n",
    "    #print \"Pr(Class=1| Doc=D5) = %6.5f, log(Pr(Class=1| Doc=D5)) = %f\" % (exp(PrClass1GivenDoc), PrClass1GivenDoc) \n",
    "    \n",
    "  \n",
    "\n",
    "# END STUDENT CODE HW231MAPPER_CLASSIFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/reducer_classify.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/reducer_classify.py\n",
    "#!/usr/bin/env python\n",
    "from operator import itemgetter\n",
    "import sys, operator, math\n",
    "import numpy as np\n",
    "\n",
    "numberOfRecords = 0\n",
    "NumberOfMisclassifications=0\n",
    "classificationAccurary = 0\n",
    "\n",
    "# START STUDENT CODE HW231REDUCER_CLASSIFY\n",
    "\n",
    "# input comes from STDIN\n",
    "print \"%s\\t%s\\t%s\"%(\"PREDICTION\",\"LABEL\",\"DOCID\")\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    #line = line.strip()\n",
    "    # split the line into words\n",
    "    parts = line.split(\"\\t\")\n",
    "    docID, preds, label = parts[0:3]\n",
    "    print \"%s\\t%s\\t%s\"%( preds, label.rstrip(),docID.rstrip())\n",
    "    if int(preds) != int(label):\n",
    "        NumberOfMisclassifications +=1\n",
    "    numberOfRecords+=1\n",
    "\n",
    "classificationAccurary = float(float(numberOfRecords-NumberOfMisclassifications) / float(numberOfRecords)) * float(100)\n",
    "print 'Multinomial Naive Bayes Classifier Results are Total Records: %d, Misclassifications: %d, Accuracy :%2.5f' % (numberOfRecords, NumberOfMisclassifications, classificationAccurary)\n",
    "# END STUDENT CODE HW231REDUCER_CLASSIFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x NaiveBayes/mapper_classify.py\n",
    "!chmod a+x NaiveBayes/reducer_classify.py\n",
    "!chmod a+x NaiveBayes/NaiveBayesModel.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Map Reduce Job to learn a multinomical Naive Model from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted chineseExample_train.txt\n",
      "Deleted HW231HADOOP_CHINESE_UNIT_TEST/model\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob3141502217011817157.jar tmpDir=null\n",
      "17/05/22 23:08:23 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/22 23:08:24 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/22 23:08:26 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/05/22 23:08:26 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/05/22 23:08:26 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1495464826300_0064\n",
      "17/05/22 23:08:27 INFO impl.YarnClientImpl: Submitted application application_1495464826300_0064\n",
      "17/05/22 23:08:27 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1495464826300_0064/\n",
      "17/05/22 23:08:27 INFO mapreduce.Job: Running job: job_1495464826300_0064\n",
      "17/05/22 23:08:43 INFO mapreduce.Job: Job job_1495464826300_0064 running in uber mode : false\n",
      "17/05/22 23:08:43 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/05/22 23:08:53 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/05/22 23:09:02 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/05/22 23:09:02 INFO mapreduce.Job: Job job_1495464826300_0064 completed successfully\n",
      "17/05/22 23:09:02 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=188\n",
      "\t\tFILE: Number of bytes written=353388\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=391\n",
      "\t\tHDFS: Number of bytes written=182\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=15327\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6017\n",
      "\t\tTotal time spent by all map tasks (ms)=15327\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6017\n",
      "\t\tTotal vcore-seconds taken by all map tasks=15327\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=6017\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=15694848\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6161408\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=11\n",
      "\t\tMap output bytes=160\n",
      "\t\tMap output materialized bytes=194\n",
      "\t\tInput split bytes=236\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce shuffle bytes=194\n",
      "\t\tReduce input records=11\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=22\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=145\n",
      "\t\tCPU time spent (ms)=2720\n",
      "\t\tPhysical memory (bytes) snapshot=715456512\n",
      "\t\tVirtual memory (bytes) snapshot=4113739776\n",
      "\t\tTotal committed heap usage (bytes)=600309760\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=155\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=182\n",
      "17/05/22 23:09:02 INFO streaming.StreamJob: Output directory: HW231HADOOP_CHINESE_UNIT_TEST/model/\n"
     ]
    }
   ],
   "source": [
    "# START STUDENT CODE HW231HADOOP_MODEL\n",
    "# STEP 1: make input directory on HDFS\n",
    "#!hdfs dfs -mkdir -p /user/Xxxxxxx\n",
    "\n",
    "# STEP2:  upload data to HDFS\n",
    "!hdfs dfs -rm chineseExample_train.txt\n",
    "!rm NaiveBayes/chineseExample_train.txt\n",
    "!head -n 4 NaiveBayes/chineseExample.txt  > NaiveBayes/chineseExample_train.txt\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/chineseExample_train.txt\n",
    "!hdfs dfs -rm -r HW231HADOOP_CHINESE_UNIT_TEST/model/\n",
    "\n",
    "##################### IMPORTANT ########################################################################\n",
    "# Make sure you have the correct paths to the jar file as wel as the input and output files!!\n",
    "# make sure to include the -files option. Do ***** NOT ****** put spaces between the file paths!\n",
    "########################################################################################################\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1\"  \\\n",
    "  -files NaiveBayes/mapper_model.py,NaiveBayes/reducer_model.py \\\n",
    "  -mapper mapper_model.py\\\n",
    "  -reducer reducer_model.py \\\n",
    "  -input chineseExample_train.txt\\\n",
    "  -output HW231HADOOP_CHINESE_UNIT_TEST/model/\\\n",
    "  -numReduceTasks 1\\\n",
    "  -cmdenv PATH=/opt/anaconda/bin:$PATH\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    " # END STUDENT CODE HW231HADOOP_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 root supergroup          0 2017-05-22 23:09 HW231HADOOP_CHINESE_UNIT_TEST/model/_SUCCESS\r\n",
      "-rw-r--r--   1 root supergroup        182 2017-05-22 23:08 HW231HADOOP_CHINESE_UNIT_TEST/model/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "#put results file into local directories\n",
    "\n",
    "!hdfs dfs -ls HW231HADOOP_CHINESE_UNIT_TEST/model/\n",
    "!rm NaiveBayes/NaiveBayes.txt\n",
    "!hdfs dfs -get HW231HADOOP_CHINESE_UNIT_TEST/model/part-00000 NaiveBayes/NaiveBayes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Map Reduce Job to classify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted chineseExample_test.txt\n",
      "Deleted HW231HADOOP_CHINESE_UNIT_TEST/classifications\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob885585165650283461.jar tmpDir=null\n",
      "17/05/22 23:09:29 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/22 23:09:30 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/22 23:09:32 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/05/22 23:09:32 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/05/22 23:09:33 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1495464826300_0065\n",
      "17/05/22 23:09:33 INFO impl.YarnClientImpl: Submitted application application_1495464826300_0065\n",
      "17/05/22 23:09:33 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1495464826300_0065/\n",
      "17/05/22 23:09:33 INFO mapreduce.Job: Running job: job_1495464826300_0065\n",
      "17/05/22 23:09:44 INFO mapreduce.Job: Job job_1495464826300_0065 running in uber mode : false\n",
      "17/05/22 23:09:44 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/05/22 23:09:54 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/05/22 23:10:01 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/05/22 23:10:02 INFO mapreduce.Job: Job job_1495464826300_0065 completed successfully\n",
      "17/05/22 23:10:02 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=15\n",
      "\t\tFILE: Number of bytes written=351959\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=294\n",
      "\t\tHDFS: Number of bytes written=91\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=15251\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5739\n",
      "\t\tTotal time spent by all map tasks (ms)=15251\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5739\n",
      "\t\tTotal vcore-seconds taken by all map tasks=15251\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5739\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=15617024\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5876736\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=1\n",
      "\t\tMap output bytes=7\n",
      "\t\tMap output materialized bytes=21\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=21\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=2\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=150\n",
      "\t\tCPU time spent (ms)=3380\n",
      "\t\tPhysical memory (bytes) snapshot=755490816\n",
      "\t\tVirtual memory (bytes) snapshot=4093165568\n",
      "\t\tTotal committed heap usage (bytes)=641728512\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=60\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=91\n",
      "17/05/22 23:10:02 INFO streaming.StreamJob: Output directory: HW231HADOOP_CHINESE_UNIT_TEST/classifications/\n"
     ]
    }
   ],
   "source": [
    "# START STUDENT CODE HW231HADOOP_CLASSIFY\n",
    "\n",
    "!hdfs dfs -rm chineseExample_test.txt    \n",
    "!rm NaiveBayes/chineseExample_test.txt\n",
    "!tail -n 1 NaiveBayes/chineseExample.txt  > NaiveBayes/chineseExample_test.txt\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/chineseExample_test.txt\n",
    "!hdfs dfs -rm -r HW231HADOOP_CHINESE_UNIT_TEST/classifications/\n",
    "\n",
    "##################### IMPORTANT ########################################################################\n",
    "# Make sure you have the correct paths to the jar file as wel as the input and output files!!\n",
    "# make sure to include the -files option. Do ***** NOT ****** put spaces between the file paths!\n",
    "########################################################################################################\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1\"  \\\n",
    "  -files NaiveBayes\\\n",
    "  -mapper NaiveBayes/mapper_classify.py\\\n",
    "  -reducer NaiveBayes/reducer_classify.py  \\\n",
    "  -input chineseExample_test.txt \\\n",
    "  -output HW231HADOOP_CHINESE_UNIT_TEST/classifications/\\\n",
    "  -numReduceTasks 1\\\n",
    "  -cmdenv PATH=/opt/anaconda/bin:$PATH\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# END STUDENT CODE HW231HADOOP_CLASSIFY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the  accuracy measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION\tLABEL\tDOCID\r\n",
      "0\t0\tD5\r\n",
      "Multinomial Naive Bayes Classifier Results are1,0,100.00000\t\r\n"
     ]
    }
   ],
   "source": [
    "# START STUDENT CODE HW231HADOOP_CLASSIFY_RESULTS\n",
    "# (you may or may not need to add anything else in this block depending on your implementation)\n",
    "\n",
    "!hdfs dfs -cat HW231HADOOP_CHINESE_UNIT_TEST/classifications/part-00000\n",
    "\n",
    "# END STUDENT CODE HW231HADOOP_CLASSIFY_RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a systems test to regression test your map reduce job\n",
    "Write a systems test to test your learning algorithm implementation using the following \"Chinese\" dataset.\n",
    "Please reserve document D5 as an independent test document (i.e., dont use it for training. Just use it for testing) Use the Chinese dataset to  unit test your Mapper, reducer and final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinese dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/chineseExample.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/chineseExample.txt\n",
    "D1\t1\tChinese Beijing\tChinese\n",
    "D2\t1\tChinese Chinese\tShanghai\n",
    "D3\t1\tChinese\tMacao\n",
    "D4\t0\tTokyo Japan\tChinese\n",
    "D5\t0\tChinese Chinese\tChinese Tokyo Japan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm NaiveBayes/NaiveBayes.txt\n",
    "!head -n 4 NaiveBayes/chineseExample.txt  |NaiveBayes/mapper_model.py| sort -k1,1 |NaiveBayes/reducer_model.py>NaiveBayes/NaiveBayes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D5\t0\t0\r\n"
     ]
    }
   ],
   "source": [
    "!echo 'D5\t0\tChinese Chinese\tChinese Tokyo Japan' |NaiveBayes/mapper_classify.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_classification (__main__.Testchinese) ... ok\n",
      "test_model (__main__.Testchinese) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.013s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=0 failures=0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# START STUDENT CODE HW231HADOOP_CHINESE_UNIT_TEST\n",
    "!head -n 4 NaiveBayes/chineseExample.txt  |NaiveBayes/mapper_model.py| sort -k1,1 |NaiveBayes/reducer_model.py>NaiveBayes/NaiveBayes.txt\n",
    "import unittest\n",
    "import re\n",
    "class Testchinese(unittest.TestCase):\n",
    "\n",
    "    def test_model(self):\n",
    "        NBModel = NaiveBayesModel(\"NaiveBayes/NaiveBayes.txt\")\n",
    "        self.assertEqual([1,3,0.25,0.75], map(float, NBModel.model[\"ClassPriors\"]))\n",
    "        self.assertAlmostEqual(0.333,  float(NBModel.model[\"chinese\"][2]),2)\n",
    "        self.assertAlmostEqual(0.125,  float(NBModel.model[\"macao\"][3]),2)\n",
    "        \n",
    "    def test_classification(self):\n",
    "        NBModel = NaiveBayesModel(\"NaiveBayes/NaiveBayes.txt\")\n",
    "        line = 'D5\t0\tChinese Chinese\tChinese Tokyo Japan'.strip()\n",
    "        # split the line into words\n",
    "        parts = line.split(\"\\t\")\n",
    "        docID, docClass, title = parts[0:3]\n",
    "        if len(parts) == 4:\n",
    "            body = parts[3]\n",
    "        else:\n",
    "            body = \"\"\n",
    "        emailStr = regex.sub(' ', title.lower() + \" \" +body.lower()) #replace each punctuation with a space\n",
    "        emailStr = re.sub( '\\s+', ' ', emailStr )            # replace multiple spaces with a space\n",
    "        words = emailStr.split()\n",
    "        PrClass0GivenDoc, PrClass1GivenDoc = NBModel.classify(words)\n",
    "        self.assertAlmostEqual(0.0010282922839403295,  PrClass0GivenDoc,2)\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(Testchinese)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)\n",
    "# END STUDENT CODE HW231HADOOP_CHINESE_UNIT_TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.3.2 Learn a multinomial naive Bayes model (with no smoothing) by hand\n",
    "Learn the multinomial naive Bayes by hand and show the formulas, and your calculations in a nice tabular form. \n",
    "\n",
    "Compare your hand calculations for the following:\n",
    "\n",
    "* the learnt multinomial naive Bayes with NO smoothing\n",
    "* the classification of the D5 test document\n",
    "\n",
    "with textbook calculation listed here:\n",
    "\n",
    "* Note the worked example  [here](https://www.dropbox.com/s/f17c4mvmm5fuwav/chineseTestCaseFullyWorkedOut.png) is with smoothing \"https://www.dropbox.com/s/f17c4mvmm5fuwav/chineseTestCaseFullyWorkedOut.png\". It is taken from the IRBook [chapter](http://nlp.stanford.edu/IR-book/pdf/13bayes.pdf) on Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download the image with worked solution and render it in the next cell below.\n",
    "!curl \"https://www.dropbox.com/s/f17c4mvmm5fuwav/chineseTestCaseFullyWorkedOut.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"chineseTestCaseFullyWorkedOut.png\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<img src=\"chineseTestCaseFullyWorkedOut.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand calculations for Multinomial naive Bayes (Learning and classifiction)\n",
    "\n",
    "* Insert hand calculations for learning a multinomial naive Bayes Classifier from the Chinese dataset\n",
    "* Insert hand calculations for classifying the test document \"D5\" using the learnt multinomial naive Bayes Classifier from the Chinese dataset\n",
    "\n",
    "#### HINTS:\n",
    "__ Because Markdown is a superset of HTML you can even add things like HTML tables__ \n",
    "\n",
    "For more background on notebook formatting see: [here](https://athena.brynmawr.edu/jupyter/hub/dblank/public/Jupyter%20Notebook%20Users%20Manual.ipynb, notebook formating)\n",
    "\n",
    "#### Example table in MD\n",
    "\n",
    "| This | is   |\n",
    "|------|------|\n",
    "|   a  | table|\n",
    "\n",
    "\n",
    "\n",
    "#### Learnt multinomial naive Bayes Model with Smoothing\n",
    "\n",
    "| Word | Word Class Conditional counts and probs. \\n dsdsd|\n",
    "|------|------|\n",
    "|Beijing|[0.0, 1.0, 0.111, 0.142]|\n",
    "|Chinese|[1.0, 5.0, 0.222, 0.428]|\n",
    "|Tokyo| [1.0, 0.0, 0.222, 0.0714]|\n",
    "|Shanghai|[0.0, 1.0, 0.111, 0.142]|\n",
    "|__ClassPrior__| [1.0, 3.0, 0.25, 0.75]|\n",
    "|Japan| [1.0, 0.0, 0.222, 0.071]|\n",
    "|Macao|[0.0, 1.0, 0.111, 0.142]|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "D1\t1\tChinese Beijing\tChinese\n",
    "D2\t1\tChinese Chinese\tShanghai\n",
    "D3\t1\tChinese\tMacao\n",
    "D4\t0\tTokyo Japan\tChinese\n",
    "Testing record D5\t0\tChinese Chinese\tChinese Tokyo Japan\n",
    "\n",
    "\n",
    "We are not using smoothing so math for each conditional probability becomes:-\n",
    "\n",
    "P(word|Class) = number of times \"word\" occurs in particular class  /  total number of words in that class\n",
    "\n",
    "No of words in class -SPAM = 8 \n",
    "No of words in class -HAM  =  3\n",
    "\n",
    "    P(chinese|Class = SPAM) = 5/ 8 = 0.625\n",
    "    P(chinese|class =HAM) = 1/3= 0.33333\n",
    "\n",
    "Similarly, \n",
    "\n",
    "    P(Beijing|Class = SPAM)  = 1/8 = 0.125 \n",
    "    P(Beijing|Class = HAM) = 0/3 = 0\n",
    "\n",
    "\n",
    "\n",
    "| Word | Pr(HAM)  |Pr(SPAM)   |\n",
    "|------|----------|-----------|\n",
    "|chinese | 0.3333   |0.625   |\n",
    "|beijing| 0   |0.125  |\n",
    "|tokyo | 0.3333   |0.0  |\n",
    "|shanghai | 0.0   |0.125   |\n",
    "|japan | 0.3333   |0.0   |\n",
    "|macao | 0.0 |0.125   |\n",
    "\n",
    "Additionaly we need prior probabilities of classes  here we have 3 SPAM and 1 HAM example\n",
    "so P(class= SPAM) = 3/4 = 0.75\n",
    "   P(class = HAM) = 1/4 = 0.25\n",
    "   \n",
    "Now for statement lets calculate probabilitis for each class .\n",
    "D5\t0\tChinese Chinese\tChinese Tokyo Japan\n",
    "\n",
    "    P(class = HAM) = 0.25 *0.3333 * 0.3333 * 0.3333 * 0.3333 * 0.3333 = 0.0010282922839403295\n",
    "    P(class=SPAM) = 0.75 * 0.625 * 0.625 * 0.625 *0*0 = 0\n",
    "\n",
    "So we take the arg max for last calcualtion and we will classify this as HAM i.e. class 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.3.3 Learn a multinomial naive Bayes model (with no smoothing) for SPAM filtering\n",
    "\n",
    "Systems test your code first with the Chinese Example and show the resulting model.\n",
    "\n",
    "Learn a SPAM filtering model from the ENRON dataset provided above. Save the model to file SPAM_Model_MNB.tsv.\n",
    "\n",
    "Show the top 10 terms  alphabetically sortig the words increasing and their corresponding model entries. Write a mapreduce job to accomplish this. Show the bottom 10 terms also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Map Reduce Job to learn a multinomical Naive Model from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted enronemail_1h.txt\n",
      "Deleted HW233HADOOP_MODEL_SPAM/model\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob4217934581366582297.jar tmpDir=null\n",
      "17/05/23 01:04:50 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/23 01:04:50 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/23 01:04:52 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/05/23 01:04:52 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/05/23 01:04:52 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1495464826300_0074\n",
      "17/05/23 01:04:53 INFO impl.YarnClientImpl: Submitted application application_1495464826300_0074\n",
      "17/05/23 01:04:53 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1495464826300_0074/\n",
      "17/05/23 01:04:53 INFO mapreduce.Job: Running job: job_1495464826300_0074\n",
      "17/05/23 01:05:01 INFO mapreduce.Job: Job job_1495464826300_0074 running in uber mode : false\n",
      "17/05/23 01:05:01 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/05/23 01:05:13 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/05/23 01:05:21 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/05/23 01:05:22 INFO mapreduce.Job: Job job_1495464826300_0074 completed successfully\n",
      "17/05/23 01:05:22 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1172006\n",
      "\t\tFILE: Number of bytes written=2696982\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217089\n",
      "\t\tHDFS: Number of bytes written=196151\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16820\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6282\n",
      "\t\tTotal time spent by all map tasks (ms)=16820\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6282\n",
      "\t\tTotal vcore-seconds taken by all map tasks=16820\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=6282\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=17223680\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6432768\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=33203\n",
      "\t\tMap output bytes=1105594\n",
      "\t\tMap output materialized bytes=1172012\n",
      "\t\tInput split bytes=222\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5408\n",
      "\t\tReduce shuffle bytes=1172012\n",
      "\t\tReduce input records=33203\n",
      "\t\tReduce output records=5409\n",
      "\t\tSpilled Records=66406\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=199\n",
      "\t\tCPU time spent (ms)=6090\n",
      "\t\tPhysical memory (bytes) snapshot=782577664\n",
      "\t\tVirtual memory (bytes) snapshot=4102815744\n",
      "\t\tTotal committed heap usage (bytes)=640155648\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=196151\n",
      "17/05/23 01:05:22 INFO streaming.StreamJob: Output directory: HW233HADOOP_MODEL_SPAM/model/\n",
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup          0 2017-05-23 01:05 HW233HADOOP_MODEL_SPAM/model/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup     196151 2017-05-23 01:05 HW233HADOOP_MODEL_SPAM/model/part-00000\n",
      "*****First top 10 lines******\n",
      "ClassPriors\t56,44,0.56,0.44\n",
      "\u0001\t21,0,0.00146689019279,0.0\n",
      "0\t0,6,0.0,0.000317678826706\n",
      "00\t19,14,0.00132718636491,0.000741250595648\n",
      "000\t25,27,0.00174629784856,0.00142955472018\n",
      "001\t2,1,0.000139703827885,5.29464711177e-05\n",
      "0011\t1,0,6.98519139424e-05,0.0\n",
      "00450\t0,1,0.0,5.29464711177e-05\n",
      "0080\t0,1,0.0,5.29464711177e-05\n",
      "01\t18,7,0.00125733445096,0.000370625297824\n",
      "***** bottom 10 lines******\n",
      "zac\t0,2,0.0,0.000105892942235\n",
      "zadorozhny\t4,0,0.00027940765577,0.0\n",
      "zero\t3,1,0.000209555741827,5.29464711177e-05\n",
      "zesto\t0,10,0.0,0.000529464711177\n",
      "zimin\t5,0,0.000349259569712,0.0\n",
      "zinc\t0,1,0.0,5.29464711177e-05\n",
      "zk\t0,1,0.0,5.29464711177e-05\n",
      "zo\t0,2,0.0,0.000105892942235\n",
      "zolam\t0,2,0.0,0.000105892942235\n",
      "zxs\t0,1,0.0,5.29464711177e-05\n"
     ]
    }
   ],
   "source": [
    "# START STUDENT CODE HW231HADOOP_MODEL_SPAM\n",
    "# STEP 1: make input directory on HDFS\n",
    "# !hdfs dfs -mkdir -p /user/Xxxxxxx\n",
    "\n",
    "# STEP2:  upload data to HDFS\n",
    "!hdfs dfs -rm enronemail_1h.txt\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/enronemail_1h.txt\n",
    "# STEP3: Make sure to remove the results directiry\n",
    "!hdfs dfs -rm -r HW233HADOOP_MODEL_SPAM/model/\n",
    "# STEP4: Run job\n",
    "\n",
    "# STEP5: display model (first 10 lines only)\n",
    "\n",
    "\n",
    "\n",
    "##################### IMPORTANT ########################################################################\n",
    "# Make sure you have the correct paths to the jar file as wel as the input and output files!!\n",
    "# make sure to include the -files option. Do ***** NOT ****** put spaces between the file paths!\n",
    "########################################################################################################\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1\"  \\\n",
    "  -files NaiveBayes/mapper_model.py,NaiveBayes/reducer_model.py \\\n",
    "  -mapper mapper_model.py\\\n",
    "  -reducer reducer_model.py \\\n",
    "  -input enronemail_1h.txt \\\n",
    "  -output HW233HADOOP_MODEL_SPAM/model/\\\n",
    "  -numReduceTasks 1\\\n",
    "  -cmdenv PATH=/opt/anaconda/bin:$PATH\n",
    "\n",
    "!hdfs dfs -ls HW233HADOOP_MODEL_SPAM/model/\n",
    "!rm NaiveBayes/SPAM_Model_MNB.tsv\n",
    "!hdfs dfs -get HW233HADOOP_MODEL_SPAM/model//part-00000 NaiveBayes/SPAM_Model_MNB.tsv\n",
    "print \"*****First top 10 lines******\"\n",
    "!head -n 10 NaiveBayes/SPAM_Model_MNB.tsv\n",
    "print \"***** bottom 10 lines******\"\n",
    "!tail -n 10 NaiveBayes/SPAM_Model_MNB.tsv\n",
    "# END STUDENT CODE HW231HADOOP_MODEL_SPAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.3.4 Classify Documents using the learnt Multinomial Naive Bayes model using Hadoop Streaming\n",
    "   \n",
    "Classify each Enron email messages using the learnt Naive Bayes classifier (Testing on the training set is bad practice but we will allow that here to simplify the work here).\n",
    "\n",
    "Write a separate map-reduce job to classify a corpus of documents using a provided/learnt Multinomial Naive Bayes model. A model file consisting of the triples  _word\\tPr(Word|HAM)\\tPr(Word|SPAM)_ should be broadcast to the worker nodes using the _-file_ command line option when running a Hadoop streaming job. Please write the corresponding mapper and reducer portions of this classifier job. \n",
    "\n",
    "#### Note: Map Tasks and map lifecycles\n",
    "Note that for each chunk in the input data a  mapper task is executed. Each mapper task  has three phases: a init phase (to initialize variables used down stream in the mapper task or read in data from disk that might be used downstream in the map task); a loop to process each record in the input stream; and a final phase that is executed prior to the map task finishing. A Reduce task goes through a similar lifecycle.\n",
    "\n",
    "#### NOTE: on small multiplying small numbers\n",
    "Multiplying lots of probabilities, which are between 0 and 1, can result in floating-point underflow. Since log(xy) = log(x) + log(y), it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities. Please pay attention to probabilites that are zero! They will need special attention. Count up how many times clasification of a document results in  a zero class posterior probabilty for each class and report when using the Enron training set for evaluation. \n",
    " \n",
    "* Report the performance of your learnt classifier in terms of misclassifcation error rate of your multinomial Naive Bayes Classifier. \n",
    "  *   Error Rate = misclassification rate with respect to a provided set (say training set in this case). It is more formally defined here:\n",
    " \n",
    "* Let DF represent the evalution set in the following:\n",
    "\n",
    "  * Err(Model, DF) = |{(X, c(X))  DF : c(X) != Model(x)}|   / |DF|\n",
    "\n",
    "Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model Model\n",
    "\n",
    "\n",
    "__In this exercise, please complete the following tasks:__\n",
    "* Once again unit test your classifier map reduce job using the Chinese example. Please show a trace of your prediction and classification steps.\n",
    "* Once you are happy with the results from the Chinese dataset, repeat the process for the Enron dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/mapper_classify.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/mapper_classify.py\n",
    "#!/usr/bin/env python\n",
    "from NaiveBayesModel import NaiveBayesModel\n",
    "import sys, re, string, subprocess\n",
    "import sys, operator, math \n",
    "import numpy as np\n",
    "from math import log\n",
    "from math import exp\n",
    "# Init mapper phase \n",
    "\n",
    "# read the MODEL into memory\n",
    "# The model file resides the local disk (make sure to ship it home from HDFS).\n",
    "# In the Hadoop command linke be sure to add the follow the -files commmand line option\n",
    "NBModel = NaiveBayesModel(\"NaiveBayes/SPAM_Model_MNB.tsv\") \n",
    "#NBModel.printModel()\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# inner loop mapper phase: process each record\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    parts = line.split(\"\\t\")\n",
    "    docID, docClass, title = parts[0:3]\n",
    "    if len(parts) == 4:\n",
    "        body = parts[3]\n",
    "    else:\n",
    "        body = \"\"\n",
    "    # use subject and body \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    emailStr = regex.sub(' ', title.lower() + \" \" +body.lower()) #replace each punctuation with a space\n",
    "    emailStr = re.sub( '\\s+', ' ', emailStr )            # replace multiple spaces with a space\n",
    "    # split the line into words\n",
    "    words = emailStr.split()\n",
    "\n",
    "# START STUDENT CODE HW231MAPPER_CLASSIFY\n",
    "      \n",
    "\n",
    "\n",
    "    #PrClass0GivenDoc, PrClass1GivenDoc = NBModel.classify(words)\n",
    "\n",
    "    #print \"Pr(Class=0| Doc=%s) is %6.5f\" % (docID, PrClass0GivenDoc)\n",
    "    #print \"Pr(Class=1| Doc=%s) is %6.5f\" % (docID, PrClass1GivenDoc)\n",
    "\n",
    "    PrClass0GivenDoc, PrClass1GivenDoc = NBModel.classifyInLogs(words)\n",
    "    if PrClass0GivenDoc > PrClass1GivenDoc:\n",
    "        print \"%s\\t%s\\t%s\"%(docID,0,docClass)\n",
    "    else:\n",
    "        print \"%s\\t%s\\t%s\"%(docID,1,docClass)\n",
    "    #print \"Pr(Class=0| Doc=D5) = %6.5f, log(Pr(Class=0| Doc=D5)) = %f\" % (exp(PrClass0GivenDoc), PrClass0GivenDoc)\n",
    "    #print \"Pr(Class=1| Doc=D5) = %6.5f, log(Pr(Class=1| Doc=D5)) = %f\" % (exp(PrClass1GivenDoc), PrClass1GivenDoc) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Map Reduce Job to classify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted enronemail_1h.txt\n",
      "Deleted HW234HADOOP_CLASSIFY_SPAM/classifications\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob8773647298264154187.jar tmpDir=null\n",
      "17/05/23 01:05:54 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/23 01:05:55 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/23 01:05:58 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/05/23 01:05:58 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/05/23 01:05:58 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1495464826300_0075\n",
      "17/05/23 01:05:59 INFO impl.YarnClientImpl: Submitted application application_1495464826300_0075\n",
      "17/05/23 01:05:59 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1495464826300_0075/\n",
      "17/05/23 01:05:59 INFO mapreduce.Job: Running job: job_1495464826300_0075\n",
      "17/05/23 01:06:20 INFO mapreduce.Job: Job job_1495464826300_0075 running in uber mode : false\n",
      "17/05/23 01:06:20 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/05/23 01:06:34 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/05/23 01:06:44 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/05/23 01:06:44 INFO mapreduce.Job: Job job_1495464826300_0075 completed successfully\n",
      "17/05/23 01:06:45 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2878\n",
      "\t\tFILE: Number of bytes written=357658\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217089\n",
      "\t\tHDFS: Number of bytes written=2806\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=23469\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6993\n",
      "\t\tTotal time spent by all map tasks (ms)=23469\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6993\n",
      "\t\tTotal vcore-seconds taken by all map tasks=23469\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=6993\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=24032256\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=7160832\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=2672\n",
      "\t\tMap output materialized bytes=2884\n",
      "\t\tInput split bytes=222\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=2884\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=102\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=246\n",
      "\t\tCPU time spent (ms)=2350\n",
      "\t\tPhysical memory (bytes) snapshot=711667712\n",
      "\t\tVirtual memory (bytes) snapshot=4090015744\n",
      "\t\tTotal committed heap usage (bytes)=600309760\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2806\n",
      "17/05/23 01:06:45 INFO streaming.StreamJob: Output directory: HW234HADOOP_CLASSIFY_SPAM/classifications/\n"
     ]
    }
   ],
   "source": [
    "# START STUDENT CODE HW234HADOOP_CLASSIFY_SPAM\n",
    "\n",
    "#run classifier job\n",
    "!hdfs dfs -rm enronemail_1h.txt\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/enronemail_1h.txt\n",
    "!hdfs dfs -rm -r HW234HADOOP_CLASSIFY_SPAM/classifications/\n",
    "\n",
    "##################### IMPORTANT ########################################################################\n",
    "# Make sure you have the correct paths to the jar file as wel as the input and output files!!\n",
    "# make sure to include the -files option. Do ***** NOT ****** put spaces between the file paths!\n",
    "########################################################################################################\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1\"  \\\n",
    "  -files NaiveBayes\\\n",
    "  -mapper NaiveBayes/mapper_classify.py\\\n",
    "  -reducer NaiveBayes/reducer_classify.py  \\\n",
    "  -input enronemail_1h.txt \\\n",
    "  -output HW234HADOOP_CLASSIFY_SPAM/classifications/\\\n",
    "  -numReduceTasks 1\\\n",
    "  -cmdenv PATH=/opt/anaconda/bin:$PATH\n",
    "\n",
    "\n",
    "#Print accuracy\n",
    "\n",
    "# END STUDENT CODE HW234HADOOP_CLASSIFY_SPAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the  accuracy measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION\tLABEL\tDOCID\r\n",
      "0\t0\t0001.1999-12-10.farmer\r\n",
      "0\t0\t0001.1999-12-10.kaminski\r\n",
      "0\t0\t0001.2000-01-17.beck\r\n",
      "0\t0\t0001.2000-06-06.lokay\r\n",
      "0\t0\t0001.2001-02-07.kitchen\r\n",
      "0\t0\t0001.2001-04-02.williams\r\n",
      "0\t0\t0002.1999-12-13.farmer\r\n",
      "0\t0\t0002.2001-02-07.kitchen\r\n",
      "1\t1\t0002.2001-05-25.SA_and_HP\r\n",
      "1\t1\t0002.2003-12-18.GP\r\n",
      "1\t1\t0002.2004-08-01.BG\r\n",
      "0\t0\t0003.1999-12-10.kaminski\r\n",
      "0\t0\t0003.1999-12-14.farmer\r\n",
      "0\t0\t0003.2000-01-17.beck\r\n",
      "0\t0\t0003.2001-02-08.kitchen\r\n",
      "1\t1\t0003.2003-12-18.GP\r\n",
      "1\t1\t0003.2004-08-01.BG\r\n",
      "0\t0\t0004.1999-12-10.kaminski\r\n",
      "0\t0\t0004.1999-12-14.farmer\r\n",
      "0\t0\t0004.2001-04-02.williams\r\n",
      "1\t1\t0004.2001-06-12.SA_and_HP\r\n",
      "1\t1\t0004.2004-08-01.BG\r\n",
      "0\t0\t0005.1999-12-12.kaminski\r\n",
      "0\t0\t0005.1999-12-14.farmer\r\n",
      "0\t0\t0005.2000-06-06.lokay\r\n",
      "0\t0\t0005.2001-02-08.kitchen\r\n",
      "1\t1\t0005.2001-06-23.SA_and_HP\r\n",
      "1\t1\t0005.2003-12-18.GP\r\n",
      "0\t0\t0006.1999-12-13.kaminski\r\n",
      "0\t0\t0006.2001-02-08.kitchen\r\n",
      "0\t0\t0006.2001-04-03.williams\r\n",
      "1\t1\t0006.2001-06-25.SA_and_HP\r\n",
      "1\t1\t0006.2003-12-18.GP\r\n",
      "1\t1\t0006.2004-08-01.BG\r\n",
      "0\t0\t0007.1999-12-13.kaminski\r\n",
      "0\t0\t0007.1999-12-14.farmer\r\n",
      "0\t0\t0007.2000-01-17.beck\r\n",
      "0\t0\t0007.2001-02-09.kitchen\r\n",
      "1\t1\t0007.2003-12-18.GP\r\n",
      "1\t1\t0007.2004-08-01.BG\r\n",
      "0\t0\t0008.2001-02-09.kitchen\r\n",
      "1\t1\t0008.2001-06-12.SA_and_HP\r\n",
      "1\t1\t0008.2001-06-25.SA_and_HP\r\n",
      "1\t1\t0008.2003-12-18.GP\r\n",
      "1\t1\t0008.2004-08-01.BG\r\n",
      "0\t0\t0009.1999-12-13.kaminski\r\n",
      "0\t0\t0009.1999-12-14.farmer\r\n",
      "0\t0\t0009.2000-06-07.lokay\r\n",
      "0\t0\t0009.2001-02-09.kitchen\r\n",
      "1\t1\t0009.2001-06-26.SA_and_HP\r\n",
      "1\t1\t0009.2003-12-18.GP\r\n",
      "0\t0\t0010.1999-12-14.farmer\r\n",
      "0\t0\t0010.1999-12-14.kaminski\r\n",
      "0\t0\t0010.2001-02-09.kitchen\r\n",
      "1\t1\t0010.2001-06-28.SA_and_HP\r\n",
      "1\t1\t0010.2003-12-18.GP\r\n",
      "1\t1\t0010.2004-08-01.BG\r\n",
      "0\t0\t0011.1999-12-14.farmer\r\n",
      "1\t1\t0011.2001-06-28.SA_and_HP\r\n",
      "1\t1\t0011.2001-06-29.SA_and_HP\r\n",
      "1\t1\t0011.2003-12-18.GP\r\n",
      "1\t1\t0011.2004-08-01.BG\r\n",
      "0\t0\t0012.1999-12-14.farmer\r\n",
      "0\t0\t0012.1999-12-14.kaminski\r\n",
      "0\t0\t0012.2000-01-17.beck\r\n",
      "0\t0\t0012.2000-06-08.lokay\r\n",
      "0\t0\t0012.2001-02-09.kitchen\r\n",
      "1\t1\t0012.2003-12-19.GP\r\n",
      "0\t0\t0013.1999-12-14.farmer\r\n",
      "0\t0\t0013.1999-12-14.kaminski\r\n",
      "0\t0\t0013.2001-04-03.williams\r\n",
      "1\t1\t0013.2001-06-30.SA_and_HP\r\n",
      "1\t1\t0013.2004-08-01.BG\r\n",
      "0\t0\t0014.1999-12-14.kaminski\r\n",
      "0\t0\t0014.1999-12-15.farmer\r\n",
      "0\t0\t0014.2001-02-12.kitchen\r\n",
      "1\t1\t0014.2001-07-04.SA_and_HP\r\n",
      "1\t1\t0014.2003-12-19.GP\r\n",
      "1\t1\t0014.2004-08-01.BG\r\n",
      "0\t0\t0015.1999-12-14.kaminski\r\n",
      "0\t0\t0015.1999-12-15.farmer\r\n",
      "0\t0\t0015.2000-06-09.lokay\r\n",
      "0\t0\t0015.2001-02-12.kitchen\r\n",
      "1\t1\t0015.2001-07-05.SA_and_HP\r\n",
      "1\t1\t0015.2003-12-19.GP\r\n",
      "0\t0\t0016.1999-12-15.farmer\r\n",
      "0\t0\t0016.2001-02-12.kitchen\r\n",
      "1\t1\t0016.2001-07-05.SA_and_HP\r\n",
      "1\t1\t0016.2001-07-06.SA_and_HP\r\n",
      "1\t1\t0016.2003-12-19.GP\r\n",
      "1\t1\t0016.2004-08-01.BG\r\n",
      "0\t0\t0017.1999-12-14.kaminski\r\n",
      "0\t0\t0017.2000-01-17.beck\r\n",
      "0\t0\t0017.2001-04-03.williams\r\n",
      "1\t1\t0017.2003-12-18.GP\r\n",
      "1\t1\t0017.2004-08-01.BG\r\n",
      "1\t1\t0017.2004-08-02.BG\r\n",
      "0\t0\t0018.1999-12-14.kaminski\r\n",
      "1\t1\t0018.2001-07-13.SA_and_HP\r\n",
      "1\t1\t0018.2003-12-18.GP\r\n",
      "Multinomial Naive Bayes Classifier Results are Total Records: 100, Misclassifications: 0, Accuracy :100.00000\t\r\n"
     ]
    }
   ],
   "source": [
    "# START STUDENT CODE HW234HADOOP_CLASSIFY_RESULTS\n",
    "!hdfs dfs -cat HW234HADOOP_CLASSIFY_SPAM/classifications/part-00000\n",
    "\n",
    "# END STUDENT CODE HW234HADOOP_CLASSIFY_RESULTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a histogram of the  posterior probabilities\n",
    "Plot a histogram of the  posterior probabilities (i.e., Pr(Class|Doc)) for each class over the ENRON training set. Summarize what you see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of SPAM with 0 posterior probs 56\n",
      "No of HAM with 0 posterior probs 44\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9398eaba10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "# START STUDENT CODE HW234PLOT\n",
    "from NaiveBayesModel import NaiveBayesModel\n",
    "NBModel = NaiveBayesModel(\"NaiveBayes/SPAM_Model_MNB.tsv\")\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "class0 =[]\n",
    "class1 = []\n",
    "with open(\"NaiveBayes/enronemail_1h.txt\") as f:\n",
    " \n",
    "  for line in f.readlines():\n",
    "\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    parts = line.split(\"\\t\")\n",
    "    docID, docClass, title = parts[0:3]\n",
    "    if len(parts) == 4:\n",
    "        body = parts[3]\n",
    "    else:\n",
    "        body = \"\"\n",
    "    # use subject and body \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    emailStr = regex.sub(' ', title.lower() + \" \" +body.lower()) #replace each punctuation with a space\n",
    "    emailStr = re.sub( '\\s+', ' ', emailStr )            # replace multiple spaces with a space\n",
    "    # split the line into words\n",
    "    words = emailStr.split()\n",
    "\n",
    "    PrClass0GivenDoc, PrClass1GivenDoc = NBModel.classifyInLogs(words)\n",
    "    \n",
    "    class0.append( PrClass0GivenDoc)\n",
    "    class1.append(PrClass1GivenDoc)\n",
    "cls0 = np.asarray(class0)\n",
    "cls0[np.where( cls0 == float(\"-inf\"))] = 0\n",
    "cls1 = np.asarray(class1)\n",
    "cls1[np.where( cls1 == float(\"-inf\"))] = 0\n",
    "plt.hist(cls1,color ='red',label ='spam')\n",
    "plt.hist(cls0,color ='green',label='ham')\n",
    "plt.title(\"Histogram Posterior probs\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "print \"No of SPAM with 0 posterior probs\" ,len(np.where(cls1 == 0)[0])\n",
    "print \"No of HAM with 0 posterior probs\",len(np.where(cls0 == 0)[0])\n",
    "# END STUDENT CODE HW234PLOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the NB posterior probability is: \n",
    "$$\n",
    "P(Y=y_k\\mid X_1,X_2, ...,X_n)=\\frac{P(Y=y_k)\\prod_i{P(X_i\\mid Y=y_k)}}{\\sum_j{P(Y=y_j)\\prod_i{P(X_i\\mid Y=y_j)}}}\n",
    "$$\n",
    "\n",
    "if denominator becomes 0 then posteriar probability is negative infinity or even 0 for practicle purpose. \n",
    "\n",
    "Now training data has 56 HAM and 44 SPAM emails.  IF we look at the above result, posterior proabilities for SPAM class then 56 of them have -inf or 0 probability.  Similary posterior probability for HAM class 44 of them have -inf or 0 probability. \n",
    "This can happen only if denominator becomes 0. So there are words which belong to strictly one class. \n",
    "\n",
    "Also accuracy is 100% this could be mainly due to using same data for traning and validations. NB classifier is overconfident i.e. proabilities of training set are very high but errors on validation will be high too. \n",
    "\n",
    "Add -1 or Laplace smoothing helps to reduce this issue.  But considering the results it make sense to call this algorithm \"Naive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.4 Use Laplace plus-one smoothing\n",
    "\n",
    "Repeat HW2.3 with the following modification: use Laplace plus-one smoothing.  **Please replace code tags like  HW231HADOOP_CLASSIFY with HW241HADOOP_CLASSIFY through out for your submission.**\n",
    "In addition, compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.\n",
    "\n",
    "### cut and paste MULTIPLE CELLS\n",
    "**NOTE** for **Jupyter lovers**, you can now cut and paste MULTIPLE CELLS: press the ESC key and the press the SHIFT key in conjunction with UP or DOWN arrow key to select the cells you wish to copy (or delete). Then press ESC-C (to copy). Then select the cell where you wish to copy the cells (in the buffer) and press ESC-V to insert the cells in the buffer below the selected cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.4.1 Learn a Multinomial Naive Bayes model on a small  dataset (Chinese dataset: 5 documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling Phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/mapper_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/mapper_model.py\n",
    "#!/usr/bin/env python\n",
    "import sys, re, string\n",
    "\n",
    "# Init mapper phase \n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# inner loop mapper phase: process each record\n",
    "# input comes from STDIN (standard input)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    # use subject and body \n",
    "    \n",
    "    parts = line.split(\"\\t\")\n",
    "    docID, docClass, title = parts[0:3]\n",
    "    if len(parts) == 4:\n",
    "        body = parts[3]\n",
    "    else:\n",
    "        body = \"\"\n",
    "    \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    emailStr = regex.sub(' ', title.lower() + \" \" +body.lower()) #replace each punctuation with a space\n",
    "    emailStr = re.sub( '\\s+', ' ', emailStr )            # replace multiple spaces with a space\n",
    "    # split the line into words\n",
    "    words = emailStr.split()\n",
    "    \n",
    "# START STUDENT CODE HW241MAPPER_MODEL\n",
    "\n",
    "# define regex for punctuation removal\n",
    "\n",
    "# increase counters\n",
    "# write the results to STDOUT (standard output);\n",
    "# what we output here will be the input for the\n",
    "# Reduce step, i.e. the input for reducer.py\n",
    "#\n",
    "# tab-delimited; the trivial word count is 1\n",
    "    \n",
    "    for w in words: \n",
    "        print \"%s\\t%d\\t%d\\t%s\"%(w,1,int(docClass),docID)# w, \"\\\n",
    "                \n",
    "# END STUDENT CODE HW241MAPPER_MODEL    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/reducer_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/reducer_model.py\n",
    "#!/usr/bin/env python\n",
    "from operator import itemgetter\n",
    "import sys, operator\n",
    "import numpy as np\n",
    "from  collections import OrderedDict \n",
    "\n",
    "# START STUDENT CODE HW241REDUCER_MODEL\n",
    "\n",
    "current_word = None\n",
    "smooth_factor = 1 # add 1 smoothing\n",
    "current_count = [smooth_factor, smooth_factor]\n",
    "msgIDs = {}\n",
    "word = None\n",
    "wordcount = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "# remove leading and trailing whitespace\n",
    "\n",
    "# parse the input we got from mapper.py\n",
    "\n",
    "# convert count and spam flag (currently a string) to int\n",
    "\n",
    "\n",
    "# handle msgID - store all IDs as we don't have too much\n",
    "# not the best way to get prior, a two-level MapReduce jobs (ID - word) would be optimal\n",
    "    \n",
    "# calculate NB parameters, and write the dictionary to a file for the classification job\n",
    "# prior probabilities\n",
    "\n",
    "# conditional probability\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count, isSpam, msgID = line.split('\\t', 3)\n",
    "   \n",
    "   \n",
    "    # convert count and spam flag (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "        isSpam = int(isSpam)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    \n",
    "    # handle msgID - store all IDs as we don't have too much\n",
    "    # not the best way to get prior, a two-level MapReduce jobs (ID - word) would be optimal\n",
    "    if msgID not in msgIDs:\n",
    "        msgIDs[msgID] = isSpam\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:        \n",
    "        current_count[isSpam] += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # count finish for one word, save it\n",
    "            wordcount[current_word] = current_count\n",
    "        # initialize new count for new word\n",
    "        current_count = [smooth_factor, smooth_factor]\n",
    "        current_count[isSpam] += count                \n",
    "        current_word = word\n",
    "\n",
    "# do not forget to save the last word count if needed!\n",
    "if current_word == word:    \n",
    "    wordcount[current_word] = current_count\n",
    "    \n",
    "# calculate NB parameters, and write the dictionary to a file for the classification job\n",
    "# prior probabilities\n",
    "n_msg = len(msgIDs)\n",
    "n_spam = sum(msgIDs.values())\n",
    "n_ham = n_msg - n_spam\n",
    "print '%s\\t%s,%s,%s,%s' %('ClassPriors',n_ham,n_spam ,1.0*n_ham/n_msg, 1.0*n_spam/n_msg)\n",
    "\n",
    "# conditional probability\n",
    "n_total = np.sum(wordcount.values(), 0)\n",
    "\n",
    "\n",
    "wordcount =OrderedDict(sorted(wordcount.items(), key=lambda t: t[0]))\n",
    "for (key,value) in zip(wordcount.keys(), wordcount.values()/(1.0*n_total)):\n",
    "    print '%s\\t%s,%s,%s,%s' %(key,wordcount[key][0] ,wordcount[key][1],value[0], value[1])\n",
    "    \n",
    "# END STUDENT CODE HW241REDUCER_MODEL    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/mapper_classify.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/mapper_classify.py\n",
    "#!/usr/bin/env python\n",
    "from NaiveBayesModel import NaiveBayesModel\n",
    "import sys, re, string, subprocess\n",
    "import sys, operator, math \n",
    "import numpy as np\n",
    "from math import log\n",
    "from math import exp\n",
    "# Init mapper phase \n",
    "\n",
    "# read the MODEL into memory\n",
    "# The model file resides the local disk (make sure to ship it home from HDFS).\n",
    "# In the Hadoop command linke be sure to add the follow the -files commmand line option\n",
    "NBModel = NaiveBayesModel(\"NaiveBayes/SPAM_Model_MNB.tsv\") \n",
    "#NBModel.printModel()\n",
    "# define regex for punctuation removal\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# inner loop mapper phase: process each record\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    parts = line.split(\"\\t\")\n",
    "    docID, docClass, title = parts[0:3]\n",
    "    if len(parts) == 4:\n",
    "        body = parts[3]\n",
    "    else:\n",
    "        body = \"\"\n",
    "    # use subject and body \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    emailStr = regex.sub(' ', title.lower() + \" \" +body.lower()) #replace each punctuation with a space\n",
    "    emailStr = re.sub( '\\s+', ' ', emailStr )            # replace multiple spaces with a space\n",
    "    # split the line into words\n",
    "    words = emailStr.split()\n",
    "\n",
    "# START STUDENT CODE HW241MAPPER_CLASSIFY\n",
    "      \n",
    "\n",
    "\n",
    "    #PrClass0GivenDoc, PrClass1GivenDoc = NBModel.classify(words)\n",
    "\n",
    "    #print \"Pr(Class=0| Doc=%s) is %6.5f\" % (docID, PrClass0GivenDoc)\n",
    "    #print \"Pr(Class=1| Doc=%s) is %6.5f\" % (docID, PrClass1GivenDoc)\n",
    "\n",
    "    PrClass0GivenDoc, PrClass1GivenDoc = NBModel.classifyInLogs(words)\n",
    "    if PrClass0GivenDoc > PrClass1GivenDoc:\n",
    "        print \"%s\\t%s\\t%s\"%(docID,0,docClass)\n",
    "    else:\n",
    "        print \"%s\\t%s\\t%s\"%(docID,1,docClass)\n",
    "    #print \"Pr(Class=0| Doc=D5) = %6.5f, log(Pr(Class=0| Doc=D5)) = %f\" % (exp(PrClass0GivenDoc), PrClass0GivenDoc)\n",
    "    #print \"Pr(Class=1| Doc=D5) = %6.5f, log(Pr(Class=1| Doc=D5)) = %f\" % (exp(PrClass1GivenDoc), PrClass1GivenDoc) \n",
    "    \n",
    "# END STUDENT CODE HW241MAPPER_CLASSIFY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/reducer_classify.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/reducer_classify.py\n",
    "#!/usr/bin/env python\n",
    "from operator import itemgetter\n",
    "import sys, operator, math\n",
    "import numpy as np\n",
    "\n",
    "numberOfRecords = 0\n",
    "NumberOfMisclassifications=0\n",
    "classificationAccurary = 0\n",
    "\n",
    "# START STUDENT CODE HW231REDUCER_CLASSIFY\n",
    "\n",
    "# input comes from STDIN\n",
    "print \"%s\\t%s\\t%s\"%(\"PREDICTION\",\"LABEL\",\"DOCID\")\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    #line = line.strip()\n",
    "    # split the line into words\n",
    "    parts = line.split(\"\\t\")\n",
    "    docID, preds, label = parts[0:3]\n",
    "    print \"%s\\t%s\\t%s\"%( preds, label,docID)\n",
    "    if int(preds) != int(label):\n",
    "        NumberOfMisclassifications +=1\n",
    "    numberOfRecords+=1\n",
    "classificationAccurary = float(float(numberOfRecords-NumberOfMisclassifications) / float(numberOfRecords)) * float(100)\n",
    "print 'Multinomial Naive Bayes Classifier Results are Total Records: %d,Mis-classes: %d,Accuracy: %2.5f' % (numberOfRecords, NumberOfMisclassifications, classificationAccurary) \n",
    "# END STUDENT CODE HW231REDUCER_CLASSIFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x NaiveBayes/mapper_model.py\n",
    "!chmod a+x NaiveBayes/reducer_model.py \n",
    "!chmod a+x NaiveBayes/mapper_classify.py\n",
    "!chmod a+x NaiveBayes/reducer_classify.py\n",
    "!chmod a+x NaiveBayes/NaiveBayesModel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinese dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted chineseExample_train.txt\n",
      "Deleted HW241HADOOP_CHINESE_UNIT_TEST/model\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob7735635027250612933.jar tmpDir=null\n",
      "17/05/23 01:33:50 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/23 01:33:51 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/23 01:33:52 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/05/23 01:33:53 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/05/23 01:33:53 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1495464826300_0080\n",
      "17/05/23 01:33:53 INFO impl.YarnClientImpl: Submitted application application_1495464826300_0080\n",
      "17/05/23 01:33:53 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1495464826300_0080/\n",
      "17/05/23 01:33:53 INFO mapreduce.Job: Running job: job_1495464826300_0080\n",
      "17/05/23 01:34:02 INFO mapreduce.Job: Job job_1495464826300_0080 running in uber mode : false\n",
      "17/05/23 01:34:02 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/05/23 01:34:12 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/05/23 01:34:13 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/05/23 01:34:19 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/05/23 01:34:20 INFO mapreduce.Job: Job job_1495464826300_0080 completed successfully\n",
      "17/05/23 01:34:20 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=188\n",
      "\t\tFILE: Number of bytes written=353388\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=391\n",
      "\t\tHDFS: Number of bytes written=275\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16972\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4954\n",
      "\t\tTotal time spent by all map tasks (ms)=16972\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4954\n",
      "\t\tTotal vcore-seconds taken by all map tasks=16972\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4954\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=17379328\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5072896\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=11\n",
      "\t\tMap output bytes=160\n",
      "\t\tMap output materialized bytes=194\n",
      "\t\tInput split bytes=236\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce shuffle bytes=194\n",
      "\t\tReduce input records=11\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=22\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=139\n",
      "\t\tCPU time spent (ms)=2600\n",
      "\t\tPhysical memory (bytes) snapshot=728772608\n",
      "\t\tVirtual memory (bytes) snapshot=4087533568\n",
      "\t\tTotal committed heap usage (bytes)=642252800\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=155\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=275\n",
      "17/05/23 01:34:20 INFO streaming.StreamJob: Output directory: HW241HADOOP_CHINESE_UNIT_TEST/model/\n"
     ]
    }
   ],
   "source": [
    "# START STUDENT CODE HW231HADOOP_MODEL\n",
    "# STEP 1: make input directory on HDFS\n",
    "#!hdfs dfs -mkdir -p /user/Xxxxxxx\n",
    "\n",
    "# STEP2:  upload data to HDFS\n",
    "!hdfs dfs -rm chineseExample_train.txt\n",
    "!rm NaiveBayes/chineseExample_train.txt\n",
    "!head -n 4 NaiveBayes/chineseExample.txt  > NaiveBayes/chineseExample_train.txt\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/chineseExample_train.txt\n",
    "!hdfs dfs -rm -r HW241HADOOP_CHINESE_UNIT_TEST/model/\n",
    "\n",
    "##################### IMPORTANT ########################################################################\n",
    "# Make sure you have the correct paths to the jar file as wel as the input and output files!!\n",
    "# make sure to include the -files option. Do ***** NOT ****** put spaces between the file paths!\n",
    "########################################################################################################\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1\"  \\\n",
    "  -files NaiveBayes/mapper_model.py,NaiveBayes/reducer_model.py \\\n",
    "  -mapper mapper_model.py\\\n",
    "  -reducer reducer_model.py \\\n",
    "  -input chineseExample_train.txt\\\n",
    "  -output HW241HADOOP_CHINESE_UNIT_TEST/model/\\\n",
    "  -numReduceTasks 1\\\n",
    "  -cmdenv PATH=/opt/anaconda/bin:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 root supergroup          0 2017-05-23 01:34 HW241HADOOP_CHINESE_UNIT_TEST/model/_SUCCESS\r\n",
      "-rw-r--r--   1 root supergroup        275 2017-05-23 01:34 HW241HADOOP_CHINESE_UNIT_TEST/model/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "#put results file into local directories\n",
    "\n",
    "!hdfs dfs -ls HW241HADOOP_CHINESE_UNIT_TEST/model/\n",
    "!rm NaiveBayes/SPAM_Model_MNB.tsv\n",
    "!hdfs dfs -get HW241HADOOP_CHINESE_UNIT_TEST/model/part-00000 NaiveBayes/SPAM_Model_MNB.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted chineseExample_test.txt\n",
      "Deleted HW241HADOOP_CHINESE_UNIT_TEST/classifications\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob9174515566799801667.jar tmpDir=null\n",
      "17/05/23 01:34:42 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/23 01:34:42 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/23 01:34:44 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/05/23 01:34:45 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/05/23 01:34:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1495464826300_0081\n",
      "17/05/23 01:34:45 INFO impl.YarnClientImpl: Submitted application application_1495464826300_0081\n",
      "17/05/23 01:34:45 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1495464826300_0081/\n",
      "17/05/23 01:34:45 INFO mapreduce.Job: Running job: job_1495464826300_0081\n",
      "17/05/23 01:34:55 INFO mapreduce.Job: Job job_1495464826300_0081 running in uber mode : false\n",
      "17/05/23 01:34:55 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/05/23 01:35:10 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/05/23 01:35:11 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/05/23 01:35:22 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/05/23 01:35:22 INFO mapreduce.Job: Job job_1495464826300_0081 completed successfully\n",
      "17/05/23 01:35:22 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=15\n",
      "\t\tFILE: Number of bytes written=351962\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=294\n",
      "\t\tHDFS: Number of bytes written=129\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=26132\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8151\n",
      "\t\tTotal time spent by all map tasks (ms)=26132\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8151\n",
      "\t\tTotal vcore-seconds taken by all map tasks=26132\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=8151\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=26759168\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=8346624\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=1\n",
      "\t\tMap output bytes=7\n",
      "\t\tMap output materialized bytes=21\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=21\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=2\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=455\n",
      "\t\tCPU time spent (ms)=4000\n",
      "\t\tPhysical memory (bytes) snapshot=752771072\n",
      "\t\tVirtual memory (bytes) snapshot=4100390912\n",
      "\t\tTotal committed heap usage (bytes)=597164032\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=60\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=129\n",
      "17/05/23 01:35:22 INFO streaming.StreamJob: Output directory: HW241HADOOP_CHINESE_UNIT_TEST/classifications/\n"
     ]
    }
   ],
   "source": [
    " # START STUDENT CODE HW241HADOOP_CLASSIFY\n",
    "!hdfs dfs -rm chineseExample_test.txt    \n",
    "!rm NaiveBayes/chineseExample_test.txt\n",
    "!tail -n 1 NaiveBayes/chineseExample.txt  > NaiveBayes/chineseExample_test.txt\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/chineseExample_test.txt\n",
    "!hdfs dfs -rm -r HW241HADOOP_CHINESE_UNIT_TEST/classifications/\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1\"  \\\n",
    "    -files NaiveBayes\\\n",
    "    -mapper NaiveBayes/mapper_classify.py\\\n",
    "    -reducer NaiveBayes/reducer_classify.py  \\\n",
    "    -input chineseExample_test.txt \\\n",
    "    -output HW241HADOOP_CHINESE_UNIT_TEST/classifications/\\\n",
    "    -numReduceTasks 1\\\n",
    "    -cmdenv PATH=/opt/anaconda/bin:$PATH\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # END STUDENT CODE HW231HADOOP_CLASSIFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION\tLABEL\tDOCID\r\n",
      "1\t0\r\n",
      "\tD5\r\n",
      "Multinomial Naive Bayes Classifier Results are Total Records: 1,Mis-classes: 1,Accuracy: 0.00000\t\r\n"
     ]
    }
   ],
   "source": [
    "# START STUDENT CODE HW231HADOOP_CLASSIFY_RESULTS\n",
    "# (you may or may not need to add anything else in this block depending on your implementation)\n",
    "\n",
    "!hdfs dfs -cat HW241HADOOP_CHINESE_UNIT_TEST/classifications/part-00000\n",
    "\n",
    " # END STUDENT CODE HW231HADOOP_CLASSIFY_RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -n 4 NaiveBayes/chineseExample.txt  |NaiveBayes/mapper_model.py| sort -k1,1 |NaiveBayes/reducer_model.py>NaiveBayes/NaiveBayes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_classification (__main__.Testchinese) ... ok\n",
      "test_model (__main__.Testchinese) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.038s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=0 failures=0>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# START STUDENT CODE HW231HADOOP_CHINESE_UNIT_TEST\n",
    "!head -n 4 NaiveBayes/chineseExample.txt  |NaiveBayes/mapper_model.py| sort -k1,1 |NaiveBayes/reducer_model.py>NaiveBayes/NaiveBayes.txt\n",
    "import unittest\n",
    "import re\n",
    "class Testchinese(unittest.TestCase):\n",
    "    def test_model(self):\n",
    "        NBModel = NaiveBayesModel(\"NaiveBayes/NaiveBayes.txt\")\n",
    "        self.assertEqual([1,3,0.25,0.75], map(float, NBModel.model[\"ClassPriors\"]))\n",
    "        self.assertAlmostEqual(0.222,  float(NBModel.model[\"chinese\"][2]),3)\n",
    "        self.assertAlmostEqual(0.14285,  float(NBModel.model[\"macao\"][3]),3)\n",
    "        \n",
    "    def test_classification(self):\n",
    "        NBModel = NaiveBayesModel(\"NaiveBayes/NaiveBayes.txt\")\n",
    "        line = 'D5\t0\tChinese Chinese\tChinese Tokyo Japan'.strip()\n",
    "         # split the line into words\n",
    "        parts = line.split(\"\\t\")\n",
    "        docID, docClass, title = parts[0:3]\n",
    "        if len(parts) == 4:\n",
    "              body = parts[3]\n",
    "        else:\n",
    "              body = \"\"\n",
    "        emailStr = regex.sub(' ', title.lower() + \" \" +body.lower()) #replace each punctuation with a space\n",
    "        emailStr = re.sub( '\\s+', ' ', emailStr )            # replace multiple spaces with a space\n",
    "        words = emailStr.split()\n",
    "        PrClass0GivenDoc, PrClass1GivenDoc = NBModel.classify(words)\n",
    "        self.assertAlmostEqual(0.0001354,  PrClass0GivenDoc,2)\n",
    "        self.assertAlmostEqual(0.00291341,  PrClass0GivenDoc,2)\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(Testchinese)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)\n",
    " # END STUDENT CODE HW231HADOOP_CHINESE_UNIT_TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.4.2 Learn a multinomial naive Bayes model (with smoothing) by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Answer\n",
    "    \n",
    "    D1\t1\tChinese Beijing\tChinese\n",
    "    D2\t1\tChinese Chinese\tShanghai\n",
    "    D3\t1\tChinese\tMacao\n",
    "    D4\t0\tTokyo Japan\tChinese\n",
    "Testing record D5\t0\tChinese Chinese\tChinese Tokyo Japan\n",
    "\n",
    "\n",
    "We are using smoothing so math for each conditional probability becomes:-\n",
    "\n",
    "P(word|Class) = number of times \"word\" occurs in particular class + 1 /  total number of words in that class + Vocabularize size\n",
    "\n",
    "No of words in class -SPAM = 8 \n",
    "No of words in class -HAM  =  3\n",
    "dictionary size  = 6\n",
    "\n",
    "    P(chinese|Class = SPAM) = 5 + 1/ (8 + 6) = 6/14  \n",
    "    P(chinese|class =HAM) = 1 +1 /(3 + 6)= 2/9\n",
    "\n",
    "Similarly, \n",
    "\n",
    "    P(Beijing|Class = SPAM)  = 1+1/8 + 6= 2/9 = 0.2222\n",
    "    P(Beijing|Class = HAM) = 0 +1 /3 +6 = 1/9 = 0.1111\n",
    "\n",
    "\n",
    "\n",
    "| Word | Pr(HAM)  |Pr(SPAM)   |\n",
    "|------|----------|-----------|\n",
    "|chinese |  0.2222 |0.4285   |\n",
    "|beijing| 0.1111   |0.142857  |\n",
    "|tokyo | 0.222222   |0.071428 |\n",
    "|shanghai | 0.1111   |0.14285   |\n",
    "|japan | 0.22222   |0.07142  |\n",
    "|macao | 0.1111 |0.142857   |\n",
    "\n",
    "Additionaly we need prior probabilities of classes  here we have 3 SPAM and 1 HAM example\n",
    "so P(class= SPAM) = 3/4 = 0.75\n",
    "   P(class = HAM) = 1/4 = 0.25\n",
    "   \n",
    "Now for statement lets calculate probabilitis for each class .\n",
    "D5\t0\tChinese Chinese\tChinese Tokyo Japan\n",
    "\n",
    "    P(class = HAM) = 0.25 *0.2222 * 0.2222 * 0.2222 * 0.2222 * 0.2222 = 0.0001354129756629241\n",
    "    P(class=SPAM) = 0.75 * 0.4285 * 0.4285 *  0.4285 *0.2222*0.2222 = 0.00291341\n",
    "\n",
    "So we take the arg max for last calcualtion and we will classify this as HAM i.e. class is SPAM.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.4.3 Learn a multinomial naive Bayes model (with no smoothing) for SPAM filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Map Reduce Job to learn a multinomical Naive Model from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted enronemail_1h.txt\n",
      "Deleted HW243HADOOP_MODEL_SPAM/model\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob2087108273086325153.jar tmpDir=null\n",
      "17/05/23 01:35:44 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/23 01:35:45 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/23 01:35:46 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/05/23 01:35:46 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/05/23 01:35:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1495464826300_0082\n",
      "17/05/23 01:35:47 INFO impl.YarnClientImpl: Submitted application application_1495464826300_0082\n",
      "17/05/23 01:35:47 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1495464826300_0082/\n",
      "17/05/23 01:35:47 INFO mapreduce.Job: Running job: job_1495464826300_0082\n",
      "17/05/23 01:35:55 INFO mapreduce.Job: Job job_1495464826300_0082 running in uber mode : false\n",
      "17/05/23 01:35:55 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/05/23 01:36:07 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/05/23 01:36:15 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/05/23 01:36:15 INFO mapreduce.Job: Job job_1495464826300_0082 completed successfully\n",
      "17/05/23 01:36:15 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1172006\n",
      "\t\tFILE: Number of bytes written=2696982\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217089\n",
      "\t\tHDFS: Number of bytes written=255636\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18582\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5755\n",
      "\t\tTotal time spent by all map tasks (ms)=18582\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5755\n",
      "\t\tTotal vcore-seconds taken by all map tasks=18582\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5755\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=19027968\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5893120\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=33203\n",
      "\t\tMap output bytes=1105594\n",
      "\t\tMap output materialized bytes=1172012\n",
      "\t\tInput split bytes=222\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5408\n",
      "\t\tReduce shuffle bytes=1172012\n",
      "\t\tReduce input records=33203\n",
      "\t\tReduce output records=5409\n",
      "\t\tSpilled Records=66406\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=187\n",
      "\t\tCPU time spent (ms)=5810\n",
      "\t\tPhysical memory (bytes) snapshot=742260736\n",
      "\t\tVirtual memory (bytes) snapshot=4070760448\n",
      "\t\tTotal committed heap usage (bytes)=558891008\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=255636\n",
      "17/05/23 01:36:15 INFO streaming.StreamJob: Output directory: HW243HADOOP_MODEL_SPAM/model/\n",
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup          0 2017-05-23 01:36 HW243HADOOP_MODEL_SPAM/model/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup     255636 2017-05-23 01:36 HW243HADOOP_MODEL_SPAM/model/part-00000\n",
      "*****First top 10 lines******\n",
      "ClassPriors\t56,44,0.56,0.44\n",
      "\u0001\t22,1,0.00111539241533,4.1160732661e-05\n",
      "0\t1,7,5.06996552423e-05,0.000288125128627\n",
      "00\t20,15,0.00101399310485,0.000617410989916\n",
      "000\t26,28,0.0013181910363,0.00115250051451\n",
      "001\t3,2,0.000152098965727,8.23214653221e-05\n",
      "0011\t2,1,0.000101399310485,4.1160732661e-05\n",
      "00450\t1,2,5.06996552423e-05,8.23214653221e-05\n",
      "0080\t1,2,5.06996552423e-05,8.23214653221e-05\n",
      "01\t19,8,0.000963293449605,0.000329285861288\n",
      "***** bottom 10 lines******\n",
      "zac\t1,3,5.06996552423e-05,0.000123482197983\n",
      "zadorozhny\t5,1,0.000253498276212,4.1160732661e-05\n",
      "zero\t4,2,0.000202798620969,8.23214653221e-05\n",
      "zesto\t1,11,5.06996552423e-05,0.000452768059271\n",
      "zimin\t6,1,0.000304197931454,4.1160732661e-05\n",
      "zinc\t1,2,5.06996552423e-05,8.23214653221e-05\n",
      "zk\t1,2,5.06996552423e-05,8.23214653221e-05\n",
      "zo\t1,3,5.06996552423e-05,0.000123482197983\n",
      "zolam\t1,3,5.06996552423e-05,0.000123482197983\n",
      "zxs\t1,2,5.06996552423e-05,8.23214653221e-05\n"
     ]
    }
   ],
   "source": [
    "# START STUDENT CODE HW241HADOOP_MODEL_SPAM\n",
    "# STEP 1: make input directory on HDFS\n",
    "# !hdfs dfs -mkdir -p /user/Xxxxxxx\n",
    "\n",
    "# STEP2:  upload data to HDFS\n",
    "!hdfs dfs -rm enronemail_1h.txt\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/enronemail_1h.txt\n",
    "# STEP3: Make sure to remove the results directiry\n",
    "!hdfs dfs -rm -r HW243HADOOP_MODEL_SPAM/model/\n",
    "# STEP4: Run job\n",
    "\n",
    "# STEP5: display model (first 10 lines only)\n",
    "\n",
    "\n",
    "\n",
    "##################### IMPORTANT ########################################################################\n",
    "# Make sure you have the correct paths to the jar file as wel as the input and output files!!\n",
    "# make sure to include the -files option. Do ***** NOT ****** put spaces between the file paths!\n",
    "########################################################################################################\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1\"  \\\n",
    "  -files NaiveBayes/mapper_model.py,NaiveBayes/reducer_model.py \\\n",
    "  -mapper mapper_model.py\\\n",
    "  -reducer reducer_model.py \\\n",
    "  -input enronemail_1h.txt \\\n",
    "  -output HW243HADOOP_MODEL_SPAM/model/\\\n",
    "  -numReduceTasks 1\\\n",
    "  -cmdenv PATH=/opt/anaconda/bin:$PATH\n",
    "\n",
    "!hdfs dfs -ls HW243HADOOP_MODEL_SPAM/model/\n",
    "!rm NaiveBayes/SPAM_Model_MNB.tsv\n",
    "!hdfs dfs -get HW243HADOOP_MODEL_SPAM/model//part-00000 NaiveBayes/SPAM_Model_MNB.tsv\n",
    "print \"*****First top 10 lines******\"\n",
    "!head -n 10 NaiveBayes/SPAM_Model_MNB.tsv\n",
    "print \"***** bottom 10 lines******\"\n",
    "!tail -n 10 NaiveBayes/SPAM_Model_MNB.tsv\n",
    "# END STUDENT CODE HW241HADOOP_MODEL_SPAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.3.4 Classify Documents using the learnt Multinomial Naive Bayes model using Hadoop Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Map Reduce Job to classify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted enronemail_1h.txt\n",
      "Deleted HW244HADOOP_CLASSIFY_SPAM/classifications\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob7413994032638482562.jar tmpDir=null\n",
      "17/05/23 01:36:38 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/23 01:36:38 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/23 01:36:40 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/05/23 01:36:40 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/05/23 01:36:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1495464826300_0083\n",
      "17/05/23 01:36:41 INFO impl.YarnClientImpl: Submitted application application_1495464826300_0083\n",
      "17/05/23 01:36:41 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1495464826300_0083/\n",
      "17/05/23 01:36:41 INFO mapreduce.Job: Running job: job_1495464826300_0083\n",
      "17/05/23 01:36:51 INFO mapreduce.Job: Job job_1495464826300_0083 running in uber mode : false\n",
      "17/05/23 01:36:51 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/05/23 01:37:01 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/05/23 01:37:03 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/05/23 01:37:09 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/05/23 01:37:10 INFO mapreduce.Job: Job job_1495464826300_0083 completed successfully\n",
      "17/05/23 01:37:10 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2878\n",
      "\t\tFILE: Number of bytes written=357658\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217089\n",
      "\t\tHDFS: Number of bytes written=2897\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17729\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5571\n",
      "\t\tTotal time spent by all map tasks (ms)=17729\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5571\n",
      "\t\tTotal vcore-seconds taken by all map tasks=17729\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5571\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=18154496\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5704704\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=2672\n",
      "\t\tMap output materialized bytes=2884\n",
      "\t\tInput split bytes=222\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=2884\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=202\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=244\n",
      "\t\tCPU time spent (ms)=3510\n",
      "\t\tPhysical memory (bytes) snapshot=718843904\n",
      "\t\tVirtual memory (bytes) snapshot=4089524224\n",
      "\t\tTotal committed heap usage (bytes)=596115456\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2897\n",
      "17/05/23 01:37:10 INFO streaming.StreamJob: Output directory: HW244HADOOP_CLASSIFY_SPAM/classifications/\n"
     ]
    }
   ],
   "source": [
    "# START STUDENT CODE HW244HADOOP_CLASSIFY_SPAM\n",
    "\n",
    "#run classifier job\n",
    "!hdfs dfs -rm enronemail_1h.txt\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/enronemail_1h.txt\n",
    "!hdfs dfs -rm -r HW244HADOOP_CLASSIFY_SPAM/classifications/\n",
    "\n",
    "##################### IMPORTANT ########################################################################\n",
    "# Make sure you have the correct paths to the jar file as wel as the input and output files!!\n",
    "# make sure to include the -files option. Do ***** NOT ****** put spaces between the file paths!\n",
    "########################################################################################################\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1\"  \\\n",
    "  -files NaiveBayes\\\n",
    "  -mapper NaiveBayes/mapper_classify.py\\\n",
    "  -reducer NaiveBayes/reducer_classify.py  \\\n",
    "  -input enronemail_1h.txt \\\n",
    "  -output HW244HADOOP_CLASSIFY_SPAM/classifications/\\\n",
    "  -numReduceTasks 1\\\n",
    "  -cmdenv PATH=/opt/anaconda/bin:$PATH\n",
    "\n",
    "\n",
    "#Print accuracy\n",
    "\n",
    "# END STUDENT CODE HW234HADOOP_CLASSIFY_SPAM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Display the  accuracy measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION\tLABEL\tDOCID\r\n",
      "0\t0\r\n",
      "\t0001.1999-12-10.farmer\r\n",
      "0\t0\r\n",
      "\t0001.1999-12-10.kaminski\r\n",
      "0\t0\r\n",
      "\t0001.2000-01-17.beck\r\n",
      "0\t0\r\n",
      "\t0001.2000-06-06.lokay\r\n",
      "0\t0\r\n",
      "\t0001.2001-02-07.kitchen\r\n",
      "0\t0\r\n",
      "\t0001.2001-04-02.williams\r\n",
      "0\t0\r\n",
      "\t0002.1999-12-13.farmer\r\n",
      "0\t0\r\n",
      "\t0002.2001-02-07.kitchen\r\n",
      "1\t1\r\n",
      "\t0002.2001-05-25.SA_and_HP\r\n",
      "1\t1\r\n",
      "\t0002.2003-12-18.GP\r\n",
      "1\t1\r\n",
      "\t0002.2004-08-01.BG\r\n",
      "0\t0\r\n",
      "\t0003.1999-12-10.kaminski\r\n",
      "0\t0\r\n",
      "\t0003.1999-12-14.farmer\r\n",
      "0\t0\r\n",
      "\t0003.2000-01-17.beck\r\n",
      "0\t0\r\n",
      "\t0003.2001-02-08.kitchen\r\n",
      "1\t1\r\n",
      "\t0003.2003-12-18.GP\r\n",
      "1\t1\r\n",
      "\t0003.2004-08-01.BG\r\n",
      "0\t0\r\n",
      "\t0004.1999-12-10.kaminski\r\n",
      "0\t0\r\n",
      "\t0004.1999-12-14.farmer\r\n",
      "0\t0\r\n",
      "\t0004.2001-04-02.williams\r\n",
      "1\t1\r\n",
      "\t0004.2001-06-12.SA_and_HP\r\n",
      "1\t1\r\n",
      "\t0004.2004-08-01.BG\r\n",
      "0\t0\r\n",
      "\t0005.1999-12-12.kaminski\r\n",
      "0\t0\r\n",
      "\t0005.1999-12-14.farmer\r\n",
      "0\t0\r\n",
      "\t0005.2000-06-06.lokay\r\n",
      "0\t0\r\n",
      "\t0005.2001-02-08.kitchen\r\n",
      "1\t1\r\n",
      "\t0005.2001-06-23.SA_and_HP\r\n",
      "1\t1\r\n",
      "\t0005.2003-12-18.GP\r\n",
      "0\t0\r\n",
      "\t0006.1999-12-13.kaminski\r\n",
      "0\t0\r\n",
      "\t0006.2001-02-08.kitchen\r\n",
      "0\t0\r\n",
      "\t0006.2001-04-03.williams\r\n",
      "1\t1\r\n",
      "\t0006.2001-06-25.SA_and_HP\r\n",
      "1\t1\r\n",
      "\t0006.2003-12-18.GP\r\n",
      "1\t1\r\n",
      "\t0006.2004-08-01.BG\r\n",
      "0\t0\r\n",
      "\t0007.1999-12-13.kaminski\r\n",
      "0\t0\r\n",
      "\t0007.1999-12-14.farmer\r\n",
      "0\t0\r\n",
      "\t0007.2000-01-17.beck\r\n",
      "0\t0\r\n",
      "\t0007.2001-02-09.kitchen\r\n",
      "1\t1\r\n",
      "\t0007.2003-12-18.GP\r\n",
      "1\t1\r\n",
      "\t0007.2004-08-01.BG\r\n",
      "0\t0\r\n",
      "\t0008.2001-02-09.kitchen\r\n",
      "1\t1\r\n",
      "\t0008.2001-06-12.SA_and_HP\r\n",
      "1\t1\r\n",
      "\t0008.2001-06-25.SA_and_HP\r\n",
      "1\t1\r\n",
      "\t0008.2003-12-18.GP\r\n",
      "1\t1\r\n",
      "\t0008.2004-08-01.BG\r\n",
      "0\t0\r\n",
      "\t0009.1999-12-13.kaminski\r\n",
      "0\t0\r\n",
      "\t0009.1999-12-14.farmer\r\n",
      "0\t0\r\n",
      "\t0009.2000-06-07.lokay\r\n",
      "0\t0\r\n",
      "\t0009.2001-02-09.kitchen\r\n",
      "1\t1\r\n",
      "\t0009.2001-06-26.SA_and_HP\r\n",
      "1\t1\r\n",
      "\t0009.2003-12-18.GP\r\n",
      "0\t0\r\n",
      "\t0010.1999-12-14.farmer\r\n",
      "0\t0\r\n",
      "\t0010.1999-12-14.kaminski\r\n",
      "0\t0\r\n",
      "\t0010.2001-02-09.kitchen\r\n",
      "1\t1\r\n",
      "\t0010.2001-06-28.SA_and_HP\r\n",
      "1\t1\r\n",
      "\t0010.2003-12-18.GP\r\n",
      "1\t1\r\n",
      "\t0010.2004-08-01.BG\r\n",
      "0\t0\r\n",
      "\t0011.1999-12-14.farmer\r\n",
      "1\t1\r\n",
      "\t0011.2001-06-28.SA_and_HP\r\n",
      "1\t1\r\n",
      "\t0011.2001-06-29.SA_and_HP\r\n",
      "1\t1\r\n",
      "\t0011.2003-12-18.GP\r\n",
      "1\t1\r\n",
      "\t0011.2004-08-01.BG\r\n",
      "0\t0\r\n",
      "\t0012.1999-12-14.farmer\r\n",
      "0\t0\r\n",
      "\t0012.1999-12-14.kaminski\r\n",
      "0\t0\r\n",
      "\t0012.2000-01-17.beck\r\n",
      "0\t0\r\n",
      "\t0012.2000-06-08.lokay\r\n",
      "0\t0\r\n",
      "\t0012.2001-02-09.kitchen\r\n",
      "1\t1\r\n",
      "\t0012.2003-12-19.GP\r\n",
      "0\t0\r\n",
      "\t0013.1999-12-14.farmer\r\n",
      "0\t0\r\n",
      "\t0013.1999-12-14.kaminski\r\n",
      "0\t0\r\n",
      "\t0013.2001-04-03.williams\r\n",
      "1\t1\r\n",
      "\t0013.2001-06-30.SA_and_HP\r\n",
      "1\t1\r\n",
      "\t0013.2004-08-01.BG\r\n",
      "0\t0\r\n",
      "\t0014.1999-12-14.kaminski\r\n",
      "0\t0\r\n",
      "\t0014.1999-12-15.farmer\r\n",
      "0\t0\r\n",
      "\t0014.2001-02-12.kitchen\r\n",
      "1\t1\r\n",
      "\t0014.2001-07-04.SA_and_HP\r\n",
      "1\t1\r\n",
      "\t0014.2003-12-19.GP\r\n",
      "1\t1\r\n",
      "\t0014.2004-08-01.BG\r\n",
      "0\t0\r\n",
      "\t0015.1999-12-14.kaminski\r\n",
      "0\t0\r\n",
      "\t0015.1999-12-15.farmer\r\n",
      "0\t0\r\n",
      "\t0015.2000-06-09.lokay\r\n",
      "0\t0\r\n",
      "\t0015.2001-02-12.kitchen\r\n",
      "1\t1\r\n",
      "\t0015.2001-07-05.SA_and_HP\r\n",
      "1\t1\r\n",
      "\t0015.2003-12-19.GP\r\n",
      "0\t0\r\n",
      "\t0016.1999-12-15.farmer\r\n",
      "0\t0\r\n",
      "\t0016.2001-02-12.kitchen\r\n",
      "1\t1\r\n",
      "\t0016.2001-07-05.SA_and_HP\r\n",
      "1\t1\r\n",
      "\t0016.2001-07-06.SA_and_HP\r\n",
      "1\t1\r\n",
      "\t0016.2003-12-19.GP\r\n",
      "1\t1\r\n",
      "\t0016.2004-08-01.BG\r\n",
      "0\t0\r\n",
      "\t0017.1999-12-14.kaminski\r\n",
      "0\t0\r\n",
      "\t0017.2000-01-17.beck\r\n",
      "0\t0\r\n",
      "\t0017.2001-04-03.williams\r\n",
      "1\t1\r\n",
      "\t0017.2003-12-18.GP\r\n",
      "1\t1\r\n",
      "\t0017.2004-08-01.BG\r\n",
      "1\t1\r\n",
      "\t0017.2004-08-02.BG\r\n",
      "0\t0\r\n",
      "\t0018.1999-12-14.kaminski\r\n",
      "1\t1\r\n",
      "\t0018.2001-07-13.SA_and_HP\r\n",
      "1\t1\r\n",
      "\t0018.2003-12-18.GP\r\n",
      "Multinomial Naive Bayes Classifier Results are Total Records: 100,Mis-classes: 0,Accuracy: 100.00000\t\r\n"
     ]
    }
   ],
   "source": [
    "# START STUDENT CODE HW244HADOOP_CLASSIFY_RESULTS\n",
    "!hdfs dfs -cat HW244HADOOP_CLASSIFY_SPAM/classifications/part-00000\n",
    "\n",
    "# END STUDENT CODE HW244HADOOP_CLASSIFY_RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a histogram of the  posterior probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of SPAM with 0 posterior probs 0\n",
      "No of HAM with 0 posterior probs 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwh3XdstOVUcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9391dd2410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "# START STUDENT CODE HW234PLOT\n",
    "from NaiveBayesModel import NaiveBayesModel\n",
    "NBModel = NaiveBayesModel(\"NaiveBayes/SPAM_Model_MNB.tsv\")\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "class0 =[]\n",
    "class1 = []\n",
    "with open(\"NaiveBayes/enronemail_1h.txt\") as f:\n",
    " \n",
    "  for line in f.readlines():\n",
    "\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    parts = line.split(\"\\t\")\n",
    "    docID, docClass, title = parts[0:3]\n",
    "    if len(parts) == 4:\n",
    "        body = parts[3]\n",
    "    else:\n",
    "        body = \"\"\n",
    "    # use subject and body \n",
    "    # remove punctuations, only have white-space as delimiter\n",
    "    emailStr = regex.sub(' ', title.lower() + \" \" +body.lower()) #replace each punctuation with a space\n",
    "    emailStr = re.sub( '\\s+', ' ', emailStr )            # replace multiple spaces with a space\n",
    "    # split the line into words\n",
    "    words = emailStr.split()\n",
    "\n",
    "    PrClass0GivenDoc, PrClass1GivenDoc = NBModel.classifyInLogs(words)\n",
    "    class0.append( float(PrClass0GivenDoc))\n",
    "    class1.append(float(PrClass1GivenDoc))\n",
    "\n",
    "cls0 = np.asarray(class0)\n",
    "#cls0[np.where( cls0 == float(\"-inf\"))] = 0\n",
    "cls1 = np.asarray(class1)\n",
    "#cls1[np.where( cls1 == float(\"-inf\"))] = 0\n",
    "plt.hist(cls1,color ='red',label ='spam')\n",
    "plt.hist(cls0,color ='green',label='ham')\n",
    "plt.title(\"Histogram Posterior probs\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "print \"No of SPAM with 0 posterior probs\" ,len(np.where(cls1 == 0)[0])\n",
    "print \"No of HAM with 0 posterior probs\",len(np.where(cls0 == 0)[0])\n",
    "# END STUDENT CODE HW234PLOT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result discussion with Smoothing\n",
    "So smoothing has added vocabulary in denominator so it would never be -inf hence as expected SPAM with posterior proability are 0 and same is the case for HAM.\n",
    "\n",
    "Also probabilities are have better spread than without smoothing. This should give better results if we test on validation step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.5 Ignore rare words  (Optional)\n",
    "\n",
    "Repeat HW2.4. **Please replace code tags like  HW231HADOOP_CLASSIFY with HW251HADOOP_CLASSIFY through out for your submission.**  This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset. Report the error and the change in error. \n",
    "\n",
    "__HINT:__ ignore tokens with a frequency of less than three (3). Think of this as a preprocessing step. How many mapreduce jobs do you need to solve thus homework? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Modifications to Mapper \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/reducer_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/reducer_model.py\n",
    "#!/usr/bin/env python\n",
    "from operator import itemgetter\n",
    "import sys, operator\n",
    "import numpy as np\n",
    "from  collections import OrderedDict \n",
    "\n",
    "# START STUDENT CODE HW241REDUCER_MODEL\n",
    "\n",
    "current_word = None\n",
    "smooth_factor = 1 # add 1 smoothing\n",
    "current_count = [smooth_factor, smooth_factor]\n",
    "msgIDs = {}\n",
    "word = None\n",
    "wordcount = {}\n",
    "\n",
    "# input comes from STDIN\n",
    "# remove leading and trailing whitespace\n",
    "\n",
    "# parse the input we got from mapper.py\n",
    "\n",
    "# convert count and spam flag (currently a string) to int\n",
    "\n",
    "\n",
    "# handle msgID - store all IDs as we don't have too much\n",
    "# not the best way to get prior, a two-level MapReduce jobs (ID - word) would be optimal\n",
    "    \n",
    "# calculate NB parameters, and write the dictionary to a file for the classification job\n",
    "# prior probabilities\n",
    "\n",
    "# conditional probability\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count, isSpam, msgID = line.split('\\t', 3)\n",
    "   \n",
    "   \n",
    "    # convert count and spam flag (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "        isSpam = int(isSpam)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    \n",
    "    # handle msgID - store all IDs as we don't have too much\n",
    "    # not the best way to get prior, a two-level MapReduce jobs (ID - word) would be optimal\n",
    "    if msgID not in msgIDs:\n",
    "        msgIDs[msgID] = isSpam\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:        \n",
    "        current_count[isSpam] += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # count finish for one word, save it\n",
    "            wordcount[current_word] = current_count\n",
    "        # initialize new count for new word\n",
    "        current_count = [smooth_factor, smooth_factor]\n",
    "        current_count[isSpam] += count                \n",
    "        current_word = word\n",
    "\n",
    "# do not forget to save the last word count if needed!\n",
    "if current_word == word:    \n",
    "    wordcount[current_word] = current_count\n",
    "    \n",
    "# calculate NB parameters, and write the dictionary to a file for the classification job\n",
    "# prior probabilities\n",
    "n_msg = len(msgIDs)\n",
    "n_spam = sum(msgIDs.values())\n",
    "n_ham = n_msg - n_spam\n",
    "print '%s\\t%s,%s,%s,%s' %('ClassPriors',n_ham,n_spam ,1.0*n_ham/n_msg, 1.0*n_spam/n_msg)\n",
    "\n",
    "# conditional probability\n",
    "n_total = np.sum(wordcount.values(), 0)\n",
    "\n",
    "\n",
    "wordcount =OrderedDict(sorted(wordcount.items(), key=lambda t: t[0]))\n",
    "for (key,value) in zip(wordcount.keys(), wordcount.values()/(1.0*n_total)):\n",
    "    if sum(wordcount[key]) > 3:\n",
    "        print '%s\\t%s,%s,%s,%s' %(key,wordcount[key][0] ,wordcount[key][1],value[0], value[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted enronemail_1h.txt\n",
      "Deleted HW243HADOOP_MODEL_SPAM/model\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob2398185635513936779.jar tmpDir=null\n",
      "17/05/23 01:56:22 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/23 01:56:22 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/23 01:56:23 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/05/23 01:56:24 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/05/23 01:56:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1495464826300_0084\n",
      "17/05/23 01:56:25 INFO impl.YarnClientImpl: Submitted application application_1495464826300_0084\n",
      "17/05/23 01:56:25 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1495464826300_0084/\n",
      "17/05/23 01:56:25 INFO mapreduce.Job: Running job: job_1495464826300_0084\n",
      "17/05/23 01:56:35 INFO mapreduce.Job: Job job_1495464826300_0084 running in uber mode : false\n",
      "17/05/23 01:56:35 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/05/23 01:56:48 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/05/23 01:56:49 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/05/23 01:56:57 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/05/23 01:56:59 INFO mapreduce.Job: Job job_1495464826300_0084 completed successfully\n",
      "17/05/23 01:57:00 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1172006\n",
      "\t\tFILE: Number of bytes written=2696982\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217089\n",
      "\t\tHDFS: Number of bytes written=131943\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=22004\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=7356\n",
      "\t\tTotal time spent by all map tasks (ms)=22004\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7356\n",
      "\t\tTotal vcore-seconds taken by all map tasks=22004\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=7356\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=22532096\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=7532544\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=33203\n",
      "\t\tMap output bytes=1105594\n",
      "\t\tMap output materialized bytes=1172012\n",
      "\t\tInput split bytes=222\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5408\n",
      "\t\tReduce shuffle bytes=1172012\n",
      "\t\tReduce input records=33203\n",
      "\t\tReduce output records=2806\n",
      "\t\tSpilled Records=66406\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=193\n",
      "\t\tCPU time spent (ms)=8000\n",
      "\t\tPhysical memory (bytes) snapshot=784080896\n",
      "\t\tVirtual memory (bytes) snapshot=4090281984\n",
      "\t\tTotal committed heap usage (bytes)=640155648\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216867\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=131943\n",
      "17/05/23 01:57:00 INFO streaming.StreamJob: Output directory: HW243HADOOP_MODEL_SPAM/model/\n",
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup          0 2017-05-23 01:56 HW243HADOOP_MODEL_SPAM/model/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup     131943 2017-05-23 01:56 HW243HADOOP_MODEL_SPAM/model/part-00000\n",
      "*****First top 10 lines******\n",
      "ClassPriors\t56,44,0.56,0.44\n",
      "\u0001\t22,1,0.00111539241533,4.1160732661e-05\n",
      "0\t1,7,5.06996552423e-05,0.000288125128627\n",
      "00\t20,15,0.00101399310485,0.000617410989916\n",
      "000\t26,28,0.0013181910363,0.00115250051451\n",
      "001\t3,2,0.000152098965727,8.23214653221e-05\n",
      "01\t19,8,0.000963293449605,0.000329285861288\n",
      "012\t5,1,0.000253498276212,4.1160732661e-05\n",
      "02\t20,1,0.00101399310485,4.1160732661e-05\n",
      "03\t4,1,0.000202798620969,4.1160732661e-05\n",
      "***** bottom 10 lines******\n",
      "yourself\t1,12,5.06996552423e-05,0.000493928791932\n",
      "yoursuccess\t1,3,5.06996552423e-05,0.000123482197983\n",
      "zaak\t1,3,5.06996552423e-05,0.000123482197983\n",
      "zac\t1,3,5.06996552423e-05,0.000123482197983\n",
      "zadorozhny\t5,1,0.000253498276212,4.1160732661e-05\n",
      "zero\t4,2,0.000202798620969,8.23214653221e-05\n",
      "zesto\t1,11,5.06996552423e-05,0.000452768059271\n",
      "zimin\t6,1,0.000304197931454,4.1160732661e-05\n",
      "zo\t1,3,5.06996552423e-05,0.000123482197983\n",
      "zolam\t1,3,5.06996552423e-05,0.000123482197983\n"
     ]
    }
   ],
   "source": [
    "# START STUDENT CODE HW251HADOOP_MODEL_SPAM\n",
    "# STEP 1: make input directory on HDFS\n",
    "# !hdfs dfs -mkdir -p /user/Xxxxxxx\n",
    "\n",
    "# STEP2:  upload data to HDFS\n",
    "!hdfs dfs -rm enronemail_1h.txt\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/enronemail_1h.txt\n",
    "# STEP3: Make sure to remove the results directiry\n",
    "!hdfs dfs -rm -r HW243HADOOP_MODEL_SPAM/model/\n",
    "# STEP4: Run job\n",
    "\n",
    "# STEP5: display model (first 10 lines only)\n",
    "\n",
    "\n",
    "\n",
    "##################### IMPORTANT ########################################################################\n",
    "# Make sure you have the correct paths to the jar file as wel as the input and output files!!\n",
    "# make sure to include the -files option. Do ***** NOT ****** put spaces between the file paths!\n",
    "########################################################################################################\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1\"  \\\n",
    "  -files NaiveBayes/mapper_model.py,NaiveBayes/reducer_model.py \\\n",
    "  -mapper mapper_model.py\\\n",
    "  -reducer reducer_model.py \\\n",
    "  -input enronemail_1h.txt \\\n",
    "  -output HW243HADOOP_MODEL_SPAM/model/\\\n",
    "  -numReduceTasks 1\\\n",
    "  -cmdenv PATH=/opt/anaconda/bin:$PATH\n",
    "\n",
    "!hdfs dfs -ls HW243HADOOP_MODEL_SPAM/model/\n",
    "!rm NaiveBayes/SPAM_Model_MNB.tsv\n",
    "!hdfs dfs -get HW243HADOOP_MODEL_SPAM/model//part-00000 NaiveBayes/SPAM_Model_MNB.tsv\n",
    "print \"*****First top 10 lines******\"\n",
    "!head -n 10 NaiveBayes/SPAM_Model_MNB.tsv\n",
    "print \"***** bottom 10 lines******\"\n",
    "!tail -n 10 NaiveBayes/SPAM_Model_MNB.tsv\n",
    "# END STUDENT CODE HW251HADOOP_MODEL_SPAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `enronemail_1h.txt': No such file or directory\n",
      "rm: `HW254HADOOP_CLASSIFY_SPAM/classifications/': No such file or directory\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob2171003369127400477.jar tmpDir=null\n",
      "17/05/23 01:58:40 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/23 01:58:41 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/05/23 01:58:44 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/05/23 01:58:44 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/05/23 01:58:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1495464826300_0086\n",
      "17/05/23 01:58:45 INFO impl.YarnClientImpl: Submitted application application_1495464826300_0086\n",
      "17/05/23 01:58:46 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1495464826300_0086/\n",
      "17/05/23 01:58:46 INFO mapreduce.Job: Running job: job_1495464826300_0086\n",
      "17/05/23 01:58:57 INFO mapreduce.Job: Job job_1495464826300_0086 running in uber mode : false\n",
      "17/05/23 01:58:57 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/05/23 01:59:05 INFO mapreduce.Job: Task Id : attempt_1495464826300_0086_m_000000_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "17/05/23 01:59:06 INFO mapreduce.Job: Task Id : attempt_1495464826300_0086_m_000001_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "17/05/23 01:59:14 INFO mapreduce.Job: Task Id : attempt_1495464826300_0086_m_000000_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "17/05/23 01:59:15 INFO mapreduce.Job: Task Id : attempt_1495464826300_0086_m_000001_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "17/05/23 01:59:28 INFO mapreduce.Job: Task Id : attempt_1495464826300_0086_m_000000_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "17/05/23 01:59:30 INFO mapreduce.Job: Task Id : attempt_1495464826300_0086_m_000001_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "17/05/23 01:59:37 INFO mapreduce.Job:  map 50% reduce 100%\n",
      "17/05/23 01:59:38 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/05/23 01:59:38 INFO mapreduce.Job: Job job_1495464826300_0086 failed with state FAILED due to: Task failed task_1495464826300_0086_m_000000\n",
      "Job failed as tasks failed. failedMaps:1 failedReduces:0\n",
      "\n",
      "17/05/23 01:59:38 INFO mapreduce.Job: Counters: 13\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=7\n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=8\n",
      "\t\tOther local map tasks=6\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=63433\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=63433\n",
      "\t\tTotal vcore-seconds taken by all map tasks=63433\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=64955392\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "17/05/23 01:59:38 ERROR streaming.StreamJob: Job not successful!\n",
      "Streaming Command Failed!\n"
     ]
    }
   ],
   "source": [
    "# START STUDENT CODE HW244HADOOP_CLASSIFY_SPAM\n",
    "\n",
    "#run classifier job\n",
    "!hdfs dfs -rm enronemail_1h.txt\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/enronemail_1h.txt\n",
    "!hdfs dfs -rm -r HW254HADOOP_CLASSIFY_SPAM/classifications/\n",
    "\n",
    "##################### IMPORTANT ########################################################################\n",
    "# Make sure you have the correct paths to the jar file as wel as the input and output files!!\n",
    "# make sure to include the -files option. Do ***** NOT ****** put spaces between the file paths!\n",
    "########################################################################################################\n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1\"  \\\n",
    "  -files NaiveBayes\\\n",
    "  -mapper NaiveBayes/mapper_classify.py\\\n",
    "  -reducer NaiveBayes/reducer_classify.py  \\\n",
    "  -input enronemail_1h.txt \\\n",
    "  -output HW254HADOOP_CLASSIFY_SPAM/classifications/\\\n",
    "  -numReduceTasks 1\\\n",
    "  -cmdenv PATH=/opt/anaconda/bin:$PATH\n",
    "\n",
    "\n",
    "#Print accuracy\n",
    "\n",
    "# END STUDENT CODE HW234HADOOP_CLASSIFY_SPAM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.6 Benchmark your code with the Python SciKit-Learn (OPTIONAL)\n",
    "\n",
    "HW2.6 Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm\n",
    "\n",
    "It always a good idea to benchmark your solutions against publicly available libraries/frameworks such as SciKit-Learn, the Machine Learning toolkit available in Python. In this exercise, we benchmark ourselves against the SciKit-Learn implementation of multinomial Naive Bayes.  For more information on this implementation see: http://scikit-learn.org/stable/modules/naive_bayes.html more  \n",
    "\n",
    "In this exercise, please complete the following tasks:\n",
    "\n",
    "* Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW2.5 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "* Prepare a table to present your results, where rows correspond to approach used (SkiKit-Learn versus your Hadoop implementation) and the column presents the training misclassification error\n",
    "* Explain/justify any differences in terms of training error rates over the dataset in HW2.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "import csv, re, string\n",
    "import numpy as np\n",
    "\n",
    "# read email message, and organize training data\n",
    "f = open('NaiveBayes/enronemail_1h.txt', 'r')\n",
    "txt = f.read().strip()\n",
    "f.close()\n",
    "emails = txt.split('\\n')\n",
    "train_label = [msg.strip().split('\\t', 2)[1] for msg in emails]\n",
    "train_data = [msg.strip().split('\\t', 2)[-1] for msg in emails]\n",
    "# get rid of the funky characters\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "train_data = [' '.join(regex.sub(' ', msg).split()).decode('latin-1') for msg in train_data]\n",
    "\n",
    "# read test data\n",
    "with open('NaiveBayes/enronemail_1h.txt', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    emails = list(reader)\n",
    "test_label = [msg[1] for msg in emails]\n",
    "test_data = [msg[2]+msg[3] if len(msg)==4 else msg[2] for msg in emails]\n",
    "\n",
    "# feature vectorization\n",
    "uniVectorizer = CountVectorizer()\n",
    "dtmTrain = uniVectorizer.fit_transform(train_data) \n",
    "dtmTest = uniVectorizer.transform(test_data)\n",
    "\n",
    "# multinomial Naive Bayes Classifier from sklearn\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(dtmTrain, train_label)\n",
    "pred_mnb = mnb.predict_proba(dtmTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f6c800a94d0>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJDUMBUlS4/8D5zbDYieinkEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c87953e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(pred_mnb[0],color ='green',label ='spam')\n",
    "plt.hist(pred_mnb[1],color ='red',label='ham')\n",
    "plt.title(\"Probabilities\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.6.1 Bernoulli Naive Bayes (OPTIONAL: note this exercise is a stretch HW and optional)\n",
    "-  Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW2.6 and report the misclassification error \n",
    "-  Discuss the performance differences in terms of misclassification error rates over the dataset in HW2.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn. Why such big differences. Explain. \n",
    "\n",
    "Which approach to Naive Bayes would you recommend for SPAM detection? Justify your selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.7 Preprocess the Entire Spam Dataset (OPTIONAL)\n",
    "\n",
    "The Enron SPAM data in the following folder [enron1-Training-Data-RAW](https://www.dropbox.com/sh/hemnvr0422nr36g/AAAPoK-aYxkFGxGjzaeRNEwSa?dl=0) is in raw text form (with subfolders for SPAM and HAM that contain raw email messages in the following form:\n",
    "\n",
    "   * Line 1 contains the subject\n",
    "   * The remaining lines contain the body of the email message.\n",
    "\n",
    "In Python write a script to produce a TSV file called train-Enron-1.txt that has a similar format as the enronemail_1h.txt that you have been using so far. Please pay attend to funky characters and tabs. Check your resulting formated email data in Excel and in Python (e.g., count up the number of fields in each row; the number of SPAM mails and the number of HAM emails). Does each row correspond to an email record with four values? Note: use \"NA\" to denote empty field values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.8 Build and evaluate a NB classifier on the  Entire Spam Dataset (OPTIONAL) <a name=\"2.8\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "Using Hadoop Map-Reduce write job(s) to perform the following:\n",
    " -- Train a multinomial Naive Bayes Classifier with Laplace plus one smoothing using the data extracted in HW2.7 (i.e., train-Enron-1.txt). Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Drop tokens with a frequency of less than three (3).\n",
    " -- Test the learnt classifier using enronemail_1h.txt and report the misclassification error rate. Remember to use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). How do we treat tokens in the test set that do not appear in the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.8.1 OPTIONAL\n",
    "-  Run  both the Multinomial Naive Bayes and the Bernoulli Naive Bayes algorithms from SciKit-Learn (using default settings) over the same training data used in HW2.8 and report the misclassification error on both the training set and the testing set\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SciKit-Learn Multinomial NB; SciKit-Learn Bernouili NB; Your Hadoop implementation)  and the columns presents the training misclassification error, and the misclassification error on the test data set\n",
    "-  Discuss the performance differences in terms of misclassification error rates over the test and training datasets by the different implementations. Which approch (Bernouili versus Multinomial) would you recommend for SPAM detection? Justify your selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><div class='jumbotron'><h2 style='color:green'>-------  END OF HOWEWORK --------</h2></div></center>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
