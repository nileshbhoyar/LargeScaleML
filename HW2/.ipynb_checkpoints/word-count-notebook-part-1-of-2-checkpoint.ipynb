{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Sorting-with-linux-sort\" data-toc-modified-id=\"Sorting-with-linux-sort-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Sorting with linux sort</a></div><div class=\"lev3 toc-item\"><a href=\"#Refer-to-the-Sort-Notebook:\" data-toc-modified-id=\"Refer-to-the-Sort-Notebook:-101\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Refer to the Sort Notebook:</a></div><div class=\"lev1 toc-item\"><a href=\"#Key-patterns---Practice-makes-perfect!\" data-toc-modified-id=\"Key-patterns---Practice-makes-perfect!-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Key patterns - Practice makes perfect!</a></div><div class=\"lev3 toc-item\"><a href=\"#Word-Count-(custom-partitioner,-custom-sort)\" data-toc-modified-id=\"Word-Count-(custom-partitioner,-custom-sort)-201\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Word Count (custom partitioner, custom sort)</a></div><div class=\"lev3 toc-item\"><a href=\"#Word-Relative-Frequencies-(order-inversion,-custom-partitioner,-sort)\" data-toc-modified-id=\"Word-Relative-Frequencies-(order-inversion,-custom-partitioner,-sort)-202\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>Word Relative Frequencies (order inversion, custom partitioner, sort)</a></div><div class=\"lev1 toc-item\"><a href=\"#Key-Patterns:\" data-toc-modified-id=\"Key-Patterns:-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Key Patterns:</a></div><div class=\"lev2 toc-item\"><a href=\"#Word-Count-(custom-partitioner,-custom-sort)\" data-toc-modified-id=\"Word-Count-(custom-partitioner,-custom-sort)-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Word Count (custom partitioner, custom sort)</a></div><div class=\"lev3 toc-item\"><a href=\"#Word-count-on-a-single-machine-using-a-key-value-dictionary\" data-toc-modified-id=\"Word-count-on-a-single-machine-using-a-key-value-dictionary-311\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span></a></div><div class=\"lev3 toc-item\"><a href=\"#Word-count-on-multiple-machines-using-a-map-reduce-using-key-value-records-(stateless)\" data-toc-modified-id=\"Word-count-on-multiple-machines-using-a-map-reduce-using-key-value-records-(stateless)-312\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span></a></div><div class=\"lev3 toc-item\"><a href=\"#Word-count-on-multiple--machines--using-a-map-reduce-using-key-value-records-(stateless,-with-combiners)\" data-toc-modified-id=\"Word-count-on-multiple--machines--using-a-map-reduce-using-key-value-records-(stateless,-with-combiners)-313\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span></a></div><div class=\"lev3 toc-item\"><a href=\"#Total-sort-of-word-count-with-one-reducer\" data-toc-modified-id=\"Total-sort-of-word-count-with-one-reducer-314\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;</span></a></div><div class=\"lev3 toc-item\"><a href=\"#Total-sort-of-word-count-(tweaking-the-shuffle-phase)-multiple-reducers\" data-toc-modified-id=\"Total-sort-of-word-count-(tweaking-the-shuffle-phase)-multiple-reducers-315\"><span class=\"toc-item-num\">3.1.5&nbsp;&nbsp;</span></a></div><div class=\"lev2 toc-item\"><a href=\"#QUIZ:\" data-toc-modified-id=\"QUIZ:-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>QUIZ:</a></div><div class=\"lev3 toc-item\"><a href=\"#Using-4-reducers,-sort-on-frequency-(word-count).-Can-you-do-this-in-one-MR-job?\" data-toc-modified-id=\"Using-4-reducers,-sort-on-frequency-(word-count).-Can-you-do-this-in-one-MR-job?-321\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Using 4 reducers, sort on frequency (word count). Can you do this in one MR job?</a></div><div class=\"lev2 toc-item\"><a href=\"#QUIZ:\" data-toc-modified-id=\"QUIZ:-33\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>QUIZ:</a></div><div class=\"lev3 toc-item\"><a href=\"#Calculate-Vocabulary-size-(10-mins)\" data-toc-modified-id=\"Calculate-Vocabulary-size-(10-mins)-331\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Calculate Vocabulary size (10 mins)</a></div><div class=\"lev3 toc-item\"><a href=\"#ANSWER\" data-toc-modified-id=\"ANSWER-332\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>ANSWER</a></div><div class=\"lev2 toc-item\"><a href=\"#Word-Relative-Frequencies-(order-inversion,-custom-partitioner,-sort)\" data-toc-modified-id=\"Word-Relative-Frequencies-(order-inversion,-custom-partitioner,-sort)-34\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Word Relative Frequencies (order inversion, custom partitioner, sort)</a></div><div class=\"lev3 toc-item\"><a href=\"#Calculate-the-word-relative-frequency-using-a-&quot;stateful&quot;-reducer-on-a-single-reducer.\" data-toc-modified-id=\"Calculate-the-word-relative-frequency-using-a-&quot;stateful&quot;-reducer-on-a-single-reducer.-341\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span></a></div><div class=\"lev3 toc-item\"><a href=\"#Calculate-the-word-relative-frequency-using-a-&quot;stateless&quot;-reducer-(order-inversion)-on-a-single-reducer\" data-toc-modified-id=\"Calculate-the-word-relative-frequency-using-a-&quot;stateless&quot;-reducer-(order-inversion)-on-a-single-reducer-342\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span></a></div><div class=\"lev3 toc-item\"><a href=\"#Calculate-the-word-relative-frequency-using-a-&quot;stateless&quot;-reducer-(order-inversion)-on-multiple-reducers\" data-toc-modified-id=\"Calculate-the-word-relative-frequency-using-a-&quot;stateless&quot;-reducer-(order-inversion)-on-multiple-reducers-343\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span></a></div><div class=\"lev3 toc-item\"><a href=\"#Calculate-the-word-relative-frequency-using-a-&quot;stateless&quot;-reducer-(order-inversion)-on-a-multiple-reducers-with-combiners\" data-toc-modified-id=\"Calculate-the-word-relative-frequency-using-a-&quot;stateless&quot;-reducer-(order-inversion)-on-a-multiple-reducers-with-combiners-344\"><span class=\"toc-item-num\">3.4.4&nbsp;&nbsp;</span></a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cache the path to the jar file so you only have to change it in one spot.\n",
    "jarFile = '/usr/local/Cellar/hadoop/2.7.2/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting with linux sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Refer to the Sort Notebook:\n",
    "[http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/y6tbrw2jxpzni8l/_total-sort-guide-spark2.01-JAN27-2017.ipynb](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/y6tbrw2jxpzni8l/_total-sort-guide-spark2.01-JAN27-2017.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"top\"></a>\n",
    "# Key patterns - Practice makes perfect!\n",
    "### Word Count (custom partitioner, custom sort)\n",
    "1.0  [Word count on a single machine using a key-value dictionary](#1)     \n",
    "2.0  [Word count on multiple  machines  using a map reduce using key-value records (stateless)](#2)   \n",
    "3.0  [Word count on multiple  machines  using a map reduce using key-value records (stateless, with combiners)](#3)       \n",
    "4.1  [Total sort of word count - one reducer](#4.1)    \n",
    "4.2  [Total sort of word count (tweaking the shuffle phase) - multiple reducers](#4.2)    \n",
    "\n",
    "\n",
    "### Word Relative Frequencies (order inversion, custom partitioner, sort)\n",
    "5.0 [Calculate the word relative frequency using a \"stateful\" reducer on a single reducer.](#5)    \n",
    "6.0 [Calculate the word relative frequency using a \"stateless\" reducer (order inversion) on a single reducer](#6)    \n",
    "7.0 [Calculate the word relative frequency using a \"stateless\" reducer (order inversion) on a multiple reducers](#7)    \n",
    "8.0 [Calculate the word relative frequency using a \"stateless\" reducer (order inversion) on a multiple reducers with combiners](#8)    \n",
    "9.0 [Extending word count to do Multinomial Naive Bayes modeling](#9)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Key Patterns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count (custom partitioner, custom sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"1\">Word count on a single machine using a key-value dictionary</a>\n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 7)\n",
      "('accounting', 1)\n",
      "('activities', 3)\n",
      "('additional', 1)\n",
      "('administration', 1)\n",
      "('all', 3)\n",
      "('allocation', 1)\n",
      "('also', 3)\n",
      "('america', 2)\n",
      "('among', 1)\n"
     ]
    }
   ],
   "source": [
    "# Here is an example of wordcounting with a defaultdict (dictionary structure with a nice \n",
    "# default behaviours when a key does not exist in the dictionary\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "line = \"\"\" 0017.2000-01-17.beck\t0\t global risk management operations\t\" congratulations, sally!!!  kk  ----------------------forwarded by kathy kokas/corp/enron on 01/17/2000  08:08 pm---------------------------  from: rick causey 01/17/2000 06:04 pm  sent by: enron announcements  to: all enron worldwide  cc:  subject: global risk management operations  recognizing enron \u0001, s increasing worldwide presence in the wholesale energy  business and the need to insure outstanding internal controls for all of our  risk management activities, regardless of location, a global risk management  operations function has been created under the direction of sally w. beck,  vice president. in this role, sally will report to rick causey, executive  vice president and chief accounting officer.  sally \u0001, s responsibilities with regard to global risk management operations  will mirror those of other recently created enron global functions. in this  role, sally will work closely with all enron geographic regions and wholesale  companies to insure that each entity receives individualized regional support  while also focusing on the following global responsibilities:  1. enhance communication among risk management operations professionals.  2. assure the proliferation of best operational practices around the globe.  3. facilitate the allocation of human resources.  4. provide training for risk management operations personnel.  5. coordinate user requirements for shared operational systems.  6. oversee the creation of a global internal control audit plan for risk  management activities.  7. establish procedures for opening new risk management operations offices  and create key benchmarks for measuring on-going risk controls.  each regional operations team will continue its direct reporting relationship  within its business unit, and will collaborate with sally in the delivery of  these critical items. the houston-based risk management operations team under  sue frusco \u0001, s leadership, which currently supports risk management activities  for south america and australia, will also report directly to sally.  sally retains her role as vice president of energy operations for enron  north america, reporting to the ena office of the chairman. she has been in  her current role over energy operations since 1997, where she manages risk  consolidation and reporting, risk management administration, physical product  delivery, confirmations and cash management for ena \u0001, s physical commodity  trading, energy derivatives trading and financial products trading.  sally has been with enron since 1992, when she joined the company as a  manager in global credit. prior to joining enron, sally had four years  experience as a commercial banker and spent seven years as a registered  securities principal with a regional investment banking firm. she also owned  and managed a retail business for several years.  please join me in supporting sally in this additional coordination role for  global risk management operations.\"\"\"\n",
    "\n",
    "# Initialize a defaultdict, where the dafault value is an int\n",
    "wordCounts=defaultdict(int)\n",
    "\n",
    "# loop over all the words in the line, and increment the count as you see words \n",
    "for word in re.findall(r'[a-z]+', line.lower()):\n",
    "    wordCounts[word] += 1\n",
    "\n",
    "# Sort by word, and print the top 10 words with their counts   \n",
    "for key in sorted(wordCounts)[0:10]:\n",
    "    print (key, wordCounts[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"2\">Word count on multiple machines using a map reduce using key-value records (stateless)</a>\n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!curl 'http://www.gutenberg.org/files/11/11-0.txt' -o alicesTExtFilename.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing chineseExample.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile chineseExample.txt\n",
    "Chinese Beijing\tChinese\n",
    "Chinese Chinese\tShanghai\n",
    "Chinese\tMacao\n",
    "Tokyo Japan\tChinese\n",
    "Chinese Chinese\tChinese Tokyo Japan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "  line = line.strip()\n",
    "  for word in re.findall(r'[a-z]+', line.lower()):\n",
    "    print word,\"\\t\",1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    word, count = line.split('\\t')\n",
    "    count = int(count)\n",
    "    \n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "    \n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/user/koza/MROH': File exists\n",
      "mkdir: `/user/koza/MROH/wordcount': File exists\n"
     ]
    }
   ],
   "source": [
    "# Create some directories\n",
    "!hdfs dfs -mkdir /user/koza/MROH\n",
    "!hdfs dfs -mkdir /user/koza/MROH/wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/koza/MROH/wordcount/chineseExample.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "# Put the data into HDFS\n",
    "!hdfs dfs -put chineseExample.txt /user/koza/MROH/wordcount/chineseExample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numReduceTasks = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/koza/MROH/wordcount/output2': No such file or directory\n",
      "packageJobJar: [/var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/hadoop-unjar670235650687107972/] [] /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/streamjob5267599149240690105.jar tmpDir=null\n",
      "17/01/28 16:12:32 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/28 16:12:33 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/28 16:12:33 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/28 16:12:33 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/28 16:12:33 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485602883926_0033\n",
      "17/01/28 16:12:33 INFO impl.YarnClientImpl: Submitted application application_1485602883926_0033\n",
      "17/01/28 16:12:33 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1485602883926_0033/\n",
      "17/01/28 16:12:33 INFO mapreduce.Job: Running job: job_1485602883926_0033\n",
      "17/01/28 16:12:38 INFO mapreduce.Job: Job job_1485602883926_0033 running in uber mode : false\n",
      "17/01/28 16:12:38 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/28 16:12:43 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/28 16:12:47 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "17/01/28 16:12:49 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "17/01/28 16:12:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/28 16:12:50 INFO mapreduce.Job: Job job_1485602883926_0033 completed successfully\n",
      "17/01/28 16:12:50 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=223\n",
      "\t\tFILE: Number of bytes written=728674\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=411\n",
      "\t\tHDFS: Number of bytes written=61\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4239\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=9128\n",
      "\t\tTotal time spent by all map tasks (ms)=4239\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9128\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4239\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=9128\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4340736\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9347072\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5\n",
      "\t\tMap output records=16\n",
      "\t\tMap output bytes=167\n",
      "\t\tMap output materialized bytes=247\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce shuffle bytes=247\n",
      "\t\tReduce input records=16\n",
      "\t\tReduce output records=6\n",
      "\t\tSpilled Records=32\n",
      "\t\tShuffled Maps =8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tGC time elapsed (ms)=240\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=997195776\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=177\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=61\n",
      "17/01/28 16:12:50 INFO streaming.StreamJob: Output directory: /user/koza/MROH/wordcount/output2\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/koza/MROH/wordcount/output2\n",
    "\n",
    "!hadoop jar {jarFile} \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1\" \\\n",
    "    -files mapper2.py,reducer2.py \\\n",
    "    -mapper mapper2.py \\\n",
    "    -reducer reducer2.py \\\n",
    "    -input /user/koza/MROH/wordcount/chineseExample.txt \\\n",
    "    -output /user/koza/MROH/wordcount/output2 \\\n",
    "    -numReduceTasks {numReduceTasks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "-rw-r--r--   1 koza supergroup          0 2017-01-28 16:12 /user/koza/MROH/wordcount/output2/_SUCCESS\r\n",
      "-rw-r--r--   1 koza supergroup         32 2017-01-28 16:12 /user/koza/MROH/wordcount/output2/part-00000\r\n",
      "-rw-r--r--   1 koza supergroup          0 2017-01-28 16:12 /user/koza/MROH/wordcount/output2/part-00001\r\n",
      "-rw-r--r--   1 koza supergroup          0 2017-01-28 16:12 /user/koza/MROH/wordcount/output2/part-00002\r\n",
      "-rw-r--r--   1 koza supergroup         29 2017-01-28 16:12 /user/koza/MROH/wordcount/output2/part-00003\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/koza/MROH/wordcount/output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== part-00000 ===\n",
      "\n",
      "chinese \t9\n",
      "macao \t1\n",
      "shanghai \t1\n",
      "\n",
      "=== part-00001 ===\n",
      "\n",
      "\n",
      "=== part-00002 ===\n",
      "\n",
      "\n",
      "=== part-00003 ===\n",
      "\n",
      "beijing \t1\n",
      "japan \t2\n",
      "tokyo \t2\n"
     ]
    }
   ],
   "source": [
    "## PARTIAL SORT!\n",
    "for i in range(numReduceTasks):\n",
    "  print \"\\n=== part-0000\"+str(i),\"===\\n\"\n",
    "  !hdfs dfs -cat /user/koza/MROH/wordcount/output2/part-0000{i} | head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"3\">Word count on multiple  machines  using a map reduce using key-value records (stateless, with combiners)</a>\n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/koza/MROH/wordcount/output3': No such file or directory\n",
      "packageJobJar: [/var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/hadoop-unjar655164813556058579/] [] /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/streamjob1855617446528868483.jar tmpDir=null\n",
      "17/01/28 16:13:53 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/28 16:13:53 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/28 16:13:53 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/28 16:13:53 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/28 16:13:53 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485602883926_0034\n",
      "17/01/28 16:13:53 INFO impl.YarnClientImpl: Submitted application application_1485602883926_0034\n",
      "17/01/28 16:13:53 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1485602883926_0034/\n",
      "17/01/28 16:13:53 INFO mapreduce.Job: Running job: job_1485602883926_0034\n",
      "17/01/28 16:13:58 INFO mapreduce.Job: Job job_1485602883926_0034 running in uber mode : false\n",
      "17/01/28 16:13:58 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/28 16:14:03 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/28 16:14:08 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "17/01/28 16:14:09 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "17/01/28 16:14:10 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/28 16:14:11 INFO mapreduce.Job: Job job_1485602883926_0034 completed successfully\n",
      "17/01/28 16:14:11 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=110\n",
      "\t\tFILE: Number of bytes written=730332\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=411\n",
      "\t\tHDFS: Number of bytes written=61\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4309\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8676\n",
      "\t\tTotal time spent by all map tasks (ms)=4309\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8676\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4309\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8676\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4412416\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=8884224\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5\n",
      "\t\tMap output records=16\n",
      "\t\tMap output bytes=167\n",
      "\t\tMap output materialized bytes=134\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=16\n",
      "\t\tCombine output records=7\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce shuffle bytes=134\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=6\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tGC time elapsed (ms)=241\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=995622912\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=177\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=61\n",
      "17/01/28 16:14:11 INFO streaming.StreamJob: Output directory: /user/koza/MROH/wordcount/output3\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/koza/MROH/wordcount/output3\n",
    "\n",
    "!hadoop jar {jarFile} \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1\" \\\n",
    "    -files mapper2.py,reducer2.py \\\n",
    "    -mapper mapper2.py \\\n",
    "    -combiner reducer2.py \\\n",
    "    -reducer reducer2.py \\\n",
    "    -input /user/koza/MROH/wordcount/chineseExample.txt \\\n",
    "    -output /user/koza/MROH/wordcount/output3 \\\n",
    "    -numReduceTasks {numReduceTasks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== part-00000 ===\n",
      "\n",
      "chinese \t9\n",
      "macao \t1\n",
      "shanghai \t1\n",
      "\n",
      "=== part-00001 ===\n",
      "\n",
      "\n",
      "=== part-00002 ===\n",
      "\n",
      "\n",
      "=== part-00003 ===\n",
      "\n",
      "beijing \t1\n",
      "japan \t2\n",
      "tokyo \t2\n"
     ]
    }
   ],
   "source": [
    "for i in range(numReduceTasks):\n",
    "  print \"\\n=== part-0000\"+str(i),\"===\\n\"\n",
    "  !hdfs dfs -cat /user/koza/MROH/wordcount/output3/part-0000{i} | head\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"4.1\">Total sort of word count with one reducer</a>\n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper4.1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper4.1.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "for line in sys.stdin:\n",
    "  line = line.strip()\n",
    "  for word in re.findall(r'[a-z]+', line.lower()):\n",
    "    print word,\"\\t\",1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer4.1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer4.1.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    word, count = line.split('\\t')\n",
    "    count = int(count)\n",
    "    \n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "    \n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/01/28 16:15:56 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/koza/MROH/wordcount/output4.1\n",
      "packageJobJar: [/var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/hadoop-unjar7535077057860647055/] [] /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/streamjob4016212451635018761.jar tmpDir=null\n",
      "17/01/28 16:15:58 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/28 16:15:58 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/28 16:15:58 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/28 16:15:58 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/28 16:15:58 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485602883926_0036\n",
      "17/01/28 16:15:58 INFO impl.YarnClientImpl: Submitted application application_1485602883926_0036\n",
      "17/01/28 16:15:58 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1485602883926_0036/\n",
      "17/01/28 16:15:58 INFO mapreduce.Job: Running job: job_1485602883926_0036\n",
      "17/01/28 16:16:04 INFO mapreduce.Job: Job job_1485602883926_0036 running in uber mode : false\n",
      "17/01/28 16:16:04 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/28 16:16:08 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/28 16:16:13 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/28 16:16:13 INFO mapreduce.Job: Job job_1485602883926_0036 completed successfully\n",
      "17/01/28 16:16:13 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=92\n",
      "\t\tFILE: Number of bytes written=365295\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=411\n",
      "\t\tHDFS: Number of bytes written=61\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4170\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1797\n",
      "\t\tTotal time spent by all map tasks (ms)=4170\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1797\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4170\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1797\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4270080\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1840128\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5\n",
      "\t\tMap output records=16\n",
      "\t\tMap output bytes=167\n",
      "\t\tMap output materialized bytes=98\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=16\n",
      "\t\tCombine output records=7\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce shuffle bytes=98\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=6\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=123\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=552599552\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=177\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=61\n",
      "17/01/28 16:16:13 INFO streaming.StreamJob: Output directory: /user/koza/MROH/wordcount/output4.1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/koza/MROH/wordcount/output4.1\n",
    "\n",
    "!hadoop jar {jarFile} \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1\" \\\n",
    "    -files mapper4.1.py,reducer4.1.py \\\n",
    "    -mapper mapper4.1.py \\\n",
    "    -combiner reducer4.1.py \\\n",
    "    -reducer reducer4.1.py \\\n",
    "    -input /user/koza/MROH/wordcount/chineseExample.txt \\\n",
    "    -output /user/koza/MROH/wordcount/output4.1 \\\n",
    "    -numReduceTasks 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beijing \t1\r\n",
      "chinese \t9\r\n",
      "japan \t2\r\n",
      "macao \t1\r\n",
      "shanghai \t1\r\n",
      "tokyo \t2\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/koza/MROH/wordcount/output4.1/part-00000 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"4.2\">Total sort of word count (tweaking the shuffle phase) multiple reducers</a>\n",
    "Assuming uniform distribution over key space. Ulitmately you will want a percentile based partitioner. For details see the sampling section in the Sort Notebook [http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/y6tbrw2jxpzni8l/_total-sort-guide-spark2.01-JAN27-2017.ipynb#Section4](http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/y6tbrw2jxpzni8l/_total-sort-guide-spark2.01-JAN27-2017.ipynb#Section4)   \n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper4.2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper4.2.py\n",
    "#!/usr/bin/python\n",
    "from __future__ import division\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "count = 0\n",
    "numReducers = int(os.environ.get('NUM_PARTITIONS', '4')) # default to 4\n",
    "\n",
    "##################################### PARTITION THE DATA INTO N BUCKETS ####################################\n",
    "# lowercase chars range from 97 -> 122.\n",
    "# there are 26 chars. So let's partion by spliting at 26/n \n",
    "# ord('a') -> 97\n",
    "# chr(97) -> 'a'\n",
    "# ord(word[0])-96  --> get the number of the first letter between 1 and 26, such that a -> 1; z -> 26\n",
    "############################################################################################################\n",
    "\n",
    "def makeKey(word,n):\n",
    "  divisor = 26/n\n",
    "  return int(math.ceil((ord(word[0])-96)/divisor))\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "  line = line.strip()\n",
    "  for word in re.findall(r'[a-z]+', line.lower()):\n",
    "    # prepend a key based on the number of reducers\n",
    "    key = makeKey(word,numReducers)\n",
    "    print key,\"\\t\",word,\"\\t\",1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer4.2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer4.2.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    \n",
    "    key, word, count = line.split('\\t')\n",
    "    count = int(count)\n",
    "    \n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s\\t%s' % (key,current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "    \n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s\\t%s' % (key,current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/01/29 11:07:57 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/koza/MROH/wordcount/output4.2\n",
      "packageJobJar: [/var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/hadoop-unjar6724952486701868949/] [] /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/streamjob6234178667239276620.jar tmpDir=null\n",
      "17/01/29 11:07:58 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 11:07:58 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 11:07:59 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/29 11:07:59 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/29 11:07:59 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485602883926_0072\n",
      "17/01/29 11:07:59 INFO impl.YarnClientImpl: Submitted application application_1485602883926_0072\n",
      "17/01/29 11:07:59 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1485602883926_0072/\n",
      "17/01/29 11:07:59 INFO mapreduce.Job: Running job: job_1485602883926_0072\n",
      "17/01/29 11:08:04 INFO mapreduce.Job: Job job_1485602883926_0072 running in uber mode : false\n",
      "17/01/29 11:08:04 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/29 11:08:08 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/29 11:08:13 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "17/01/29 11:08:14 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "17/01/29 11:08:15 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/29 11:08:15 INFO mapreduce.Job: Job job_1485602883926_0072 completed successfully\n",
      "17/01/29 11:08:15 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=131\n",
      "\t\tFILE: Number of bytes written=733566\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=411\n",
      "\t\tHDFS: Number of bytes written=79\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4306\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8938\n",
      "\t\tTotal time spent by all map tasks (ms)=4306\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8938\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4306\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8938\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4409344\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9152512\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5\n",
      "\t\tMap output records=16\n",
      "\t\tMap output bytes=215\n",
      "\t\tMap output materialized bytes=155\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=16\n",
      "\t\tCombine output records=7\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=155\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=6\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tGC time elapsed (ms)=253\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=1002438656\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=177\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=79\n",
      "17/01/29 11:08:15 INFO streaming.StreamJob: Output directory: /user/koza/MROH/wordcount/output4.2\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/koza/MROH/wordcount/output4.2\n",
    "\n",
    "!hadoop jar {jarFile} \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2\" \\\n",
    "    -files mapper4.2.py,reducer4.2.py \\\n",
    "    -mapper mapper4.2.py \\\n",
    "    -combiner reducer4.2.py \\\n",
    "    -reducer reducer4.2.py \\\n",
    "    -input /user/koza/MROH/wordcount/chineseExample.txt \\\n",
    "    -output /user/koza/MROH/wordcount/output4.2 \\\n",
    "    -numReduceTasks 4 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -cmdenv NUM_PARTITIONS=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== part-00000 ===\n",
      "\n",
      "4 \ttokyo \t2\n",
      "\n",
      "=== part-00001 ===\n",
      "\n",
      "3 \tshanghai \t1\n",
      "\n",
      "=== part-00002 ===\n",
      "\n",
      "2 \tjapan \t2\n",
      "2 \tmacao \t1\n",
      "\n",
      "=== part-00003 ===\n",
      "\n",
      "1 \tbeijing \t1\n",
      "1 \tchinese \t9\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "  print \"\\n=== part-0000\"+str(i),\"===\\n\"\n",
    "  !hdfs dfs -cat /user/koza/MROH/wordcount/output4.2/part-0000{i}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUIZ: \n",
    "### Using 4 reducers, sort on frequency (word count). Can you do this in one MR job?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "No. In order to do the word countwe have to sort on the word. A second pass is required to sort on the frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUIZ:\n",
    "### Calculate Vocabulary size (10 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER\n",
    "There are lots of ways to do this. One way is to use the output from the word count with an identity mapper and simply add up the lines in a single reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vocab_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile vocab_reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "count = 0\n",
    "for line in sys.stdin:\n",
    "  count += 1\n",
    "print count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/01/28 16:22:31 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/koza/MROH/wordcount/vocab\n",
      "packageJobJar: [/var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/hadoop-unjar475798261940662101/] [] /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/streamjob5537003422403446907.jar tmpDir=null\n",
      "17/01/28 16:22:32 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/28 16:22:32 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/28 16:22:33 INFO mapred.FileInputFormat: Total input paths to process : 4\n",
      "17/01/28 16:22:33 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "17/01/28 16:22:33 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485602883926_0039\n",
      "17/01/28 16:22:33 INFO impl.YarnClientImpl: Submitted application application_1485602883926_0039\n",
      "17/01/28 16:22:33 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1485602883926_0039/\n",
      "17/01/28 16:22:33 INFO mapreduce.Job: Running job: job_1485602883926_0039\n",
      "17/01/28 16:22:39 INFO mapreduce.Job: Job job_1485602883926_0039 running in uber mode : false\n",
      "17/01/28 16:22:39 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/28 16:22:44 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/28 16:22:48 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/28 16:22:48 INFO mapreduce.Job: Job job_1485602883926_0039 completed successfully\n",
      "17/01/28 16:22:48 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=97\n",
      "\t\tFILE: Number of bytes written=603567\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=555\n",
      "\t\tHDFS: Number of bytes written=3\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=4\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12353\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1830\n",
      "\t\tTotal time spent by all map tasks (ms)=12353\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1830\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12353\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1830\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=12649472\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1873920\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=6\n",
      "\t\tMap output records=6\n",
      "\t\tMap output bytes=79\n",
      "\t\tMap output materialized bytes=115\n",
      "\t\tInput split bytes=476\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=115\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=12\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=267\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=955252736\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=79\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3\n",
      "17/01/28 16:22:48 INFO streaming.StreamJob: Output directory: /user/koza/MROH/wordcount/vocab\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/koza/MROH/wordcount/vocab\n",
    "!hadoop jar {jarFile} \\\n",
    "    -files vocab_reducer.py \\\n",
    "    -reducer vocab_reducer.py \\\n",
    "    -mapper /bin/cat \\\n",
    "    -input /user/koza/MROH/wordcount/output4.2 \\\n",
    "    -output /user/koza/MROH/wordcount/vocab \\\n",
    "    -numReduceTasks 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 koza supergroup          0 2017-01-28 16:22 /user/koza/MROH/wordcount/vocab/_SUCCESS\r\n",
      "-rw-r--r--   1 koza supergroup          3 2017-01-28 16:22 /user/koza/MROH/wordcount/vocab/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/koza/MROH/wordcount/vocab/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/koza/MROH/wordcount/vocab/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Relative Frequencies (order inversion, custom partitioner, sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['**ClassWordCountTotals', '2', 'a', 'd', 's']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfItems = ['a','s','d','2','**ClassWordCountTotals']\n",
    "sorted(listOfItems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"5\">Calculate the word relative frequency using a \"stateful\" reducer on a single reducer.</a>\n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"6\">Calculate the word relative frequency using a \"stateless\" reducer (order inversion) on a single reducer</a>\n",
    "__QUIZ:__ Is it more efficient to do one pass over the data and count the totals suing the order inversion pattern, or, to divide the task into two MR jobs and count up the totals and relative frequencies on the second pass?    \n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper6.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper6.py\n",
    "#!/usr/bin/python\n",
    "from __future__ import division\n",
    "import math\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "  line = line.strip()\n",
    "  for word in re.findall(r'[a-z]+', line.lower()):\n",
    "   \n",
    "    print \"**CountTotals\\t\",1\n",
    "    print word,\"\\t\",1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer6.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer6.py\n",
    "#!/usr/bin/python\n",
    "from __future__ import division\n",
    "import sys\n",
    "        \n",
    "total = 0\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    word, count = line.split()\n",
    "    count = int(count)\n",
    "    \n",
    "    # Order inversion pattern - counts up all the words first!\n",
    "    if word == \"**CountTotals\":\n",
    "        total += count\n",
    "    else:\n",
    "        if current_word == word:\n",
    "            current_count += count\n",
    "        else:\n",
    "            if current_word:\n",
    "                print '%s\\t%s\\t%s\\t%s' % (current_word, current_count, current_count/total,total)\n",
    "            current_count = count\n",
    "            current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s\\t%s\\t%s' % (current_word, current_count, current_count/total,total)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/01/28 16:43:13 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/koza/MROH/wordcount/output6\n",
      "packageJobJar: [/var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/hadoop-unjar2730532263278377098/] [] /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/streamjob6478016498858373958.jar tmpDir=null\n",
      "17/01/28 16:43:14 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/28 16:43:14 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/28 16:43:15 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/28 16:43:15 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/28 16:43:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1485602883926_0043\n",
      "17/01/28 16:43:15 INFO impl.YarnClientImpl: Submitted application application_1485602883926_0043\n",
      "17/01/28 16:43:15 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1485602883926_0043/\n",
      "17/01/28 16:43:15 INFO mapreduce.Job: Running job: job_1485602883926_0043\n",
      "17/01/28 16:43:20 INFO mapreduce.Job: Job job_1485602883926_0043 running in uber mode : false\n",
      "17/01/28 16:43:20 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/28 16:43:25 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/28 16:43:29 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/28 16:43:29 INFO mapreduce.Job: Job job_1485602883926_0043 completed successfully\n",
      "17/01/28 16:43:29 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=493\n",
      "\t\tFILE: Number of bytes written=365083\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=411\n",
      "\t\tHDFS: Number of bytes written=113\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4203\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1907\n",
      "\t\tTotal time spent by all map tasks (ms)=4203\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1907\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4203\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1907\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4303872\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1952768\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5\n",
      "\t\tMap output records=32\n",
      "\t\tMap output bytes=423\n",
      "\t\tMap output materialized bytes=499\n",
      "\t\tInput split bytes=234\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=7\n",
      "\t\tReduce shuffle bytes=499\n",
      "\t\tReduce input records=32\n",
      "\t\tReduce output records=6\n",
      "\t\tSpilled Records=64\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=118\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\t\tTotal committed heap usage (bytes)=552599552\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=177\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=113\n",
      "17/01/28 16:43:29 INFO streaming.StreamJob: Output directory: /user/koza/MROH/wordcount/output6\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/koza/MROH/wordcount/output6\n",
    "\n",
    "!hadoop jar {jarFile} \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1\" \\\n",
    "    -files reducer6.py,mapper6.py \\\n",
    "    -reducer reducer6.py \\\n",
    "    -mapper mapper6.py \\\n",
    "    -input /user/koza/MROH/wordcount/chineseExample.txt \\\n",
    "    -output /user/koza/MROH/wordcount/output6 \\\n",
    "    -numReduceTasks 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\tcount\trelFreq\ttotal\n",
      "==================================================\n",
      "beijing\t1\t0.0625\t16\n",
      "chinese\t9\t0.5625\t16\n",
      "japan\t2\t0.125\t16\n",
      "macao\t1\t0.0625\t16\n",
      "shanghai\t1\t0.0625\t16\n",
      "tokyo\t2\t0.125\t16\n"
     ]
    }
   ],
   "source": [
    "print \"word\\tcount\\trelFreq\\ttotal\"\n",
    "print \"=\"*50\n",
    "!hdfs dfs -cat /user/koza/MROH/wordcount/output6/part*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"7\">Calculate the word relative frequency using a \"stateless\" reducer (order inversion) on multiple reducers</a>\n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"8\">Calculate the word relative frequency using a \"stateless\" reducer (order inversion) on a multiple reducers with combiners</a>\n",
    "[back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "473px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
